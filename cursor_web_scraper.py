#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script cÃ o web cursor.com Ä‘á»ƒ thu tháº­p thÃ´ng tin vá» AI
TÃ¡c giáº£: Cursor Assistant cho anh NghÄ©a
NgÃ y: 25/10/2025
"""

import requests
from bs4 import BeautifulSoup
import json
import time
import os
from datetime import datetime
import re
from urllib.parse import urljoin, urlparse
import logging

# Thiáº¿t láº­p logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('cursor_scraping.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

class CursorWebScraper:
    def __init__(self):
        self.base_url = "https://cursor.com"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        self.scraped_data = {
            'main_page': {},
            'features': [],
            'pricing': {},
            'documentation': [],
            'blog_posts': [],
            'ai_insights': [],
            'metadata': {
                'scraped_at': datetime.now().isoformat(),
                'total_pages': 0,
                'successful_pages': 0
            }
        }
        
    def get_page_content(self, url):
        """Láº¥y ná»™i dung trang web"""
        try:
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            return response.text
        except Exception as e:
            logging.error(f"Lá»—i khi láº¥y ná»™i dung tá»« {url}: {str(e)}")
            return None
    
    def scrape_main_page(self):
        """CÃ o trang chá»§ cursor.com"""
        logging.info("ğŸš€ Báº¯t Ä‘áº§u cÃ o trang chá»§ cursor.com...")
        
        content = self.get_page_content(self.base_url)
        if not content:
            return
            
        soup = BeautifulSoup(content, 'html.parser')
        
        # Thu tháº­p thÃ´ng tin chÃ­nh
        main_data = {
            'title': soup.find('title').text if soup.find('title') else '',
            'description': '',
            'headlines': [],
            'features_overview': [],
            'cta_buttons': [],
            'testimonials': [],
            'ai_mentions': []
        }
        
        # TÃ¬m description
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc:
            main_data['description'] = meta_desc.get('content', '')
        
        # TÃ¬m cÃ¡c tiÃªu Ä‘á» chÃ­nh
        for tag in ['h1', 'h2', 'h3']:
            for headline in soup.find_all(tag):
                text = headline.get_text().strip()
                if text and len(text) > 10:
                    main_data['headlines'].append({
                        'tag': tag,
                        'text': text
                    })
        
        # TÃ¬m cÃ¡c tÃ­nh nÄƒng
        feature_sections = soup.find_all(['div', 'section'], class_=re.compile(r'feature|benefit|capability', re.I))
        for section in feature_sections:
            feature_text = section.get_text().strip()
            if feature_text and len(feature_text) > 20:
                main_data['features_overview'].append(feature_text)
        
        # TÃ¬m cÃ¡c nÃºt CTA
        for button in soup.find_all(['button', 'a'], class_=re.compile(r'cta|button|download|get|start', re.I)):
            button_text = button.get_text().strip()
            if button_text:
                main_data['cta_buttons'].append(button_text)
        
        # TÃ¬m cÃ¡c testimonial
        for testimonial in soup.find_all(['div', 'blockquote'], class_=re.compile(r'testimonial|review|quote', re.I)):
            testimonial_text = testimonial.get_text().strip()
            if testimonial_text and len(testimonial_text) > 30:
                main_data['testimonials'].append(testimonial_text)
        
        # TÃ¬m cÃ¡c mention vá» AI
        ai_keywords = ['AI', 'artificial intelligence', 'machine learning', 'neural', 'GPT', 'assistant', 'intelligent']
        for element in soup.find_all(text=re.compile('|'.join(ai_keywords), re.I)):
            if element.parent:
                ai_text = element.parent.get_text().strip()
                if ai_text and len(ai_text) > 20:
                    main_data['ai_mentions'].append(ai_text)
        
        self.scraped_data['main_page'] = main_data
        logging.info(f"âœ… ÄÃ£ cÃ o xong trang chá»§: {len(main_data['headlines'])} tiÃªu Ä‘á», {len(main_data['features_overview'])} tÃ­nh nÄƒng")
    
    def find_and_scrape_subpages(self):
        """TÃ¬m vÃ  cÃ o cÃ¡c trang con quan trá»ng"""
        logging.info("ğŸ” TÃ¬m kiáº¿m cÃ¡c trang con quan trá»ng...")
        
        content = self.get_page_content(self.base_url)
        if not content:
            return
            
        soup = BeautifulSoup(content, 'html.parser')
        
        # TÃ¬m cÃ¡c link quan trá»ng
        important_links = []
        for link in soup.find_all('a', href=True):
            href = link['href']
            text = link.get_text().strip().lower()
            
            # CÃ¡c tá»« khÃ³a quan trá»ng
            important_keywords = [
                'features', 'pricing', 'docs', 'documentation', 'blog', 
                'about', 'ai', 'intelligence', 'capabilities', 'how-it-works',
                'tutorial', 'guide', 'help', 'support'
            ]
            
            if any(keyword in text or keyword in href.lower() for keyword in important_keywords):
                full_url = urljoin(self.base_url, href)
                if full_url.startswith(self.base_url):
                    important_links.append({
                        'url': full_url,
                        'text': text,
                        'type': self.categorize_link(text, href)
                    })
        
        # Loáº¡i bá» duplicate
        unique_links = []
        seen_urls = set()
        for link in important_links:
            if link['url'] not in seen_urls:
                unique_links.append(link)
                seen_urls.add(link['url'])
        
        logging.info(f"ğŸ“‹ TÃ¬m tháº¥y {len(unique_links)} trang con quan trá»ng")
        
        # CÃ o tá»«ng trang con
        for i, link in enumerate(unique_links[:10]):  # Giá»›i háº¡n 10 trang Ä‘á»ƒ trÃ¡nh quÃ¡ táº£i
            logging.info(f"ğŸ“„ CÃ o trang {i+1}/{min(10, len(unique_links))}: {link['text']}")
            self.scrape_subpage(link)
            time.sleep(2)  # Nghá»‰ 2 giÃ¢y giá»¯a cÃ¡c request
    
    def categorize_link(self, text, href):
        """PhÃ¢n loáº¡i link"""
        text_lower = text.lower()
        href_lower = href.lower()
        
        if any(word in text_lower for word in ['feature', 'capability', 'function']):
            return 'features'
        elif any(word in text_lower for word in ['price', 'cost', 'plan', 'subscription']):
            return 'pricing'
        elif any(word in text_lower for word in ['doc', 'guide', 'tutorial', 'help']):
            return 'documentation'
        elif any(word in text_lower for word in ['blog', 'news', 'article']):
            return 'blog'
        elif any(word in text_lower for word in ['ai', 'intelligence', 'smart']):
            return 'ai_insights'
        else:
            return 'general'
    
    def scrape_subpage(self, link_info):
        """CÃ o má»™t trang con cá»¥ thá»ƒ"""
        url = link_info['url']
        page_type = link_info['type']
        
        content = self.get_page_content(url)
        if not content:
            return
            
        soup = BeautifulSoup(content, 'html.parser')
        
        page_data = {
            'url': url,
            'title': soup.find('title').text if soup.find('title') else '',
            'content': soup.get_text().strip(),
            'headings': [],
            'ai_related_content': [],
            'scraped_at': datetime.now().isoformat()
        }
        
        # Thu tháº­p cÃ¡c heading
        for tag in ['h1', 'h2', 'h3', 'h4']:
            for heading in soup.find_all(tag):
                heading_text = heading.get_text().strip()
                if heading_text:
                    page_data['headings'].append({
                        'level': tag,
                        'text': heading_text
                    })
        
        # TÃ¬m ná»™i dung liÃªn quan Ä‘áº¿n AI
        ai_keywords = ['AI', 'artificial intelligence', 'machine learning', 'neural', 'GPT', 'assistant', 'intelligent', 'smart', 'automated']
        for element in soup.find_all(text=re.compile('|'.join(ai_keywords), re.I)):
            if element.parent:
                ai_content = element.parent.get_text().strip()
                if ai_content and len(ai_content) > 30:
                    page_data['ai_related_content'].append(ai_content)
        
        # LÆ°u vÃ o cáº¥u trÃºc dá»¯ liá»‡u phÃ¹ há»£p
        if page_type == 'features':
            self.scraped_data['features'].append(page_data)
        elif page_type == 'pricing':
            self.scraped_data['pricing'] = page_data
        elif page_type == 'documentation':
            self.scraped_data['documentation'].append(page_data)
        elif page_type == 'blog':
            self.scraped_data['blog_posts'].append(page_data)
        elif page_type == 'ai_insights':
            self.scraped_data['ai_insights'].append(page_data)
        
        self.scraped_data['metadata']['successful_pages'] += 1
        logging.info(f"âœ… ÄÃ£ cÃ o xong: {page_data['title']}")
    
    def save_data(self):
        """LÆ°u dá»¯ liá»‡u Ä‘Ã£ cÃ o Ä‘Æ°á»£c"""
        # Táº¡o thÆ° má»¥c lÆ°u trá»¯
        os.makedirs('cursor_ai_library', exist_ok=True)
        
        # LÆ°u dá»¯ liá»‡u JSON
        with open('cursor_ai_library/raw_data.json', 'w', encoding='utf-8') as f:
            json.dump(self.scraped_data, f, ensure_ascii=False, indent=2)
        
        # Táº¡o bÃ¡o cÃ¡o tá»•ng há»£p
        self.create_summary_report()
        
        logging.info("ğŸ’¾ ÄÃ£ lÆ°u dá»¯ liá»‡u vÃ o cursor_ai_library/")
    
    def create_summary_report(self):
        """Táº¡o bÃ¡o cÃ¡o tá»•ng há»£p"""
        report = f"""
# BÃO CÃO Tá»”NG Há»¢P - CURSOR.COM AI LIBRARY
NgÃ y táº¡o: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}

## ğŸ“Š THá»NG KÃŠ Tá»”NG QUAN
- Tá»•ng sá»‘ trang Ä‘Ã£ cÃ o: {self.scraped_data['metadata']['successful_pages']}
- Sá»‘ tÃ­nh nÄƒng tÃ¬m tháº¥y: {len(self.scraped_data['features'])}
- Sá»‘ bÃ i blog: {len(self.scraped_data['blog_posts'])}
- Sá»‘ tÃ i liá»‡u: {len(self.scraped_data['documentation'])}
- Sá»‘ insight vá» AI: {len(self.scraped_data['ai_insights'])}

## ğŸ¯ THÃ”NG TIN CHÃNH Tá»ª TRANG CHá»¦
**TiÃªu Ä‘á»:** {self.scraped_data['main_page'].get('title', 'N/A')}
**MÃ´ táº£:** {self.scraped_data['main_page'].get('description', 'N/A')}

### CÃ¡c tiÃªu Ä‘á» quan trá»ng:
"""
        
        for headline in self.scraped_data['main_page'].get('headlines', [])[:10]:
            report += f"- {headline['text']}\n"
        
        report += f"""
### TÃ­nh nÄƒng chÃ­nh:
"""
        for feature in self.scraped_data['main_page'].get('features_overview', [])[:10]:
            report += f"- {feature}\n"
        
        report += f"""
### CÃ¡c mention vá» AI:
"""
        for ai_mention in self.scraped_data['main_page'].get('ai_mentions', [])[:10]:
            report += f"- {ai_mention}\n"
        
        # LÆ°u bÃ¡o cÃ¡o
        with open('cursor_ai_library/SUMMARY_REPORT.md', 'w', encoding='utf-8') as f:
            f.write(report)
    
    def run(self):
        """Cháº¡y toÃ n bá»™ quÃ¡ trÃ¬nh cÃ o web"""
        logging.info("ğŸš€ Báº¯t Ä‘áº§u cÃ o web cursor.com...")
        
        try:
            # CÃ o trang chá»§
            self.scrape_main_page()
            
            # TÃ¬m vÃ  cÃ o cÃ¡c trang con
            self.find_and_scrape_subpages()
            
            # LÆ°u dá»¯ liá»‡u
            self.save_data()
            
            logging.info("âœ… HoÃ n thÃ nh cÃ o web! Dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c lÆ°u vÃ o cursor_ai_library/")
            
        except Exception as e:
            logging.error(f"âŒ Lá»—i trong quÃ¡ trÃ¬nh cÃ o web: {str(e)}")

if __name__ == "__main__":
    scraper = CursorWebScraper()
    scraper.run()