# CURSOR AI - TH√îNG TIN K·ª∏ THU·∫¨T

*T√†i li·ªáu ƒë∆∞·ª£c t·∫°o t·ª± ƒë·ªông t·ª´ cursor.com - 25/10/2025 08:21:11*

## üî¨ NGHI√äN C·ª®U V√Ä PH√ÅT TRI·ªÇN

### C√°c nghi√™n c·ª©u m·ªõi nh·∫•t:

## Features ¬∑ Cursor

### The best way to build software.

### The best way to build software.

### Codebase understanding

### Powerful, yet flexible

### Codebase understanding

### Powerful, yet flexible

### Across the entire development process

### Across the entire development process

{"@context":"https://schema.org","@type":"Organization","@id":"https://cursor.com/#org","name":"Cursor","description":"Cursor is the best way to code with AI.","url":"https://cursor.com","logo":{"@type":"ImageObject","url":"https://cursor.com/public/opengraph-image.png","width":200,"height":60},"sameAs":["https://twitter.com/cursor_ai","https://www.linkedin.com/company/cursorai"],"contactPoint":{"@type":"ContactPoint","contactType":"customer service","url":"https://cursor.com/contact-sales"},"foundingDate":"2023"}

{"@context":"https://schema.org","@type":"WebSite","@id":"https://cursor.com/#website","name":"Cursor","description":"Cursor is the best way to code with AI.","url":"https://cursor.com","publisher":{"@type":"Organization","@id":"https://cursor.com/#org"}}

This element contains an interactive demo for sighted users. It's a demonstration of Cursor's IDE showing AI-powered coding assistance features. The interface is displayed over a scenic painted landscape wallpaper, giving the demo an artistic backdrop.

This element contains an interactive demo for sighted users. It's a demonstration of Cursor's IDE showing AI-powered coding assistance features. The interface is displayed over a scenic painted landscape wallpaper, giving the demo an artistic backdrop.

This element contains an interactive demo for sighted users. It's a demonstration of Cursor integrated within Slack, showing AI-powered assistance inside team communication. The interface is displayed over a scenic painted landscape wallpaper, giving the demo an artistic backdrop.

I implemented direct linking for changelog entries and updated Node.js version constraints across the project to improve compatibility and maintainability.

This element contains an interactive demo for sighted users. It's a demonstration of Cursor integrated within GitHub, showing AI-powered code review and debugging assistance. The interface is displayed over a scenic painted landscape wallpaper, giving the demo an artistic backdrop.

This element contains an interactive demo for sighted users. It's a demonstration of Cursor's IDE showing AI-powered coding assistance features. The interface is displayed over a scenic painted landscape wallpaper, giving the demo an artistic backdrop.

This element contains an interactive demo for sighted users. It's a demonstration of Cursor's IDE showing AI-powered coding assistance features. The interface is displayed over a scenic painted landscape wallpaper, giving the demo an artistic backdrop.

This element contains an interactive demo for sighted users. It's a demonstration of Cursor integrated within Slack, showing AI-powered assistance inside team communication. The interface is displayed over a scenic painted landscape wallpaper, giving the demo an artistic backdrop.

I implemented direct linking for changelog entries and updated Node.js version constraints across the project to improve compatibility and maintainability.

This element contains an interactive demo for sighted users. It's a demonstration of Cursor integrated within GitHub, showing AI-powered code review and debugging assistance. The interface is displayed over a scenic painted landscape wallpaper, giving the demo an artistic backdrop.

self.__next_f.push([1,"5:[\"$\",\"html\",null,{\"lang\":\"en-US\",\"suppressHydrationWarning\":true,\"className\":\"__variable_81079b __variable_58c592\",\"children\":[\"$\",\"body\",null,{\"className\":\"bg-theme-bg text-theme-text min-h-screen pt-[var(--site-header-height)]\",\"children\":[[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"Organization\\\",\\\"@id\\\":\\\"https://cursor.com/#org\\\",\\\"name\\\":\\\"Cursor\\\",\\\"description\\\":\\\"Cursor is the best way to code with AI.\\\",\\\"url\\\":\\\"https://cursor.com\\\",\\\"logo\\\":{\\\"@type\\\":\\\"ImageObject\\\",\\\"url\\\":\\\"https://cursor.com/public/opengraph-image.png\\\",\\\"width\\\":200,\\\"height\\\":60},\\\"sameAs\\\":[\\\"https://twitter.com/cursor_ai\\\",\\\"https://www.linkedin.com/company/cursorai\\\"],\\\"contactPoint\\\":{\\\"@type\\\":\\\"ContactPoint\\\",\\\"contactType\\\":\\\"customer service\\\",\\\"url\\\":\\\"https://cursor.com/contact-sales\\\"},\\\"foundingDate\\\":\\\"2023\\\"}\"}}],[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"WebSite\\\",\\\"@id\\\":\\\"https://cursor.com/#website\\\",\\\"name\\\":\\\"Cursor\\\",\\\"description\\\":\\\"Cursor is the best way to code with AI.\\\",\\\"url\\\":\\\"https://cursor.com\\\",\\\"publisher\\\":{\\\"@type\\\":\\\"Organization\\\",\\\"@id\\\":\\\"https://cursor.com/#org\\\"}}\"}}],\"$L11\"]}]}]\n"])

self.__next_f.push([1,"11:[\"$\",\"$L12\",null,{\"dictionary\":{\"next\":\"Next\",\"previous\":\"Previous\",\"back\":\"Back\",\"home\":\"Home\",\"blog\":\"Blog\",\"changelog\":\"Changelog\",\"topics\":\"Topics\",\"older posts\":\"Older posts\",\"newer posts\":\"Newer posts\",\"previous post\":\"Previous post\",\"next post\":\"Next post\",\"view more posts\":\"View more posts\",\"view all changelog entries\":\"See what‚Äôs new in Cursor\",\"by\":\"by\",\"in\":\"in\",\"author\":\"Author\",\"filed under\":\"Filed under\",\"submit\":\"Submit\",\"submitting...\":\"Submitting...\",\"company name\":\"Company name\",\"company size (engineers)\":\"Company size (Engineers)\",\"select company size\":\"Select company size\",\"additional details? (optional)\":\"Additional details? (Optional)\",\"how are you looking to use cursor?\":\"How are you looking to use Cursor?\",\"recommended\":\"Recommended\",\"features\":\"Features\",\"feature\":\"Feature\",\"download\":\"Download\",\"upgrade\":\"Upgrade\",\"pricing\":\"Pricing\",\"in progress\":\"In progress\",\"loading\":\"Loading\",\"error\":\"Error\",\"success\":\"Success\",\"reset\":\"Reset\",\"disable\":\"Disable\",\"enable\":\"Enable\",\"save\":\"Save\",\"cancel\":\"Cancel\",\"edit\":\"Edit\",\"delete\":\"Delete\",\"yes\":\"Yes\",\"no\":\"No\",\"all\":\"All\",\"none\":\"None\",\"select\":\"Select\",\"search\":\"Search\",\"menu\":\"Menu\",\"close\":\"Close\",\"open\":\"Open\",\"language\":\"Language\",\"english\":\"English\",\"japanese\":\"Japanese\",\"chinese\":\"Chinese\",\"all posts\":\"All Posts\",\"company\":\"Company\",\"news\":\"News\",\"product\":\"Product\",\"research\":\"Research\",\"stories\":\"Stories\",\"soc 2 certified\":\"SOC 2 Certified\",\"latest from cursor\":\"Latest from Cursor\"},\"dictionaryTranslations\":{\"next\":\"Next\",\"previous\":\"Previous\",\"back\":\"Back\",\"home\":\"Home\",\"blog\":\"Blog\",\"changelog\":\"Changelog\",\"topics\":\"Topics\",\"older posts\":\"Older posts\",\"newer posts\":\"Newer posts\",\"previous post\":\"Previous post\",\"next post\":\"Next post\",\"view more posts\":\"View more posts\",\"view all changelog entries\":\"See what‚Äôs new in Cursor\",\"by\":\"by\",\"in\":\"in\",\"author\":\"Author\",\"filed under\":\"Filed under\",\"submit\":\"Submit\",\"submitting...\":\"Submitting...\",\"company name\":\"Company name\",\"company size (engineers)\":\"Company size (Engineers)\",\"select company size\":\"Select company size\",\"additional details? (optional)\":\"Additional details? (Optional)\",\"how are you looking to use cursor?\":\"How are you looking to use Cursor?\",\"recommended\":\"Recommended\",\"features\":\"Features\",\"feature\":\"Feature\",\"download\":\"Download\",\"upgrade\":\"Upgrade\",\"pricing\":\"Pricing\",\"in progress\":\"In progress\",\"loading\":\"Loading\",\"error\":\"Error\",\"success\":\"Success\",\"reset\":\"Reset\",\"disable\":\"Disable\",\"enable\":\"Enable\",\"save\":\"Save\",\"cancel\":\"Cancel\",\"edit\":\"Edit\",\"delete\":\"Delete\",\"yes\":\"Yes\",\"no\":\"No\",\"all\":\"All\",\"none\":\"None\",\"select\":\"Select\",\"search\":\"Search\",\"menu\":\"Menu\",\"close\":\"Close\",\"open\":\"Open\",\"language\":\"Language\",\"english\":\"English\",\"japanese\":\"Japanese\",\"chinese\":\"Chinese\",\"all posts\":\"All Posts\",\"company\":\"Company\",\"news\":\"News\",\"product\":\"Product\",\"research\":\"Research\",\"stories\":\"Stories\",\"soc 2 certified\":\"SOC 2 Certified\",\"latest from cursor\":\"Latest from Cursor\"},\"translations\":{},\"locale\":\"en-US\",\"locales\":[\"en-US\",\"cn\",\"ja\",\"zh-Hant\"],\"defaultLocale\":\"en-US\",\"translationRequired\":false,\"dialectTranslationRequired\":false,\"region\":\"$undefined\",\"gtServicesEnabled\":true,\"projectId\":\"prj_jdh3imw75wenlfdi2jf1n41k\",\"translationEnabled\":true,\"runtimeUrl\":\"https://runtime2.gtx.dev\",\"devApiKey\":\"$undefined\",\"dictionaryEnabled\":true,\"renderSettings\":{\"method\":\"default\",\"timeout\":8000},\"developmentApiEnabled\":false,\"localeRoutingEnabledCookieName\":\"generaltranslation.locale-routing-enabled\",\"referrerLocaleCookieName\":\"generaltranslation.referrer-locale\",\"localeCookieName\":\"generaltranslation.locale\",\"resetLocaleCookieName\":\"generaltranslation.locale-reset\",\"customMapping\":{\"cn\":{\"code\":\"zh\"}},\"children\":[\"$\",\"$L13\",null,{\"enableSystem\":true,\"defaultTheme\":\"system\",\"disableTransitionOnChange\":true,\"storageKey\":\"marketing-theme\",\"children\":[\"$\",\"$L14\",null,{\"children\":[[\"$\",\"$L15\",null,{}],[\"$\",\"$e\",null,{\"fallback\":null,\"children\":\"$L16\"}],\"$L17\",[\"$\",\"$L18\",null,{}],\"$L19\",[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[\"$L1a\",[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/marketing-static/_next/static/css/8739a6a13754aafd.css?dpl=dpl_9sfZMY2911UstTVCyjNMFa1nrAJW\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"/marketing-static/_next/static/css/400864fd90c96a10.css?dpl=dpl_9sfZMY2911UstTVCyjNMFa1nrAJW\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}],\"$L1b\",\"$L1c\",\"$L1d\"]}]}]}]\n"])

self.__next_f.push([1,"1a:[\"$\",\"$L22\",null,{\"latestVersion\":{\"linux\":[{\"downloadUrl\":\"https://api2.cursor.sh/updates/download/golden/linux-arm64-deb/cursor/1.7\",\"label\":\"Linux .deb (ARM64)\"},{\"downloadUrl\":\"https://api2.cursor.sh/updates/download/golden/linux-x64-deb/cursor/1.7\",\"label\":\"Linux .deb (x64)\"},{\"downloadUrl\":\"https://api2.cursor.sh/updates/download/golden/linux-arm64-rpm/cursor/1.7\",\"label\":\"Linux RPM (ARM64)\"},{\"downloadUrl\":\"https://api2.cursor.sh/updates/download/golden/linux-x64-rpm/cursor/1.7\",\"label\":\"Linux RPM (x64)\"},{\"downloadUrl\":\"https://api2.cursor.sh/updates/download/golden/linux-arm64/cursor/1.7\",\"label\":\"Linux AppImage (ARM64)\"},{\"downloadUrl\":\"https://api2.cursor.sh/updates/download/golden/linux-x64/cursor/1.7\",\"label\":\"Linux AppImage (x64)\"}],\"macOS\":[{\"downloadUrl\":\"https://api2.cursor.sh/updates/download/golden/darwin-arm64/cursor/1.7\",\"label\":\"Mac (ARM64)\"},{\"downloadUrl\":\"https://api2.cursor.sh/updates/download/golden/darwin-x64/cursor/1.7\",\"label\":\"Mac (x64)\"},{\"downloadUrl\":\"https://api2.cursor.sh/updates/download/golden/darwin-universal/cursor/1.7\",\"label\":\"Mac Universal\"}],\"versionNumber\":\"1.7\",\"windows\":[{\"downloadUrl\":\"https://api2.cursor.sh/updates/download/golden/win32-x64/cursor/1.7\",\"label\":\"Windows (x64) (System)\"},{\"downloadUrl\":\"https://api2.cursor.sh/updates/download/golden/win32-x64-user/cursor/1.7\",\"label\":\"Windows (x64) (User)\"},{\"downloadUrl\":\"https://api2.cursor.sh/updates/download/golden/win32-arm64/cursor/1.7\",\"label\":\"Windows (ARM64) (System)\"},{\"downloadUrl\":\"https://api2.cursor.sh/updates/download/golden/win32-arm64-user/cursor/1.7\",\"label\":\"Windows (ARM64) (User)\"}]},\"children\":[\"$\",\"$L23\",null,{\"blogData\":[{\"_id\":\"5da81d1d-3524-44c0-84d2-c848bb69bd6b\",\"category\":\"product\",\"date\":\"2025-10-07T17:00:00.000Z\",\"excerpt\":\"Cursor can now create plans, research your codebase, and run agents for significantly longer.\",\"externalTitle\":null,\"externalUrl\":null,\"language\":\"en-US\",\"slug\":\"plan-mode\",\"thumbnailImage\":null,\"title\":\"Introducing Plan Mode\"},{\"_id\":\"b14b4ccd-b904-42d8-a224-578ab1aa6605\",\"category\":\"product\",\"date\":\"2025-10-01T17:20:00.000Z\",\"excerpt\":\"We‚Äôre speeding up the Java Language Server Protocol (LSP) and VS Code ecosystem.\",\"externalTitle\":null,\"externalUrl\":null,\"language\":\"en-US\",\"slug\":\"java\",\"thumbnailImage\":{\"_type\":\"image\",\"alt\":null,\"asset\":{\"metadata\":{\"dimensions\":null,\"isLandscape\":null,\"isPortrait\":null,\"isSquare\":true},\"url\":null},\"darkImage\":{\"metadata\":{\"dimensions\":null,\"isLandscape\":null,\"isPortrait\":null,\"isSquare\":true},\"url\":null},\"darkMobileImage\":{\"metadata\":{\"dimensions\":null},\"url\":null},\"mobileImage\":{\"metadata\":{\"dimensions\":null},\"url\":null},\"thumbnailType\":\"large\"},\"title\":\"Improving Java support in Cursor\"},{\"_id\":\"28698eca-aada-4254-ba86-bb7a1b5f43b8\",\"category\":\"research\",\"date\":\"2025-09-12T01:16:00.000Z\",\"excerpt\":\"Our new Tab model makes 21% fewer suggestions while having 28% higher accept rate.\",\"externalTitle\":null,\"externalUrl\":null,\"language\":\"en-US\",\"slug\":\"tab-rl\",\"thumbnailImage\":null,\"title\":\"Improving Cursor Tab with online RL\"},{\"_id\":\"9968b97c-6865-47af-970b-c644716aabe2\",\"category\":\"research\",\"date\":\"2025-08-29T02:55:25.007Z\",\"excerpt\":\"Achieving a 3.5x MoE layer speedup with a complete rebuild for Blackwell GPUs.\",\"externalTitle\":null,\"externalUrl\":null,\"language\":null,\"slug\":\"kernels\",\"thumbnailImage\":null,\"title\":\"1.5x faster MoE training with custom MXFP8 kernels\"}],\"children\":[\"$\",\"$L24\",null,{\"changelogData\":[{\"_id\":\"853c90ae-0e66-4ce0-9766-6e406e98a083\",\"date\":\"2025-09-29T05:08:58.822Z\",\"language\":\"en-US\",\"slug\":\"1-7\",\"title\":\"Browser Controls, Plan Mode, and Hooks\",\"version\":\"1.7\"},{\"_id\":\"8fc8a135-b0d9-4ad7-9db7-957768e793e6\",\"date\":\"2025-09-12T01:25:00.000Z\",\"language\":null,\"slug\":\"1-6\",\"title\":\"Slash commands, summarization, and improved Agent terminal\",\"version\":\"1.6\"},{\"_id\":\"04da9023-36ee-44e7-ae5c-15fea1269749\",\"date\":\"2025-08-21T19:54:00.000Z\",\"language\":\"en-US\",\"slug\":\"1-5\",\"title\":\"Linear integration, improved Agent terminal, and OS notifications\",\"version\":\"1.5\"},{\"_id\":\"d9e5960a-631a-4140-a2df-efe79921b56e\",\"date\":\"2025-08-06T19:44:00.000Z\",\"language\":null,\"slug\":\"1-4\",\"title\":\"Improved Agent tools, steerability, and usage visibility\",\"version\":\"1.4\"}],\"children\":\"$L25\"}]}]}]\n"])

self.__next_f.push([1,"19:[\"$\",\"header\",null,{\"className\":\"bg-theme-bg px-g2 fixed top-0 left-0 z-50 w-full\",\"children\":[\"$\",\"div\",null,{\"className\":\"relative z-2 container grid h-[var(--site-header-height)] grid-cols-[1fr_auto_auto] items-center lg:grid-cols-[auto_1fr_auto]\",\"children\":[\"$L26\",[\"$\",\"div\",null,{\"className\":\"col-start-1 col-end-2 row-start-1 row-end-2\",\"children\":[\"$\",\"$L27\",null,{\"title\":\"Cursor\"}]}],[\"$\",\"$L28\",null,{\"mainNavigation\":{\"_createdAt\":\"2025-06-25T17:34:29Z\",\"_id\":\"6d03b6cc-bfae-4884-abf9-c8fcae321aa8\",\"_rev\":\"kjFDLJNvlIZ5LZwJBGfqJ8\",\"_system\":{\"base\":{\"id\":\"6d03b6cc-bfae-4884-abf9-c8fcae321aa8\",\"rev\":\"3JcS3A5GjOK7pB9VbWt8ez\"}},\"_type\":\"mainNavigation\",\"_updatedAt\":\"2025-10-03T17:54:03Z\",\"items\":[{\"children\":null,\"link\":{\"_type\":\"link\",\"file\":null,\"href\":null,\"label\":\"Features\",\"linkType\":\"page\",\"openInNewTab\":null,\"page\":\"features\",\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"subnavColumns\":\"one\",\"title\":null,\"type\":\"link\"},{\"children\":null,\"link\":{\"_type\":\"link\",\"file\":null,\"href\":null,\"label\":\"Enterprise\",\"linkType\":\"page\",\"openInNewTab\":false,\"page\":\"enterprise\",\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"subnavColumns\":null,\"title\":null,\"type\":\"link\"},{\"children\":null,\"link\":{\"_type\":\"link\",\"file\":null,\"href\":\"/pricing\",\"label\":\"Pricing\",\"linkType\":\"href\",\"openInNewTab\":false,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"subnavColumns\":null,\"title\":null,\"type\":\"link\"},{\"children\":[{\"_key\":\"a3696f9e6cdb\",\"_type\":\"link\",\"file\":null,\"href\":\"/changelog\",\"label\":\"Changelog\",\"linkType\":\"href\",\"openInNewTab\":false,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},{\"_key\":\"b647cebb79d2\",\"_type\":\"link\",\"file\":null,\"href\":\"/blog\",\"label\":\"Blog\",\"linkType\":\"href\",\"openInNewTab\":false,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},{\"_key\":\"af349d1d2f5f\",\"_type\":\"link\",\"file\":null,\"href\":\"/docs\",\"label\":\"Docs\",\"linkType\":\"href\",\"openInNewTab\":false,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},{\"_key\":\"447badf81268\",\"_type\":\"link\",\"file\":null,\"href\":null,\"label\":\"Community\",\"linkType\":\"page\",\"openInNewTab\":false,\"page\":\"community\",\"post\":null,\"postLanguage\":null,\"simplePage\":null},{\"_key\":\"a064252bc419\",\"_type\":\"link\",\"file\":null,\"href\":\"https://forum.cursor.com/\",\"label\":\"Forum\",\"linkType\":\"href\",\"openInNewTab\":true,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},{\"_key\":\"deea9a680819\",\"_type\":\"link\",\"file\":null,\"href\":\"https://anysphere.inc/\",\"label\":\"Careers\",\"linkType\":\"href\",\"openInNewTab\":true,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null}],\"link\":{\"_type\":\"link\",\"file\":null,\"href\":\"/changelog\",\"label\":\"Resources\",\"linkType\":\"href\",\"openInNewTab\":false,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"subnavColumns\":\"two\",\"title\":\"Resources\",\"type\":\"link\"}],\"language\":\"en-US\",\"navigationCta1\":{\"icon\":null,\"link\":{\"_type\":\"link\",\"file\":null,\"href\":\"https://cursor.com/dashboard\",\"label\":\"Sign in\",\"linkType\":\"href\",\"openInNewTab\":true,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"size\":\"small\",\"variant\":\"ghost\"},\"navigationCta2\":{\"icon\":null,\"link\":{\"_type\":\"link\",\"file\":null,\"href\":\"/download\",\"label\":\"Download\",\"linkType\":\"href\",\"openInNewTab\":null,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"size\":\"small\",\"variant\":\"primary\"}},\"currentLocale\":\"en-US\"}],[\"$\",\"div\",null,{\"className\":\"gap-g0.75 col-start-2 col-end-3 row-start-1 row-end-2 flex items-center justify-self-end lg:col-start-3 lg:col-end-[-1]\",\"children\":[\"$L29\",\"$L2a\"]}]]}]}]\n"])

self.__next_f.push([1,"1b:[\"$\",\"footer\",null,{\"className\":\"pt-v3 pb-v3 md:pb-g3 px-g2 bg-theme-card-hex relative\",\"children\":[[\"$\",\"div\",null,{\"className\":\"mb-v4.5 container\",\"children\":[\"$\",\"nav\",null,{\"children\":[\"$\",\"div\",null,{\"className\":\"gap-x-g1 gap-y-v2 grid grid-cols-2 md:grid-cols-5\",\"children\":[[\"$\",\"div\",\"0\",{\"children\":[[\"$\",\"h3\",null,{\"className\":\"type-base md:type-sm text-theme-text-sec pb-v2.5/12\",\"children\":\"Product\"}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",\"0\",{\"children\":[\"$\",\"$L2b\",null,{\"link\":{\"_key\":\"bd3849c1404f\",\"_type\":\"link\",\"file\":null,\"href\":null,\"label\":\"Features\",\"linkType\":\"page\",\"openInNewTab\":false,\"page\":\"features\",\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"className\":\"type-base md:type-sm py-v2.5/12 footer-link inline-block\",\"currentLocale\":\"en-US\",\"children\":[\"Features\",\"$undefined\"]}]}],[\"$\",\"li\",\"1\",{\"children\":[\"$\",\"$L2b\",null,{\"link\":{\"_key\":\"059fd2fbd5cc\",\"_type\":\"link\",\"file\":null,\"href\":null,\"label\":\"Enterprise\",\"linkType\":\"page\",\"openInNewTab\":false,\"page\":\"enterprise\",\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"className\":\"type-base md:type-sm py-v2.5/12 footer-link inline-block\",\"currentLocale\":\"en-US\",\"children\":[\"Enterprise\",\"$undefined\"]}]}],[\"$\",\"li\",\"2\",{\"children\":[\"$\",\"$L2b\",null,{\"link\":{\"_key\":\"d7ed5ceda74a\",\"_type\":\"link\",\"file\":null,\"href\":null,\"label\":\"Bugbot\",\"linkType\":\"page\",\"openInNewTab\":false,\"page\":\"bugbot\",\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"className\":\"type-base md:type-sm py-v2.5/12 footer-link inline-block\",\"currentLocale\":\"en-US\",\"children\":[\"Bugbot\",\"$undefined\"]}]}],[\"$\",\"li\",\"3\",{\"children\":[\"$\",\"$L2b\",null,{\"link\":{\"_key\":\"2d8d3619c635\",\"_type\":\"link\",\"file\":null,\"href\":null,\"label\":\"CLI\",\"linkType\":\"page\",\"openInNewTab\":false,\"page\":\"cli\",\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"className\":\"type-base md:type-sm py-v2.5/12 footer-link inline-block\",\"currentLocale\":\"en-US\",\"children\":[\"CLI\",\"$undefined\"]}]}],[\"$\",\"li\",\"4\",{\"children\":[\"$\",\"$L2b\",null,{\"link\":{\"_key\":\"4a9c5bbe338c\",\"_type\":\"link\",\"file\":null,\"href\":\"/pricing\",\"label\":\"Pricing\",\"linkType\":\"href\",\"openInNewTab\":false,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"className\":\"type-base md:type-sm py-v2.5/12 footer-link inline-block\",\"currentLocale\":\"en-US\",\"children\":[\"Pricing\",\"$undefined\"]}]}]]}]]}],[\"$\",\"div\",\"1\",{\"children\":[[\"$\",\"h3\",null,{\"className\":\"type-base md:type-sm text-theme-text-sec pb-v2.5/12\",\"children\":\"Resources\"}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",\"0\",{\"children\":[\"$\",\"$L2b\",null,{\"link\":{\"_key\":\"88cade86b69f\",\"_type\":\"link\",\"file\":null,\"href\":\"/download\",\"label\":\"Download\",\"linkType\":\"href\",\"openInNewTab\":false,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"className\":\"type-base md:type-sm py-v2.5/12 footer-link inline-block\",\"currentLocale\":\"en-US\",\"children\":[\"Download\",\"$undefined\"]}]}],[\"$\",\"li\",\"1\",{\"children\":[\"$\",\"$L2b\",null,{\"link\":{\"_key\":\"ac54cf6b8655\",\"_type\":\"link\",\"file\":null,\"href\":\"/agents\",\"label\":\"Web Agents\",\"linkType\":\"href\",\"openInNewTab\":true,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"className\":\"type-base md:type-sm py-v2.5/12 footer-link inline-block\",\"currentLocale\":\"en-US\",\"children\":[\"Web Agents\",\"$undefined\"]}]}],[\"$\",\"li\",\"2\",{\"children\":[\"$\",\"$L2b\",null,{\"link\":{\"_key\":\"437357adbec9\",\"_type\":\"link\",\"file\":null,\"href\":\"/changelog\",\"label\":\"Changelog\",\"linkType\":\"href\",\"openInNewTab\":false,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"className\":\"type-base md:type-sm py-v2.5/12 footer-link inline-block\",\"currentLocale\":\"en-US\",\"children\":[\"Changelog\",\"$undefined\"]}]}],[\"$\",\"li\",\"3\",{\"children\":[\"$\",\"$L2b\",null,{\"link\":{\"_key\":\"02984c67264f\",\"_type\":\"link\",\"file\":null,\"href\":\"/docs\",\"label\":\"Docs\",\"linkType\":\"href\",\"openInNewTab\":false,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"className\":\"type-base md:type-sm py-v2.5/12 footer-link inline-block\",\"currentLocale\":\"en-US\",\"children\":[\"Docs\",\"$undefined\"]}]}],[\"$\",\"li\",\"4\",{\"children\":[\"$\",\"$L2b\",null,{\"link\":{\"_key\":\"162f532a26d5\",\"_type\":\"link\",\"file\":null,\"href\":\"https://forum.cursor.com/\",\"label\":\"Forum\",\"linkType\":\"href\",\"openInNewTab\":true,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"className\":\"type-base md:type-sm py-v2.5/12 footer-link inline-block\",\"currentLocale\":\"en-US\",\"children\":[\"Forum\",[\"$\",\"span\",null,{\"className\":\"footer-link__icon\",\"children\":\" ‚Üó\"}]]}]}],[\"$\",\"li\",\"5\",{\"children\":[\"$\",\"$L2b\",null,{\"link\":{\"_key\":\"15adb9498c9b\",\"_type\":\"link\",\"file\":null,\"href\":\"https://status.cursor.com/\",\"label\":\"Status\",\"linkType\":\"href\",\"openInNewTab\":true,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"className\":\"type-base md:type-sm py-v2.5/12 footer-link inline-block\",\"currentLocale\":\"en-US\",\"children\":[\"Status\",[\"$\",\"span\",null,{\"className\":\"footer-link__icon\",\"children\":\" ‚Üó\"}]]}]}]]}]]}],[\"$\",\"div\",\"2\",{\"children\":[[\"$\",\"h3\",null,{\"className\":\"type-base md:type-sm text-theme-text-sec pb-v2.5/12\",\"children\":\"Company\"}],[\"$\",\"ul\",null,{\"children\":[\"$L2c\",\"$L2d\",\"$L2e\",\"$L2f\",\"$L30\"]}]]}],\"$L31\",\"$L32\"]}]}]}],\"$L33\"]}]\n"])

self.__next_f.push([1,"32:[\"$\",\"div\",\"4\",{\"children\":[[\"$\",\"h3\",null,{\"className\":\"type-base md:type-sm text-theme-text-sec pb-v2.5/12\",\"children\":\"Connect\"}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",\"0\",{\"children\":[\"$\",\"$L2b\",null,{\"link\":{\"_key\":\"a47878555d9f\",\"_type\":\"link\",\"file\":null,\"href\":\"https://x.com/cursor_ai\",\"label\":\"X\",\"linkType\":\"href\",\"openInNewTab\":true,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"className\":\"type-base md:type-sm py-v2.5/12 footer-link inline-block\",\"currentLocale\":\"en-US\",\"children\":[\"X\",[\"$\",\"span\",null,{\"className\":\"footer-link__icon\",\"children\":\" ‚Üó\"}]]}]}],[\"$\",\"li\",\"1\",{\"children\":[\"$\",\"$L2b\",null,{\"link\":{\"_key\":\"04d633aeb0d5\",\"_type\":\"link\",\"file\":null,\"href\":\"https://www.linkedin.com/company/cursorai\",\"label\":\"LinkedIn\",\"linkType\":\"href\",\"openInNewTab\":true,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"className\":\"type-base md:type-sm py-v2.5/12 footer-link inline-block\",\"currentLocale\":\"en-US\",\"children\":[\"LinkedIn\",[\"$\",\"span\",null,{\"className\":\"footer-link__icon\",\"children\":\" ‚Üó\"}]]}]}],[\"$\",\"li\",\"2\",{\"children\":[\"$\",\"$L2b\",null,{\"link\":{\"_key\":\"79a67a3532b1\",\"_type\":\"link\",\"file\":null,\"href\":\"https://www.youtube.com/@cursor_ai\",\"label\":\"YouTube\",\"linkType\":\"href\",\"openInNewTab\":true,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"className\":\"type-base md:type-sm py-v2.5/12 footer-link inline-block\",\"currentLocale\":\"en-US\",\"children\":[\"YouTube\",[\"$\",\"span\",null,{\"className\":\"footer-link__icon\",\"children\":\" ‚Üó\"}]]}]}]]}]]}]\n"])

self.__next_f.push([1,"33:[\"$\",\"div\",null,{\"className\":\"gap-v2 container flex flex-col items-start justify-between md:flex-row md:items-center\",\"children\":[[\"$\",\"div\",null,{\"className\":\"text-theme-text-sec gap-g1.5 flex items-center\",\"children\":[[\"$\",\"small\",null,{\"className\":\"type-base md:type-sm\",\"children\":[\"¬© \",2025,\" \",[\"$\",\"a\",null,{\"href\":\"https://anysphere.inc\",\"className\":\"hover:text-theme-text active:text-theme-text\",\"children\":\"Anysphere, Inc.\"}]]}],[\"$\",\"small\",null,{\"className\":\"type-base md:type-sm\",\"children\":[\"üõ° \",[\"$\",\"$L2b\",null,{\"link\":{\"linkType\":\"href\",\"href\":\"/security\"},\"currentLocale\":\"en-US\",\"className\":\"hover:text-theme-text active:text-theme-text inline\",\"children\":\"$L34\"}]]}]]}],[\"$\",\"div\",null,{\"className\":\"gap-g1.5 flex items-center\",\"children\":[[\"$\",\"$L35\",null,{}],[\"$\",\"$L36\",null,{}]]}]]}]\n"])

self.__next_f.push([1,"26:[\"$\",\"a\",null,{\"href\":\"#main\",\"className\":\"skipnav btn btn--sm\",\"children\":\"Skip to content\",\"data-_gt\":\"$undefined\"}]\n29:[\"$\",\"$L2b\",null,{\"link\":\"$19:props:children:props:children:2:props:mainNavigation:navigationCta1:link\",\"className\":\"btn--ghost btn--sm\",\"ctaTracking\":\"$undefined\",\"download\":\"$undefined\",\"onClick\":\"$undefined\",\"children\":[\"Sign in\",null]}]\n2a:[\"$\",\"$L2b\",null,{\"link\":\"$19:props:children:props:children:2:props:mainNavigation:navigationCta2:link\",\"className\":\"max-sm:hidden btn btn--sm\",\"ctaTracking\":\"$undefined\",\"download\":\"$undefined\",\"onClick\":\"$undefined\",\"children\":[\"Download\",null]}]\n"])

self.__next_f.push([1,"3a:I[84243,[],\"IconMark\"]\n25:[\"$\",\"main\",null,{\"id\":\"main\",\"data-sanity\":\"id=notFoundPage;type=page;path=pageBuilder;base=http%3A%2F%2Flocalhost%3A3333\",\"children\":[\"$L37\"]}]\n34:\"SOC 2 Certified\"\n"])

self.__next_f.push([1,"6:[null,[\"$\",\"$L22\",null,{\"latestVersion\":{\"linux\":[{\"downloadUrl\":\"https://api2.cursor.sh/updates/download/golden/linux-arm64-deb/cursor/1.7\",\"label\":\"Linux .deb (ARM64)\"},{\"downloadUrl\":\"https://api2.cursor.sh/updates/download/golden/linux-x64-deb/cursor/1.7\",\"label\":\"Linux .deb (x64)\"},{\"downloadUrl\":\"https://api2.cursor.sh/updates/download/golden/linux-arm64-rpm/cursor/1.7\",\"label\":\"Linux RPM (ARM64)\"},{\"downloadUrl\":\"https://api2.cursor.sh/updates/download/golden/linux-x64-rpm/cursor/1.7\",\"label\":\"Linux RPM (x64)\"},{\"downloadUrl\":\"https://api2.cursor.sh/updates/download/golden/linux-arm64/cursor/1.7\",\"label\":\"Linux AppImage (ARM64)\"},{\"downloadUrl\":\"https://api2.cursor.sh/updates/download/golden/linux-x64/cursor/1.7\",\"label\":\"Linux AppImage (x64)\"}],\"macOS\":[{\"downloadUrl\":\"https://api2.cursor.sh/updates/download/golden/darwin-arm64/cursor/1.7\",\"label\":\"Mac (ARM64)\"},{\"downloadUrl\":\"https://api2.cursor.sh/updates/download/golden/darwin-x64/cursor/1.7\",\"label\":\"Mac (x64)\"},{\"downloadUrl\":\"https://api2.cursor.sh/updates/download/golden/darwin-universal/cursor/1.7\",\"label\":\"Mac Universal\"}],\"versionNumber\":\"1.7\",\"windows\":[{\"downloadUrl\":\"https://api2.cursor.sh/updates/download/golden/win32-x64/cursor/1.7\",\"label\":\"Windows (x64) (System)\"},{\"downloadUrl\":\"https://api2.cursor.sh/updates/download/golden/win32-x64-user/cursor/1.7\",\"label\":\"Windows (x64) (User)\"},{\"downloadUrl\":\"https://api2.cursor.sh/updates/download/golden/win32-arm64/cursor/1.7\",\"label\":\"Windows (ARM64) (System)\"},{\"downloadUrl\":\"https://api2.cursor.sh/updates/download/golden/win32-arm64-user/cursor/1.7\",\"label\":\"Windows (ARM64) (User)\"}]},\"children\":[\"$\",\"$L23\",null,{\"blogData\":[{\"_id\":\"5da81d1d-3524-44c0-84d2-c848bb69bd6b\",\"category\":\"product\",\"date\":\"2025-10-07T17:00:00.000Z\",\"excerpt\":\"Cursor can now create plans, research your codebase, and run agents for significantly longer.\",\"externalTitle\":null,\"externalUrl\":null,\"language\":\"en-US\",\"slug\":\"plan-mode\",\"thumbnailImage\":null,\"title\":\"Introducing Plan Mode\"},{\"_id\":\"b14b4ccd-b904-42d8-a224-578ab1aa6605\",\"category\":\"product\",\"date\":\"2025-10-01T17:20:00.000Z\",\"excerpt\":\"We‚Äôre speeding up the Java Language Server Protocol (LSP) and VS Code ecosystem.\",\"externalTitle\":null,\"externalUrl\":null,\"language\":\"en-US\",\"slug\":\"java\",\"thumbnailImage\":{\"_type\":\"image\",\"alt\":null,\"asset\":{\"metadata\":{\"dimensions\":null,\"isLandscape\":null,\"isPortrait\":null,\"isSquare\":true},\"url\":null},\"darkImage\":{\"metadata\":{\"dimensions\":null,\"isLandscape\":null,\"isPortrait\":null,\"isSquare\":true},\"url\":null},\"darkMobileImage\":{\"metadata\":{\"dimensions\":null},\"url\":null},\"mobileImage\":{\"metadata\":{\"dimensions\":null},\"url\":null},\"thumbnailType\":\"large\"},\"title\":\"Improving Java support in Cursor\"},{\"_id\":\"28698eca-aada-4254-ba86-bb7a1b5f43b8\",\"category\":\"research\",\"date\":\"2025-09-12T01:16:00.000Z\",\"excerpt\":\"Our new Tab model makes 21% fewer suggestions while having 28% higher accept rate.\",\"externalTitle\":null,\"externalUrl\":null,\"language\":\"en-US\",\"slug\":\"tab-rl\",\"thumbnailImage\":null,\"title\":\"Improving Cursor Tab with online RL\"},{\"_id\":\"9968b97c-6865-47af-970b-c644716aabe2\",\"category\":\"research\",\"date\":\"2025-08-29T02:55:25.007Z\",\"excerpt\":\"Achieving a 3.5x MoE layer speedup with a complete rebuild for Blackwell GPUs.\",\"externalTitle\":null,\"externalUrl\":null,\"language\":null,\"slug\":\"kernels\",\"thumbnailImage\":null,\"title\":\"1.5x faster MoE training with custom MXFP8 kernels\"}],\"children\":[\"$\",\"$L24\",null,{\"changelogData\":[{\"_id\":\"853c90ae-0e66-4ce0-9766-6e406e98a083\",\"date\":\"2025-09-29T05:08:58.822Z\",\"language\":\"en-US\",\"slug\":\"1-7\",\"title\":\"Browser Controls, Plan Mode, and Hooks\",\"version\":\"1.7\"},{\"_id\":\"8fc8a135-b0d9-4ad7-9db7-957768e793e6\",\"date\":\"2025-09-12T01:25:00.000Z\",\"language\":null,\"slug\":\"1-6\",\"title\":\"Slash commands, summarization, and improved Agent terminal\",\"version\":\"1.6\"},{\"_id\":\"04da9023-36ee-44e7-ae5c-15fea1269749\",\"date\":\"2025-08-21T19:54:00.000Z\",\"language\":\"en-US\",\"slug\":\"1-5\",\"title\":\"Linear integration, improved Agent terminal, and OS notifications\",\"version\":\"1.5\"},{\"_id\":\"d9e5960a-631a-4140-a2df-efe79921b56e\",\"date\":\"2025-08-06T19:44:00.000Z\",\"language\":null,\"slug\":\"1-4\",\"title\":\"Improved Agent tools, steerability, and usage visibility\",\"version\":\"1.4\"}],\"children\":\"$L38\"}]}]}],false,\"$L39\"]\n"])

self.__next_f.push([1,"a:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"Features ¬∑ Cursor\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Built to make you extraordinarily productive, Cursor is the best way to code with AI.\"}],[\"$\",\"link\",\"2\",{\"rel\":\"manifest\",\"href\":\"/marketing-static/manifest.webmanifest\",\"crossOrigin\":\"$undefined\"}],[\"$\",\"meta\",\"3\",{\"name\":\"preconnect\",\"content\":\"https://cdn.sanity.io\"}],[\"$\",\"link\",\"4\",{\"rel\":\"canonical\",\"href\":\"https://cursor.com/features\"}],[\"$\",\"link\",\"5\",{\"rel\":\"alternate\",\"hrefLang\":\"en-US\",\"href\":\"https://cursor.com/features\"}],[\"$\",\"link\",\"6\",{\"rel\":\"alternate\",\"hrefLang\":\"cn\",\"href\":\"https://cursor.com/cn/features\"}],[\"$\",\"link\",\"7\",{\"rel\":\"alternate\",\"hrefLang\":\"ja\",\"href\":\"https://cursor.com/ja/features\"}],[\"$\",\"link\",\"8\",{\"rel\":\"alternate\",\"hrefLang\":\"zh-Hant\",\"href\":\"https://cursor.com/zh-Hant/features\"}],[\"$\",\"meta\",\"9\",{\"property\":\"og:title\",\"content\":\"Features\"}],[\"$\",\"meta\",\"10\",{\"property\":\"og:description\",\"content\":\"Built to make you extraordinarily productive, Cursor is the best way to code with AI.\"}],[\"$\",\"meta\",\"11\",{\"property\":\"og:url\",\"content\":\"https://cursor.com/features\"}],[\"$\",\"meta\",\"12\",{\"property\":\"og:site_name\",\"content\":\"Cursor\"}],[\"$\",\"meta\",\"13\",{\"property\":\"og:locale\",\"content\":\"en-US\"}],[\"$\",\"meta\",\"14\",{\"property\":\"og:image\",\"content\":\"https://cdn.sanity.io/images/2hv88549/production/581af892e88b9b7e17a19773730e5d9907c03cec-2401x1260.png?auto=format\u0026w=1200\u0026h=627\u0026fit=crop\"}],[\"$\",\"meta\",\"15\",{\"property\":\"og:image:width\",\"content\":\"1200\"}],[\"$\",\"meta\",\"16\",{\"property\":\"og:image:height\",\"content\":\"627\"}],[\"$\",\"meta\",\"17\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"18\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"19\",{\"name\":\"twitter:title\",\"content\":\"Features\"}],[\"$\",\"meta\",\"20\",{\"name\":\"twitter:description\",\"content\":\"Built to make you extraordinarily productive, Cursor is the best way to code with AI.\"}],[\"$\",\"meta\",\"21\",{\"name\":\"twitter:image\",\"content\":\"https://cdn.sanity.io/images/2hv88549/production/581af892e88b9b7e17a19773730e5d9907c03cec-2401x1260.png?auto=format\u0026w=1200\u0026h=627\u0026fit=crop\"}],[\"$\",\"meta\",\"22\",{\"name\":\"twitter:image:width\",\"content\":\"1200\"}],[\"$\",\"meta\",\"23\",{\"name\":\"twitter:image:height\",\"content\":\"627\"}],[\"$\",\"link\",\"24\",{\"rel\":\"apple-touch-icon\",\"href\":\"/marketing-static/apple-touch-icon.png\"}],[\"$\",\"link\",\"25\",{\"rel\":\"icon\",\"href\":\"/marketing-static/icon-192x192.png\",\"sizes\":\"192x192\",\"type\":\"image/png\"}],[\"$\",\"link\",\"26\",{\"rel\":\"icon\",\"href\":\"/marketing-static/icon-512x512.png\",\"sizes\":\"512x512\",\"type\":\"image/png\"}],[\"$\",\"$L3a\",\"27\",{}]],\"error\":null,\"digest\":\"$undefined\"}\n"])

self.__next_f.push([1,"37:[\"$\",\"section\",\"c6a92b6344f6,c6a92b6344f6\",{\"className\":\"section bg-theme-bg text-theme-text section--flush-y\",\"id\":\"$undefined\",\"data-sanity\":\"id=notFoundPage;type=page;path=pageBuilder:c6a92b6344f6;base=http%3A%2F%2Flocalhost%3A3333\",\"children\":[\"$\",\"div\",null,{\"className\":\"container\",\"children\":[\"$\",\"div\",null,{\"className\":\"text-left max-w-prose\",\"children\":[false,\"$undefined\",[\"$\",\"h1\",null,{\"className\":\"type-md-lg text-balance \",\"children\":\"Page not found (404)\"}],[\"$\",\"div\",null,{\"className\":\"flex justify-start mb-v1\",\"children\":[\"$\",\"div\",null,{\"className\":\"type-md-lg text-theme-text-sec stack text-balance\",\"children\":[[\"$\",\"p\",\"9f1e4b93d80d\",{\"children\":[\"Tabbed into the void.\"]}]]}]}],[\"$\",\"div\",null,{\"className\":\"gap-x-g1 flex items-center justify-start \",\"children\":[\"$L3b\"]}]]}]}]}]\n"])

self.__next_f.push([1,"38:[\"$\",\"main\",null,{\"id\":\"main\",\"data-sanity\":\"id=acd960f1-42cb-4fbc-b238-3298c7b4c4ca;type=page;path=pageBuilder;base=http%3A%2F%2Flocalhost%3A3333\",\"children\":[\"$L3c\",\"$L3d\",\"$L3e\",\"$L3f\",\"$L40\",\"$L41\",\"$L42\"]}]\n3b:[\"$\",\"$L2b\",\"0\",{\"link\":{\"_type\":\"link\",\"file\":null,\"href\":null,\"label\":\"Take me home\",\"linkType\":\"page\",\"openInNewTab\":false,\"page\":\"/\",\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"className\":\"btn\",\"ctaTracking\":null,\"download\":\"$undefined\",\"onClick\":\"$undefined\",\"children\":[\"Take me home\",[\"$\",\"div\",null,{\"aria-hidden\":\"true\",\"className\":\"btn-icon\",\"children\":[\"$\",\"span\",null,{\"className\":\"\",\"aria-hidden\":\"true\",\"children\":\"‚Üí\"}]}]]}]\n"])

self.__next_f.push([1,"3c:[\"$\",\"section\",\"46bf8ff51f64f27c25ba59c4d3a9271c,46bf8ff51f64f27c25ba59c4d3a9271c\",{\"className\":\"section bg-theme-bg text-theme-text section--flush-y\",\"id\":\"$undefined\",\"data-sanity\":\"id=acd960f1-42cb-4fbc-b238-3298c7b4c4ca;type=page;path=pageBuilder:46bf8ff51f64f27c25ba59c4d3a9271c;base=http%3A%2F%2Flocalhost%3A3333\",\"children\":[\"$\",\"div\",null,{\"className\":\"container\",\"children\":[\"$\",\"div\",null,{\"className\":\"text-left max-w-prose-medium-wide\",\"children\":[false,[\"$\",\"small\",null,{\"className\":\"type-base text-theme-text-sec block mb-v2/12\",\"children\":\"Features\"}],[\"$\",\"h1\",null,{\"className\":\"type-lg text-balance mb-v1\",\"children\":\"The best way to build software.\"}],null,[\"$\",\"div\",null,{\"className\":\"gap-x-g1 flex items-center justify-start \",\"children\":[[\"$\",\"$L43\",\"0\",{\"variant\":\"btn\",\"size\":\"regular\"}]]}]]}]}]}]\n"])

self.__next_f.push([1,"40:[\"$\",\"section\",\"14ca7f7b0120,14ca7f7b0120\",{\"className\":\"section bg-theme-bg text-theme-text\",\"id\":\"$undefined\",\"data-sanity\":\"id=acd960f1-42cb-4fbc-b238-3298c7b4c4ca;type=page;path=pageBuilder:14ca7f7b0120;base=http%3A%2F%2Flocalhost%3A3333\",\"children\":[\"$\",\"div\",null,{\"className\":\"container my-v2\",\"children\":[[\"$\",\"div\",null,{\"className\":\"text-left mb-v2 max-w-prose\",\"children\":[false,\"$undefined\",[\"$\",\"h2\",null,{\"className\":\"type-md-lg text-balance \",\"children\":\"Powerful, yet flexible\"}],[\"$\",\"div\",null,{\"className\":\"flex justify-start\",\"children\":[\"$\",\"div\",null,{\"className\":\"type-md-lg text-theme-text-sec stack text-balance\",\"children\":[[\"$\",\"p\",\"f02c35cdb1f2\",{\"children\":[\"Configure Cursor so you can do your best work.\"]}]]}]}],null]}],[\"$\",\"div\",null,{\"className\":\"grid gap-g1 grid-cols-1 md:grid-cols-2 lg:grid-cols-4 items-stretch\",\"children\":[[\"$\",\"div\",\"card-0\",{\"className\":\"undefined h-full\",\"children\":[\"$\",\"$L2b\",null,{\"link\":{\"file\":null,\"href\":\"https://docs.cursor.com/en/guides/migration/vscode\",\"label\":null,\"linkType\":\"href\",\"openInNewTab\":true,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"className\":\"card card--feature flex h-full grow-1 flex-col\",\"children\":\"$L4a\"}]}],[\"$\",\"div\",\"card-1\",{\"className\":\"undefined h-full\",\"children\":[\"$\",\"$L2b\",null,{\"link\":{\"file\":null,\"href\":\"https://docs.cursor.com/context/mcp\",\"label\":null,\"linkType\":\"href\",\"openInNewTab\":true,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"className\":\"card card--feature flex h-full grow-1 flex-col\",\"children\":\"$L4b\"}]}],[\"$\",\"div\",\"card-2\",{\"className\":\"undefined h-full\",\"children\":[\"$\",\"$L2b\",null,{\"link\":{\"file\":null,\"href\":\"https://docs.cursor.com/context/rules\",\"label\":null,\"linkType\":\"href\",\"openInNewTab\":true,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"className\":\"card card--feature flex h-full grow-1 flex-col\",\"children\":\"$L4c\"}]}],[\"$\",\"div\",\"card-3\",{\"className\":\"undefined h-full\",\"children\":[\"$\",\"$L2b\",null,{\"link\":{\"file\":null,\"href\":\"https://docs.cursor.com/en/agent/chat/commands\",\"label\":null,\"linkType\":\"href\",\"openInNewTab\":true,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"className\":\"card card--feature flex h-full grow-1 flex-col\",\"children\":\"$L4d\"}]}]]}]]}]}]\n"])

self.__next_f.push([1,"41:[\"$\",\"$L4e\",\"c996bbd2cf86,c996bbd2cf86\",{\"block\":{\"_key\":\"c996bbd2cf86\",\"_type\":\"changelogList\",\"alignment\":\"left\",\"banner\":{\"linkType\":\"none\",\"openInNewTab\":false},\"textSize\":\"md\",\"title\":\"Changelog\"},\"index\":5,\"pageId\":\"acd960f1-42cb-4fbc-b238-3298c7b4c4ca\",\"pageType\":\"page\",\"preload\":false,\"semanticLevel\":\"h2\",\"isEmbedded\":false,\"pageData\":{\"_id\":\"acd960f1-42cb-4fbc-b238-3298c7b4c4ca\",\"_type\":\"page\",\"breadcrumb\":null,\"canonicalUrl\":null,\"footnotes\":null,\"hideFromSearchEngines\":null,\"isPasswordProtected\":null,\"language\":null,\"pageBuilder\":[{\"_key\":\"46bf8ff51f64f27c25ba59c4d3a9271c\",\"_type\":\"headline\",\"alignment\":\"left\",\"banner\":{\"actionLabel\":null,\"file\":null,\"href\":null,\"label\":null,\"linkType\":\"none\",\"openInNewTab\":false,\"page\":null,\"post\":null,\"simplePage\":null},\"body\":null,\"cta\":[{\"copyButton\":false,\"downloadButton\":true,\"icon\":null,\"link\":{\"_type\":\"link\",\"file\":null,\"href\":null,\"label\":null,\"linkType\":\"none\",\"openInNewTab\":false,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"size\":\"regular\",\"tracking\":null,\"variant\":\"primary\"}],\"eyebrow\":\"Features\",\"textSize\":\"lg\",\"title\":\"The best way to build software.\"},{\"_key\":\"8cdcdc9fd44f\",\"_type\":\"section\",\"alignment\":\"left\",\"banner\":{\"linkType\":\"none\",\"openInNewTab\":false},\"cta\":null,\"footerAlignment\":\"left\",\"footerBanner\":{\"linkType\":\"none\",\"openInNewTab\":false},\"footerCta\":null,\"footerTextSize\":\"md\",\"sectionBuilder\":[{\"_key\":\"9f14b7d65a6e\",\"_type\":\"feature\",\"body\":[{\"_key\":\"9f15504e70fc\",\"_type\":\"block\",\"children\":[{\"_key\":\"2c7002fcd87e\",\"_type\":\"span\",\"marks\":[],\"text\":\"Delegate coding tasks so you can focus on higher-level direction.\"}],\"markDefs\":[],\"style\":\"normal\"}],\"link\":{\"file\":null,\"href\":null,\"label\":null,\"linkType\":\"none\",\"openInNewTab\":false,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"media\":null,\"mediaBg\":{\"_type\":\"image\",\"asset\":{\"metadata\":{\"dimensions\":{\"_type\":\"sanity.imageDimensions\",\"aspectRatio\":1.4053356282271945,\"height\":2324,\"width\":3266},\"isLandscape\":true,\"isPortrait\":false,\"isSquare\":false},\"url\":\"https://cdn.sanity.io/images/2hv88549/production/1ffde036387b7242c29496bd7b1009f2218bce43-3266x2324.jpg?auto=format\"},\"darkImage\":{\"metadata\":{\"dimensions\":null,\"isLandscape\":true,\"isPortrait\":false,\"isSquare\":false},\"url\":null},\"darkMobileImage\":{\"metadata\":{\"dimensions\":null},\"url\":null},\"isWallpaper\":true,\"mobileImage\":{\"metadata\":{\"dimensions\":null},\"url\":null}},\"mediaPosition\":\"bottomRight\",\"mediaSide\":\"right\",\"mediaType\":\"reactComponent\",\"reactComponent\":{\"_id\":\"85929782-bcc7-4cf4-a034-5ae532a579f6\",\"description\":null,\"markdownContent\":\"\u003cDemoDesktop minHeight={650} \\n windows={[\\n {\\n id: \\\"agent-react-hooks\\\",\\n title: \\\"Cursor\\\",\\n x: 50,\\n y: 50, widthPx: 680,\\n zIndex: 10, ctaHref: \\\"/download\\\", ctaLabel: \\\"Get Cursor\\\", \\n content: (\\n \u003cCursorIDE\\n layoutPreset=\\\"agent\\\"\\n showEditor={false}\\n showAgents={true}\\n showChat={true}\\n showSidebar={false} \\n scenario=\\\"startupAnalytics\\\"\\n mode={{ type: \\\"agent\\\", streamInitialMessages: true }}\\n embedded\\n /\u003e\\n ),\\n },\\n ]}\\n /\u003e\",\"title\":\"Demo Desktop: Agent\"},\"sectionId\":\"agent\",\"testimonial\":null,\"title\":\"Agent\",\"video\":null},{\"_key\":\"6c011cb33f14\",\"_type\":\"multicard\",\"alignment\":\"left\",\"banner\":{\"_type\":\"object\",\"linkType\":\"none\",\"openInNewTab\":false},\"cardType\":\"feature\",\"columns\":\"3\",\"cta\":null,\"featureCards\":[{\"_key\":\"5db205dec81f\",\"_type\":\"featureCard\",\"body\":[{\"_key\":\"d3bb720376a4\",\"_type\":\"block\",\"children\":[{\"_key\":\"d002a07c67b8\",\"_type\":\"span\",\"marks\":[],\"text\":\"Cursor‚Äôs codebase embedding model gives Agent deep understanding and recall.\"}],\"markDefs\":[],\"style\":\"normal\"}],\"link\":{\"file\":null,\"href\":null,\"label\":null,\"linkType\":\"none\",\"openInNewTab\":false,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"media\":null,\"mediaBg\":{\"_type\":\"image\",\"asset\":{\"metadata\":{\"dimensions\":{\"_type\":\"sanity.imageDimensions\",\"aspectRatio\":1.7777777777777777,\"height\":1440,\"width\":2560},\"isLandscape\":true,\"isPortrait\":false,\"isSquare\":false},\"url\":\"https://cdn.sanity.io/images/2hv88549/production/fd9b3b4cd7d670f9f7d89ef54a9d83eedc7eb8cc-2560x1440.png?auto=format\"},\"darkImage\":{\"metadata\":{\"dimensions\":{\"_type\":\"sanity.imageDimensions\",\"aspectRatio\":1.7777777777777777,\"height\":1440,\"width\":2560},\"isLandscape\":true,\"isPortrait\":false,\"isSquare\":false},\"url\":\"https://cdn.sanity.io/images/2hv88549/production/3cb319c263fd5a76115b6196b916ce8767daec13-2560x1440.png?auto=format\"},\"darkMobileImage\":{\"metadata\":{\"dimensions\":null},\"url\":null},\"isWallpaper\":true,\"mobileImage\":{\"metadata\":{\"dimensions\":null},\"url\":null}},\"mediaPosition\":\"bottomRight\",\"mediaType\":\"reactComponent\",\"reactComponent\":{\"_id\":\"e9892a31-fc5b-4259-9187-b94bd37d454f\",\"description\":null,\"markdownContent\":\"\u003cToolCalls aspectRatio={{ base: \\\"1/1\\\", md: \\\"1/1\\\" }} height=\\\"378px\\\" /\u003e\",\"title\":\"Close Up: Tool Calls (Features)\"},\"title\":\"Codebase understanding\",\"video\":null},{\"_key\":\"891f98ab8f44\",\"_type\":\"featureCard\",\"body\":[{\"_key\":\"1102dba46986\",\"_type\":\"block\",\"children\":[{\"_key\":\"f43336cfed7c\",\"_type\":\"span\",\"marks\":[],\"text\":\"Choose freely between frontier models from OpenAI, Anthropic, Gemini, and xAI.\"}],\"markDefs\":[],\"style\":\"normal\"}],\"link\":{\"file\":null,\"href\":null,\"label\":null,\"linkType\":\"none\",\"openInNewTab\":false,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"media\":null,\"mediaBg\":{\"_type\":\"image\",\"asset\":{\"metadata\":{\"dimensions\":{\"_type\":\"sanity.imageDimensions\",\"aspectRatio\":1.7777777777777777,\"height\":1440,\"width\":2560},\"isLandscape\":true,\"isPortrait\":false,\"isSquare\":false},\"url\":\"https://cdn.sanity.io/images/2hv88549/production/0e44f2d6b59d6d3ab8ea68a301f92e079d0e7e89-2560x1440.png?auto=format\"},\"darkImage\":{\"metadata\":{\"dimensions\":{\"_type\":\"sanity.imageDimensions\",\"aspectRatio\":1.7777777777777777,\"height\":1440,\"width\":2560},\"isLandscape\":true,\"isPortrait\":false,\"isSquare\":false},\"url\":\"https://cdn.sanity.io/images/2hv88549/production/23cfd8adb1efe8a6cd2f77479efbbee8b6fd7ad3-2560x1440.png?auto=format\"},\"darkMobileImage\":{\"metadata\":{\"dimensions\":null},\"url\":null},\"isWallpaper\":true,\"mobileImage\":{\"metadata\":{\"dimensions\":null},\"url\":null}},\"mediaPosition\":\"bottomRight\",\"mediaType\":\"reactComponent\",\"reactComponent\":{\"_id\":\"3c61cb4c-99a8-41bd-96f7-032db843ed6f\",\"description\":null,\"markdownContent\":\"\u003cModelMenuUI currentModel=\\\"GPT-5\\\" aspectRatio={{ base: \\\"1/1\\\", md: \\\"1/1\\\" }} height=\\\"378px\\\" /\u003e\",\"title\":\"Close Up: Model Menu (Features)\"},\"title\":\"Top model access\",\"video\":null},{\"_key\":\"6d186dd978d9\",\"_type\":\"featureCard\",\"body\":[{\"_key\":\"d046f2ab3522\",\"_type\":\"block\",\"children\":[{\"_key\":\"95f6e5c4005c\",\"_type\":\"span\",\"marks\":[],\"text\":\"Make targeted edits or run terminal commands with natural language.\"}],\"markDefs\":[],\"style\":\"normal\"}],\"link\":{\"file\":null,\"href\":null,\"label\":null,\"linkType\":\"none\",\"openInNewTab\":false,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"media\":null,\"mediaBg\":{\"_type\":\"image\",\"asset\":{\"metadata\":{\"dimensions\":{\"_type\":\"sanity.imageDimensions\",\"aspectRatio\":1.7777777777777777,\"height\":1440,\"width\":2560},\"isLandscape\":true,\"isPortrait\":false,\"isSquare\":false},\"url\":\"https://cdn.sanity.io/images/2hv88549/production/214fe44b44ebb3a38c3c7e6bde246a032d1bdc57-2560x1440.png?auto=format\"},\"darkImage\":{\"metadata\":{\"dimensions\":{\"_type\":\"sanity.imageDimensions\",\"aspectRatio\":1.7777777777777777,\"height\":1440,\"width\":2560},\"isLandscape\":true,\"isPortrait\":false,\"isSquare\":false},\"url\":\"https://cdn.sanity.io/images/2hv88549/production/e7193be5129e61392f38b191266ecafa2410bb3d-2560x1440.png?auto=format\"},\"darkMobileImage\":{\"metadata\":{\"dimensions\":null},\"url\":null},\"isWallpaper\":true,\"mobileImage\":{\"metadata\":{\"dimensions\":null},\"url\":null}},\"mediaPosition\":\"bottomRight\",\"mediaType\":\"reactComponent\",\"reactComponent\":{\"_id\":\"aae01e80-60e2-4464-a55c-e13e2bc46cdf\",\"description\":\"cmdk demo desktop\",\"markdownContent\":\"\u003cDemoDesktop disableInteractions height=\\\"378px\\\" innerPaddingPx={24}\\n windows={[\\n {\\n id: \\\"cursor-ide\\\",\\n title: \\\"\\\",\\n x: 50,\\n y: 50, widthPx: 680, heightPx: 330, shadowless: true,\\n zIndex: 10, ctaHref: \\\"/download\\\", ctaLabel: \\\"Get Cursor\\\", \\n content: (\\n\u003cCursorIDE\\n layoutPreset=\\\"ide\\\" scenario=\\\"biotech\\\" \\nshowSidebar={false} \\nshowChat={false} \\nmode={{ type: \\\"cmdk\\\", autoplay: true, selectLines: { start: 7, end: 8 }, selectionDelay: 800 }} \\nembedded \\n/\u003e\\n ),\\n },\\n ]}\\n/\u003e\",\"title\":\"Features: Cmd+K Inline Agent\"},\"title\":\"Scoped changes\",\"video\":null}],\"infoCards\":null,\"mediaCards\":null,\"sectionId\":\"=\",\"textSize\":\"md\"}],\"textSize\":\"md\"},{\"_key\":\"5bcdf8ca2f5b5f7cec3ea704688da6cb\",\"_type\":\"section\",\"alignment\":\"left\",\"banner\":{\"linkType\":\"none\",\"openInNewTab\":false},\"cta\":null,\"footerAlignment\":\"left\",\"footerBanner\":{\"linkType\":\"none\",\"openInNewTab\":false},\"footerCta\":null,\"footerTextSize\":\"md\",\"sectionBuilder\":[{\"_key\":\"9f14b7d65a6e\",\"_type\":\"feature\",\"body\":[{\"_key\":\"d6ba645f3ed8\",\"_type\":\"block\",\"children\":[{\"_key\":\"6f43c076df9f\",\"_type\":\"span\",\"marks\":[],\"text\":\"Our custom autocomplete model predicts your next actions.\"}],\"markDefs\":[],\"style\":\"normal\"}],\"link\":{\"file\":null,\"href\":null,\"label\":null,\"linkType\":\"none\",\"openInNewTab\":false,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"media\":null,\"mediaBg\":{\"_type\":\"image\",\"asset\":{\"metadata\":{\"dimensions\":{\"_type\":\"sanity.imageDimensions\",\"aspectRatio\":1.5,\"height\":1330,\"width\":1995},\"isLandscape\":true,\"isPortrait\":false,\"isSquare\":false},\"url\":\"https://cdn.sanity.io/images/2hv88549/production/6a23c94721e22f5c31f2ef72ccd7cdf9fecd9e12-1995x1330.jpg?auto=format\"},\"darkImage\":{\"metadata\":{\"dimensions\":null,\"isLandscape\":true,\"isPortrait\":false,\"isSquare\":false},\"url\":null},\"darkMobileImage\":{\"metadata\":{\"dimensions\":null},\"url\":null},\"isWallpaper\":true,\"mobileImage\":{\"metadata\":{\"dimensions\":null},\"url\":null}},\"mediaPosition\":\"bottomRight\",\"mediaSide\":\"right\",\"mediaType\":\"reactComponent\",\"reactComponent\":{\"_id\":\"f8336082-f437-4996-88f1-622aa745b65e\",\"description\":\"tab demo desktop\",\"markdownContent\":\"\u003cDemoDesktop\\n disableInteractions minHeight={650} \\n windows={[\\n {\\n id: \\\"cursor-ide\\\",\\n title: \\\"Cursor\\\",\\n x: 50,\\n y: 50,\\n widthPx: 680,\\n zIndex: 10, ctaHref: \\\"/download\\\", ctaLabel: \\\"Get Cursor\\\", \\n content: (\\n\u003cCursorIDE\\n layoutPreset=\\\"ide\\\" scenario=\\\"autoComplete\\\" \\n showSidebar={false}\\n showChat={false}\\n mode={{ type: \\\"tab\\\", autoplay: true, loop: true, advanceMs: 800 }} embedded \\n/\u003e\\n ),\\n },\\n ]}\\n/\u003e\",\"title\":\"Demo Desktop: Tab Tab Tab\"},\"sectionId\":\"tab\",\"testimonial\":null,\"title\":\"Tab\",\"video\":null},{\"_key\":\"6c011cb33f14\",\"_type\":\"multicard\",\"alignment\":\"left\",\"banner\":{\"_type\":\"object\",\"linkType\":\"none\",\"openInNewTab\":false},\"cardType\":\"feature\",\"columns\":\"3\",\"cta\":null,\"featureCards\":[{\"_key\":\"891f98ab8f44\",\"_type\":\"featureCard\",\"body\":[{\"_key\":\"ce25153fe594\",\"_type\":\"block\",\"children\":[{\"_key\":\"482d4d541284\",\"_type\":\"span\",\"marks\":[],\"text\":\"Get suggested edits across multiple lines.\"}],\"markDefs\":[],\"style\":\"normal\"}],\"link\":{\"file\":null,\"href\":null,\"label\":null,\"linkType\":\"none\",\"openInNewTab\":false,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"media\":null,\"mediaBg\":{\"_type\":\"image\",\"asset\":{\"metadata\":{\"dimensions\":{\"_type\":\"sanity.imageDimensions\",\"aspectRatio\":1.7777777777777777,\"height\":1440,\"width\":2560},\"isLandscape\":true,\"isPortrait\":false,\"isSquare\":false},\"url\":\"https://cdn.sanity.io/images/2hv88549/production/fd9b3b4cd7d670f9f7d89ef54a9d83eedc7eb8cc-2560x1440.png?auto=format\"},\"darkImage\":{\"metadata\":{\"dimensions\":{\"_type\":\"sanity.imageDimensions\",\"aspectRatio\":1.7777777777777777,\"height\":1440,\"width\":2560},\"isLandscape\":true,\"isPortrait\":false,\"isSquare\":false},\"url\":\"https://cdn.sanity.io/images/2hv88549/production/3cb319c263fd5a76115b6196b916ce8767daec13-2560x1440.png?auto=format\"},\"darkMobileImage\":{\"metadata\":{\"dimensions\":null},\"url\":null},\"isWallpaper\":true,\"mobileImage\":{\"metadata\":{\"dimensions\":null},\"url\":null}},\"mediaPosition\":\"bottomRight\",\"mediaType\":\"reactComponent\",\"reactComponent\":{\"_id\":\"051aefbd-0a51-450c-be79-4b9f7873c940\",\"description\":null,\"markdownContent\":\"\u003cDemoDesktop disableInteractions height=\\\"378px\\\" innerPaddingPx={24}\\n windows={[\\n {\\n id: \\\"cursor-ide\\\",\\n title: \\\"\\\",\\n x: 50,\\n y: 50, widthPx: 680, heightPx: 330, shadowless: true,\\n zIndex: 10, ctaHref: \\\"/download\\\", ctaLabel: \\\"Get Cursor\\\", \\n content: (\\n\u003cCursorIDE\\n layoutPreset=\\\"ide\\\"\\n scenario=\\\"changeManagement\\\"\\n showSidebar={false}\\n showChat={false}\\n mode={{ type: \\\"tab-static\\\", multiHighlight: [6, 7, 13], caretLine: 2 }}\\n embedded\\n/\u003e\\n ),\\n },\\n ]}\\n/\u003e\",\"title\":\"Features: Multi-line edits\"},\"title\":\"Multi-line edits\",\"video\":null},{\"_key\":\"6d186dd978d9\",\"_type\":\"featureCard\",\"body\":[{\"_key\":\"555b3562f029\",\"_type\":\"block\",\"children\":[{\"_key\":\"b97c211f8d0e\",\"_type\":\"span\",\"marks\":[],\"text\":\"Type naturally, Cursor will finish your thought.\"}],\"markDefs\":[],\"style\":\"normal\"}],\"link\":{\"file\":null,\"href\":null,\"label\":null,\"linkType\":\"none\",\"openInNewTab\":false,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"media\":null,\"mediaBg\":{\"_type\":\"image\",\"asset\":{\"metadata\":{\"dimensions\":{\"_type\":\"sanity.imageDimensions\",\"aspectRatio\":1.7777777777777777,\"height\":1440,\"width\":2560},\"isLandscape\":true,\"isPortrait\":false,\"isSquare\":false},\"url\":\"https://cdn.sanity.io/images/2hv88549/production/fd9b3b4cd7d670f9f7d89ef54a9d83eedc7eb8cc-2560x1440.png?auto=format\"},\"darkImage\":{\"metadata\":{\"dimensions\":{\"_type\":\"sanity.imageDimensions\",\"aspectRatio\":1.7777777777777777,\"height\":1440,\"width\":2560},\"isLandscape\":true,\"isPortrait\":false,\"isSquare\":false},\"url\":\"https://cdn.sanity.io/images/2hv88549/production/3cb319c263fd5a76115b6196b916ce8767daec13-2560x1440.png?auto=format\"},\"darkMobileImage\":{\"metadata\":{\"dimensions\":null},\"url\":null},\"isWallpaper\":true,\"mobileImage\":{\"metadata\":{\"dimensions\":null},\"url\":null}},\"mediaPosition\":\"bottomRight\",\"mediaType\":\"reactComponent\",\"reactComponent\":{\"_id\":\"f0dfa025-574c-4eab-959b-99878eadae44\",\"description\":null,\"markdownContent\":\"\u003cDemoDesktop disableInteractions height=\\\"378px\\\" innerPaddingPx={24}\\n windows={[\\n {\\n id: \\\"cursor-ide\\\",\\n title: \\\"\\\",\\n x: 50,\\n y: 50, widthPx: 680, heightPx: 330, shadowless: true,\\n zIndex: 10, ctaHref: \\\"/download\\\", ctaLabel: \\\"Get Cursor\\\", \\n content: (\\n\u003cCursorIDE\\n layoutPreset=\\\"ide\\\"\\n scenario=\\\"pseudoCode\\\"\\n showSidebar={false}\\n showChat={false}\\n mode={{ type: \\\"tab-static\\\", suggestAfterLine: 7, caretLine: 7 }}\\n embedded\\n/\u003e\\n ),\\n },\\n ]}\\n/\u003e\",\"title\":\"Feature: Smart Rewrites\"},\"title\":\"Smart rewrites\",\"video\":null},{\"_key\":\"5db205dec81f\",\"_type\":\"featureCard\",\"body\":[{\"_key\":\"d3bb720376a4\",\"_type\":\"block\",\"children\":[{\"_key\":\"d002a07c67b8\",\"_type\":\"span\",\"marks\":[],\"text\":\"Fly through edits at your cursor and across files.\"}],\"markDefs\":[],\"style\":\"normal\"}],\"link\":{\"file\":null,\"href\":null,\"label\":null,\"linkType\":\"none\",\"openInNewTab\":false,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"media\":null,\"mediaBg\":{\"_type\":\"image\",\"asset\":{\"metadata\":{\"dimensions\":{\"_type\":\"sanity.imageDimensions\",\"aspectRatio\":1.7777777777777777,\"height\":1440,\"width\":2560},\"isLandscape\":true,\"isPortrait\":false,\"isSquare\":false},\"url\":\"https://cdn.sanity.io/images/2hv88549/production/fd9b3b4cd7d670f9f7d89ef54a9d83eedc7eb8cc-2560x1440.png?auto=format\"},\"darkImage\":{\"metadata\":{\"dimensions\":{\"_type\":\"sanity.imageDimensions\",\"aspectRatio\":1.7777777777777777,\"height\":1440,\"width\":2560},\"isLandscape\":true,\"isPortrait\":false,\"isSquare\":false},\"url\":\"https://cdn.sanity.io/images/2hv88549/production/3cb319c263fd5a76115b6196b916ce8767daec13-2560x1440.png?auto=format\"},\"darkMobileImage\":{\"metadata\":{\"dimensions\":null},\"url\":null},\"isWallpaper\":true,\"mobileImage\":{\"metadata\":{\"dimensions\":null},\"url\":null}},\"mediaPosition\":\"bottomRight\",\"mediaType\":\"reactComponent\",\"reactComponent\":{\"_id\":\"f8b0ef4d-a02d-4d8d-92af-c95bbf442577\",\"description\":null,\"markdownContent\":\"\u003cDemoDesktop disableInteractions height=\\\"378px\\\" innerPaddingPx={24}\\n windows={[\\n {\\n id: \\\"cursor-ide\\\",\\n title: \\\"\\\",\\n x: 50,\\n y: 50, widthPx: 341, heightPx: 330, shadowless: true,\\n zIndex: 10, ctaHref: \\\"/download\\\", ctaLabel: \\\"Get Cursor\\\", \\n content: (\\n\u003cCursorIDE\\n layoutPreset=\\\"ide\\\"\\n scenario=\\\"changeManagement\\\"\\n showSidebar={false}\\n showChat={false}\\n mode={{ type: \\\"tab-static\\\", bottomPillLabel: \\\"Changelog.tsx\\\" }}\\n embedded\\n/\u003e\\n ),\\n },\\n ]}\\n/\u003e\",\"title\":\"Feature: Tab, Tab, Tab\"},\"title\":\"Tab, Tab, Tab\",\"video\":null}],\"infoCards\":null,\"mediaCards\":null,\"sectionId\":\"=\",\"textSize\":\"md\"}],\"textSize\":\"md\"},{\"_key\":\"5ef109abd194\",\"_type\":\"section\",\"alignment\":\"left\",\"banner\":{\"linkType\":\"none\",\"openInNewTab\":false},\"cta\":null,\"footerAlignment\":\"left\",\"footerBanner\":{\"linkType\":\"none\",\"openInNewTab\":false},\"footerCta\":null,\"footerTextSize\":\"md\",\"sectionBuilder\":[{\"_key\":\"b59b0619eff7\",\"_type\":\"feature\",\"body\":[{\"_key\":\"4df8eb57dc30\",\"_type\":\"block\",\"children\":[{\"_key\":\"7f916947361f\",\"_type\":\"span\",\"marks\":[],\"text\":\"Start tasks from Slack, issue tracker, mobile and more. Finish off in the IDE.\"}],\"markDefs\":[],\"style\":\"normal\"}],\"link\":{\"file\":null,\"href\":\"/agents\",\"label\":\"Try agents on web and mobile\",\"linkType\":\"href\",\"openInNewTab\":true,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"media\":null,\"mediaBg\":{\"_type\":\"image\",\"asset\":{\"metadata\":{\"dimensions\":{\"_type\":\"sanity.imageDimensions\",\"aspectRatio\":1.4997611084567606,\"height\":2093,\"width\":3139},\"isLandscape\":true,\"isPortrait\":false,\"isSquare\":false},\"url\":\"https://cdn.sanity.io/images/2hv88549/production/00a586c62c8782e65c0affe6363a43ed6bdbc1fd-3139x2093.jpg?auto=format\"},\"darkImage\":{\"metadata\":{\"dimensions\":null,\"isLandscape\":true,\"isPortrait\":false,\"isSquare\":false},\"url\":null},\"darkMobileImage\":{\"metadata\":{\"dimensions\":null},\"url\":null},\"isWallpaper\":true,\"mobileImage\":{\"metadata\":{\"dimensions\":null},\"url\":null}},\"mediaPosition\":\"bottomRight\",\"mediaSide\":\"right\",\"mediaType\":\"reactComponent\",\"reactComponent\":{\"_id\":\"2c2ba0d5-572a-463a-8d2c-c603f834fe61\",\"description\":null,\"markdownContent\":\"$4f\",\"title\":\"Features Demo: Slack + Mobile\"},\"sectionId\":\"ecosystem\",\"testimonial\":null,\"title\":\"Across the entire development process\",\"video\":null},{\"_key\":\"5431cb0a0703\",\"_type\":\"multicard\",\"alignment\":\"left\",\"banner\":{\"_type\":\"object\",\"linkType\":\"none\",\"openInNewTab\":false},\"cardType\":\"feature\",\"columns\":\"3\",\"cta\":null,\"featureCards\":[{\"_key\":\"7327f79c8db7\",\"_type\":\"featureCard\",\"body\":[{\"_key\":\"0e72422c5a4c\",\"_type\":\"block\",\"children\":[{\"_key\":\"822b2a4cfab4\",\"_type\":\"span\",\"marks\":[],\"text\":\"Manual to agentic coding, in one familiar editor.\"}],\"markDefs\":[],\"style\":\"normal\"}],\"link\":{\"file\":null,\"href\":\"/download\",\"label\":\"Download\",\"linkType\":\"href\",\"openInNewTab\":false,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"media\":{\"_type\":\"image\",\"asset\":{\"metadata\":{\"dimensions\":{\"_type\":\"sanity.imageDimensions\",\"aspectRatio\":1,\"height\":1216,\"width\":1216}},\"url\":\"https://cdn.sanity.io/images/2hv88549/production/c56ff51f54a841f52801180eb268680e55d5cb8b-1216x1216.png?auto=format\"},\"darkImage\":{\"metadata\":{\"dimensions\":null},\"url\":null},\"darkMobileImage\":{\"metadata\":{\"dimensions\":null},\"url\":null},\"mobileImage\":{\"metadata\":{\"dimensions\":null},\"url\":null}},\"mediaBg\":{\"_type\":\"image\",\"asset\":{\"metadata\":{\"dimensions\":{\"_type\":\"sanity.imageDimensions\",\"aspectRatio\":1.7777777777777777,\"height\":1440,\"width\":2560},\"isLandscape\":true,\"isPortrait\":false,\"isSquare\":false},\"url\":\"https://cdn.sanity.io/images/2hv88549/production/214fe44b44ebb3a38c3c7e6bde246a032d1bdc57-2560x1440.png?auto=format\"},\"darkImage\":{\"metadata\":{\"dimensions\":{\"_type\":\"sanity.imageDimensions\",\"aspectRatio\":1.7777777777777777,\"height\":1440,\"width\":2560},\"isLandscape\":true,\"isPortrait\":false,\"isSquare\":false},\"url\":\"https://cdn.sanity.io/images/2hv88549/production/e7193be5129e61392f38b191266ecafa2410bb3d-2560x1440.png?auto=format\"},\"darkMobileImage\":{\"metadata\":{\"dimensions\":null},\"url\":null},\"isWallpaper\":true,\"mobileImage\":{\"metadata\":{\"dimensions\":null},\"url\":null}},\"mediaBgColor\":\"card-02-hex\",\"mediaType\":\"reactComponent\",\"reactComponent\":{\"_id\":\"57ce3f90-bb7e-4464-b145-b1285b60562f\",\"description\":null,\"markdownContent\":\" \u003cDemoDesktop disableInteractions innerPaddingPx={24} aspectRatio={{ base: \\\"1/1\\\", md: \\\"1/1\\\" }} height=\\\"378px\\\"\\n windows={[\\n {\\n id: \\\"agent-ide\\\",\\n title: \\\"\\\",\\n x: 50,\\n y: 50, widthPx: 680, heightPx: 330, \\n zIndex: 10, shadowless: true,\\n content: (\\n \u003cCursorIDE\\n layoutPreset=\\\"ide\\\" showSidebar={false} showChat={false}\\n scenario=\\\"startupAnalytics\\\"\\n mode={{ type: \\\"agent\\\" }}\\n embedded\\n /\u003e\\n ),\\n },\\n ]}\\n /\u003e\",\"title\":\"Close Up: IDE\"},\"title\":\"IDE\",\"video\":null},{\"_key\":\"4bdd4997ed7f\",\"_type\":\"featureCard\",\"body\":[{\"_key\":\"5eaa055395b3\",\"_type\":\"block\",\"children\":[{\"_key\":\"0a838bb84708\",\"_type\":\"span\",\"marks\":[],\"text\":\"Run agents in any terminal or script.\"}],\"markDefs\":[],\"style\":\"normal\"}],\"link\":{\"file\":null,\"href\":null,\"label\":\"Learn more\",\"linkType\":\"page\",\"openInNewTab\":false,\"page\":\"cli\",\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"media\":{\"_type\":\"image\",\"asset\":{\"metadata\":{\"dimensions\":{\"_type\":\"sanity.imageDimensions\",\"aspectRatio\":1,\"height\":1216,\"width\":1216}},\"url\":\"https://cdn.sanity.io/images/2hv88549/production/c56ff51f54a841f52801180eb268680e55d5cb8b-1216x1216.png?auto=format\"},\"darkImage\":{\"metadata\":{\"dimensions\":null},\"url\":null},\"darkMobileImage\":{\"metadata\":{\"dimensions\":null},\"url\":null},\"mobileImage\":{\"metadata\":{\"dimensions\":null},\"url\":null}},\"mediaBg\":{\"_type\":\"image\",\"asset\":{\"metadata\":{\"dimensions\":{\"_type\":\"sanity.imageDimensions\",\"aspectRatio\":1.3331896551724138,\"height\":2320,\"width\":3093},\"isLandscape\":true,\"isPortrait\":false,\"isSquare\":false},\"url\":\"https://cdn.sanity.io/images/2hv88549/production/7fe2629468a1b9598c66a47f5a8d6fbdc946f0b4-3093x2320.jpg?auto=format\"},\"darkImage\":{\"metadata\":{\"dimensions\":null,\"isLandscape\":true,\"isPortrait\":false,\"isSquare\":false},\"url\":null},\"darkMobileImage\":{\"metadata\":{\"dimensions\":null},\"url\":null},\"isWallpaper\":true,\"mobileImage\":{\"metadata\":{\"dimensions\":null},\"url\":null}},\"mediaBgColor\":\"card-02-hex\",\"mediaType\":\"reactComponent\",\"reactComponent\":{\"_id\":\"329b8904-94ac-4fca-8094-32b3040e463b\",\"description\":null,\"markdownContent\":\"\u003cDemoDesktop disableInteractions innerPaddingPx={24} aspectRatio={{ base: \\\"1/1\\\", md: \\\"1/1\\\" }} height=\\\"378px\\\"\\n windows={[\\n {\\n id: \\\"agent-ide\\\",\\n title: \\\"\\\",\\n x: 50,\\n y: 50, widthPx: 680, heightPx: 330, \\n zIndex: 10, shadowless: true,\\n content: (\\n \u003cCursorAgentCLI\\n scenario=\\\"startupAnalytics\\\"\\n embedded\\n streamInitialMessages={false}\\n title=\\\"cursor-agent\\\"\\n disableInteractions\\n /\u003e\\n ),\\n },\\n ]}\\n /\u003e\",\"title\":\"Close Up: CLI\"},\"title\":\"CLI\",\"video\":null},{\"_key\":\"c0190915193e\",\"_type\":\"featureCard\",\"body\":[{\"_key\":\"13bf151c355f\",\"_type\":\"block\",\"children\":[{\"_key\":\"fdb5fc030b06\",\"_type\":\"span\",\"marks\":[],\"text\":\"Identify issues, fix in one click.\"}],\"markDefs\":[],\"style\":\"normal\"}],\"link\":{\"file\":null,\"href\":\"/bugbot\",\"label\":\"Learn more\",\"linkType\":\"page\",\"openInNewTab\":false,\"page\":\"bugbot\",\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"media\":{\"_type\":\"image\",\"asset\":{\"metadata\":{\"dimensions\":{\"_type\":\"sanity.imageDimensions\",\"aspectRatio\":1,\"height\":1216,\"width\":1216}},\"url\":\"https://cdn.sanity.io/images/2hv88549/production/c56ff51f54a841f52801180eb268680e55d5cb8b-1216x1216.png?auto=format\"},\"darkImage\":{\"metadata\":{\"dimensions\":null},\"url\":null},\"darkMobileImage\":{\"metadata\":{\"dimensions\":null},\"url\":null},\"mobileImage\":{\"metadata\":{\"dimensions\":null},\"url\":null}},\"mediaBg\":{\"_type\":\"image\",\"asset\":{\"metadata\":{\"dimensions\":{\"_type\":\"sanity.imageDimensions\",\"aspectRatio\":1.7777777777777777,\"height\":1440,\"width\":2560},\"isLandscape\":true,\"isPortrait\":false,\"isSquare\":false},\"url\":\"https://cdn.sanity.io/images/2hv88549/production/e6375f83012e4a76ff15411ce87937362c411153-2560x1440.png?auto=format\"},\"darkImage\":{\"metadata\":{\"dimensions\":{\"_type\":\"sanity.imageDimensions\",\"aspectRatio\":1.7777777777777777,\"height\":1440,\"width\":2560},\"isLandscape\":true,\"isPortrait\":false,\"isSquare\":false},\"url\":\"https://cdn.sanity.io/images/2hv88549/production/3cb319c263fd5a76115b6196b916ce8767daec13-2560x1440.png?auto=format\"},\"darkMobileImage\":{\"metadata\":{\"dimensions\":null},\"url\":null},\"isWallpaper\":true,\"mobileImage\":{\"metadata\":{\"dimensions\":null},\"url\":null}},\"mediaBgColor\":\"card-02-hex\",\"mediaType\":\"reactComponent\",\"reactComponent\":{\"_id\":\"67d9ed66-5b71-4e9a-a087-a54ecb0d7a13\",\"description\":null,\"markdownContent\":\"\u003cDemoDesktop disableInteractions innerPaddingPx={24} aspectRatio={{ base: \\\"1/1\\\", md: \\\"1/1\\\" }} height=\\\"378px\\\"\\n windows={[\\n {\\n id: \\\"github-pr\\\",\\n title: \\\"GitHub Pull Request\\\", shadowless: true,\\n x: 50,\\n y: 50, widthPx: 680, heightPx: 330,\\n zIndex: 20,\\n ctaHref: \\\"https://cursor.com/docs/bugbot\\\",\\n ctaLabel: \\\"Get Bugbot\\\",\\n content: \u003cGitHubUI /\u003e\\n }\\n ]}\\n/\u003e\",\"title\":\"Close Up: GitHub BugBot\"},\"title\":\"Review with Bugbot\",\"video\":null}],\"infoCards\":null,\"mediaCards\":null,\"textSize\":\"md\"}],\"textSize\":\"md\"},{\"_key\":\"14ca7f7b0120\",\"_type\":\"multicard\",\"alignment\":\"left\",\"banner\":{\"_type\":\"object\",\"linkType\":\"none\",\"openInNewTab\":false},\"body\":[{\"_key\":\"f02c35cdb1f2\",\"_type\":\"block\",\"children\":[{\"_key\":\"6b75b48da6cb\",\"_type\":\"span\",\"marks\":[],\"text\":\"Configure Cursor so you can do your best work.\"}],\"markDefs\":[],\"style\":\"normal\"}],\"cardType\":\"feature\",\"columns\":\"4\",\"cta\":null,\"featureCards\":[{\"_key\":\"051591b4d346\",\"_type\":\"featureCard\",\"body\":[{\"_key\":\"1693470d43a1\",\"_type\":\"block\",\"children\":[{\"_key\":\"5c4e424d5e3d\",\"_type\":\"span\",\"text\":\"Import extensions, themes, and keybindings directly from VS Code.\"}],\"markDefs\":null,\"style\":\"normal\"}],\"link\":\"$40:props:children:props:children:1:props:children:0:props:children:props:link\",\"media\":{\"_type\":\"image\",\"asset\":{\"metadata\":{\"dimensions\":{\"_type\":\"sanity.imageDimensions\",\"aspectRatio\":1,\"height\":640,\"width\":640}},\"url\":\"https://cdn.sanity.io/images/2hv88549/production/a8b297cd3acacb16cb9e90eb6c4f3d1d3c3033bc-640x640.png?auto=format\"},\"darkImage\":{\"metadata\":{\"dimensions\":null},\"url\":null},\"darkMobileImage\":{\"metadata\":{\"dimensions\":null},\"url\":null},\"mobileImage\":{\"metadata\":{\"dimensions\":null},\"url\":null}},\"mediaBg\":{\"_type\":\"image\",\"asset\":{\"metadata\":{\"dimensions\":{\"_type\":\"sanity.imageDimensions\",\"aspectRatio\":1,\"height\":4000,\"width\":4000},\"isLandscape\":false,\"isPortrait\":false,\"isSquare\":true},\"url\":\"https://cdn.sanity.io/images/2hv88549/production/dd6ee0f449998ddf62957db43119c6b9ffed08c4-4000x4000.png?auto=format\"},\"darkImage\":{\"metadata\":{\"dimensions\":{\"_type\":\"sanity.imageDimensions\",\"aspectRatio\":1,\"height\":4000,\"width\":4000},\"isLandscape\":false,\"isPortrait\":false,\"isSquare\":true},\"url\":\"https://cdn.sanity.io/images/2hv88549/production/2c212bed9ff04a532c2a51d8381e368887ae24e3-4000x4000.png?auto=format\"},\"darkMobileImage\":{\"metadata\":{\"dimensions\":null},\"url\":null},\"isWallpaper\":null,\"mobileImage\":{\"metadata\":{\"dimensions\":null},\"url\":null}},\"mediaType\":\"image\",\"reactComponent\":null,\"title\":\"1-click import\",\"video\":null},{\"_key\":\"e9afade42e7c\",\"_type\":\"featureCard\",\"body\":[{\"_key\":\"6093ea0cca2f\",\"_type\":\"block\",\"children\":[{\"_key\":\"ba67cfe2760f\",\"_type\":\"span\",\"text\":\"Connect external tools and data sources directly to Cursor.\"}],\"markDefs\":null,\"style\":\"normal\"}],\"link\":\"$40:props:children:props:children:1:props:children:1:props:children:props:link\",\"media\":{\"_type\":\"image\",\"asset\":{\"metadata\":{\"dimensions\":{\"_type\":\"sanity.imageDimensions\",\"aspectRatio\":1,\"height\":640,\"width\":640}},\"url\":\"https://cdn.sanity.io/images/2hv88549/production/a8b297cd3acacb16cb9e90eb6c4f3d1d3c3033bc-640x640.png?auto=format\"},\"darkImage\":{\"metadata\":{\"dimensions\":null},\"url\":null},\"darkMobileImage\":{\"metadata\":{\"dimensions\":null},\"url\":null},\"mobileImage\":{\"metadata\":{\"dimensions\":null},\"url\":null}},\"mediaBg\":{\"_type\":\"image\",\"asset\":{\"metadata\":{\"dimensions\":{\"_type\":\"sanity.imageDimensions\",\"aspectRatio\":1,\"height\":3200,\"width\":3200},\"isLandscape\":false,\"isPortrait\":false,\"isSquare\":true},\"url\":\"https://cdn.sanity.io/images/2hv88549/production/50ffc1473a64f3b6ee74395e0eb9b49425599b7b-3200x3200.png?auto=format\"},\"darkImage\":{\"metadata\":{\"dimensions\":{\"_type\":\"sanity.imageDimensions\",\"aspectRatio\":1,\"height\":3200,\"width\":3200},\"isLandscape\":false,\"isPortrait\":false,\"isSquare\":true},\"url\":\"https://cdn.sanity.io/images/2hv88549/production/3ac50061b7174e3b941ee15c0090b00aabdf0356-3200x3200.png?auto=format\"},\"darkMobileImage\":{\"metadata\":{\"dimensions\":null},\"url\":null},\"isWallpaper\":null,\"mobileImage\":{\"metadata\":{\"dimensions\":null},\"url\":null}},\"mediaBgColor\":\"card-03-hex\",\"mediaType\":\"image\",\"reactComponent\":null,\"title\":\"MCP servers\",\"video\":null},{\"_key\":\"2a745dbd1f7d\",\"_type\":\"featureCard\",\"body\":[{\"_key\":\"e47752e5fa1c\",\"_type\":\"block\",\"children\":[{\"_key\":\"7f290e39cf49\",\"_type\":\"span\",\"text\":\"Customize how the models behave with reusable, scoped instructions.\"}],\"markDefs\":null,\"style\":\"normal\"}],\"link\":\"$40:props:children:props:children:1:props:children:2:props:children:props:link\",\"media\":{\"_type\":\"image\",\"asset\":{\"metadata\":{\"dimensions\":{\"_type\":\"sanity.imageDimensions\",\"aspectRatio\":1.7763157894736843,\"height\":1216,\"width\":2160}},\"url\":\"https://cdn.sanity.io/images/2hv88549/production/ae6b13f8c39a97dacc9808ecd38c307e8baf5ce4-2160x1216.png?auto=format\"},\"darkImage\":{\"metadata\":{\"dimensions\":null},\"url\":null},\"darkMobileImage\":{\"metadata\":{\"dimensions\":null},\"url\":null},\"mobileImage\":{\"metadata\":{\"dimensions\":null},\"url\":null}},\"mediaBg\":null,\"mediaType\":\"reactComponent\",\"reactComponent\":{\"_id\":\"6189e87a-5700-4375-a427-fe0281acf0b7\",\"description\":null,\"markdownContent\":\"\u003cCursorRules \\n aspectRatio={{ base: \\\"1/1\\\" }}\\n foregroundMaxWidth={680}\\n showSidebar={false}\\n showChat={false}\\n showAgents={false}\\n position=\\\"bottom-right\\\"\\n disableSyntaxHighlighting={true}\\n/\u003e\",\"title\":\"Close Up: Cursor Rules\"},\"title\":\"Rules \u0026 memories \",\"video\":null},{\"_key\":\"1c17851aa29c\",\"_type\":\"featureCard\",\"body\":[{\"_key\":\"019cb354f3b7\",\"_type\":\"block\",\"children\":[{\"_key\":\"a595a4d53456\",\"_type\":\"span\",\"text\":\"Use and manage reusable prompts within your team.\"}],\"markDefs\":null,\"style\":\"normal\"}],\"link\":\"$40:props:children:props:children:1:props:children:3:props:children:props:link\",\"media\":null,\"mediaBg\":null,\"mediaPosition\":\"bottomRight\",\"mediaType\":\"reactComponent\",\"reactComponent\":{\"_id\":\"f8e57aad-0b5d-4384-93c9-72c27e42fe92\",\"description\":null,\"markdownContent\":\"\u003cCustomCommands aspectRatio=\\\"1/1\\\" /\u003e\",\"title\":\"Close up: Cursor commands\"},\"title\":\"Custom commands\",\"video\":null}],\"infoCards\":null,\"mediaCards\":null,\"textSize\":\"md\",\"title\":\"Powerful, yet flexible\"},\"$41:props:block\",{\"_key\":\"235962ca1836\",\"_type\":\"headline\",\"alignment\":\"center\",\"banner\":{\"actionLabel\":null,\"file\":null,\"href\":null,\"label\":null,\"linkType\":\"none\",\"openInNewTab\":false,\"page\":null,\"post\":null,\"simplePage\":null},\"body\":null,\"cta\":[{\"copyButton\":null,\"downloadButton\":true,\"icon\":\"download\",\"link\":{\"_type\":\"link\",\"file\":null,\"href\":\"/download\",\"label\":\"Download\",\"linkType\":\"href\",\"openInNewTab\":false,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"size\":\"regular\",\"tracking\":null,\"variant\":\"primary\"}],\"textSize\":\"2xl\",\"title\":\"Try Cursor now.\"}],\"seoDescription\":null,\"seoKeywords\":null,\"seoTitle\":null,\"slug\":{\"_type\":\"slug\",\"current\":\"features\"},\"socialDescription\":null,\"socialImage\":null,\"socialTitle\":null,\"title\":\"Features\",\"translationMetadata\":{\"_id\":\"translation.metadata.acd960f1-42cb-4fbc-b238-3298c7b4c4ca\",\"translations\":[{\"_key\":\"en-US\",\"document\":{\"_id\":\"acd960f1-42cb-4fbc-b238-3298c7b4c4ca\",\"_type\":\"page\",\"language\":null,\"slug\":\"features\"}},{\"_key\":\"cn\",\"document\":{\"_id\":\"oX2XKfmSw1CZillJfEmByi\",\"_type\":\"page\",\"language\":\"cn\",\"slug\":\"features\"}},{\"_key\":\"ja\",\"document\":{\"_id\":\"K4tdAJtZqIlru9YbYLgyJy\",\"_type\":\"page\",\"language\":\"ja\",\"slug\":\"features\"}},{\"_key\":\"zh-Hant\",\"document\":{\"_id\":\"6kg5k912uqjIhBQgl8agDr\",\"_type\":\"page\",\"language\":\"zh-Hant\",\"slug\":\"features\"}}]}},\"isLast\":\"$undefined\",\"nextBlock\":\"$undefined\"}]\n"])

self.__next_f.push([1,"42:[\"$\",\"section\",\"235962ca1836,235962ca1836\",{\"className\":\"section bg-theme-bg text-theme-text section--headline\",\"id\":\"$undefined\",\"data-sanity\":\"id=acd960f1-42cb-4fbc-b238-3298c7b4c4ca;type=page;path=pageBuilder:235962ca1836;base=http%3A%2F%2Flocalhost%3A3333\",\"children\":[\"$\",\"div\",null,{\"className\":\"container\",\"children\":[\"$\",\"div\",null,{\"className\":\"text-center mx-auto max-w-prose-medium-wide\",\"children\":[false,\"$undefined\",[\"$\",\"h2\",null,{\"className\":\"type-xl sm:type-2xl text-balance mx-auto mb-v1\",\"children\":\"Try Cursor now.\"}],null,[\"$\",\"div\",null,{\"className\":\"gap-x-g1 flex items-center justify-center \",\"children\":[[\"$\",\"$L43\",\"0\",{\"variant\":\"btn\",\"size\":\"regular\"}]]}]]}]}]}]\n"])

self.__next_f.push([1,"44:[\"$\",\"section\",\"9f14b7d65a6e,9f14b7d65a6e\",{\"className\":\"section bg-theme-bg text-theme-text section--flush\",\"id\":\"agent\",\"data-sanity\":\"id=acd960f1-42cb-4fbc-b238-3298c7b4c4ca;type=page;path=pageBuilder:9f14b7d65a6e;base=http%3A%2F%2Flocalhost%3A3333\",\"children\":[\"$\",\"div\",null,{\"className\":\"container mb-g1\",\"children\":[\"$\",\"div\",null,{\"className\":\"grid grid-rows-[auto_1fr]\",\"children\":[[\"$\",\"div\",null,{\"className\":\"card card--large grid-cursor col-span-full row-span-full gap-y-0 max-lg:grid-rows-subgrid\",\"children\":[[\"$\",\"div\",null,{\"className\":\"col-span-full row-start-1 row-end-2 grid lg:row-start-1 lg:row-end-3 lg:items-center lg:col-start-1 lg:col-end-9 lg:pl-g0.25 lg:pr-g3\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-prose w-full lg:justify-self-start\",\"children\":[[\"$\",\"div\",null,{\"className\":\"type-base grow-1\",\"children\":[[\"$\",\"h3\",null,{\"className\":\"type-base md:type-md text-pretty\",\"children\":\"Agent\"}],[\"$\",\"div\",null,{\"className\":\"type-base md:type-md text-theme-text-sec text-pretty\",\"children\":[[\"$\",\"p\",\"9f15504e70fc\",{\"children\":[\"Delegate coding tasks so you can focus on higher-level direction.\"]}]]}],null]}],false]}]}],[\"$\",\"div\",null,{\"className\":\"max-lg:mt-g1.75 col-span-full row-start-2 row-end-3 grid cursor-default items-end lg:row-start-1 lg:row-end-3 lg:items-center lg:col-start-9 lg:col-end-25\"}]]}],[\"$\",\"div\",null,{\"className\":\"grid-cursor p-g1.75 col-span-full row-span-full gap-y-0 max-lg:grid-rows-subgrid\",\"children\":[[\"$\",\"div\",null,{\"className\":\"col-span-full row-start-1 row-end-2 grid lg:row-start-1 lg:row-end-2 lg:items-center lg:col-start-1 lg:col-end-9 lg:pl-g0.25 lg:pr-g3\"}],[\"$\",\"div\",null,{\"className\":\"max-lg:pt-v1 col-span-full row-start-2 row-end-3 grid cursor-default items-end lg:row-start-2 lg:row-end-3 lg:items-center lg:col-start-9 lg:col-end-25\",\"children\":\"$L50\"}]]}]]}]}]}]\n"])

self.__next_f.push([1,"45:[\"$\",\"section\",\"6c011cb33f14,6c011cb33f14\",{\"className\":\"section bg-theme-bg text-theme-text section--flush\",\"id\":\"=\",\"data-sanity\":\"id=acd960f1-42cb-4fbc-b238-3298c7b4c4ca;type=page;path=pageBuilder:6c011cb33f14;base=http%3A%2F%2Flocalhost%3A3333\",\"children\":[\"$\",\"div\",null,{\"className\":\"container my-v2\",\"children\":[false,[\"$\",\"div\",null,{\"className\":\"grid gap-g1 grid-cols-1 md:grid-cols-2 lg:grid-cols-3 items-stretch mb-g1\",\"children\":[[\"$\",\"div\",\"card-0\",{\"className\":\"undefined h-full\",\"children\":[\"$\",\"div\",null,{\"className\":\"card flex h-full grow-1 flex-col\",\"children\":\"$L51\"}]}],[\"$\",\"div\",\"card-1\",{\"className\":\"undefined h-full\",\"children\":[\"$\",\"div\",null,{\"className\":\"card flex h-full grow-1 flex-col\",\"children\":\"$L52\"}]}],[\"$\",\"div\",\"card-2\",{\"className\":\"undefined h-full\",\"children\":[\"$\",\"div\",null,{\"className\":\"card flex h-full grow-1 flex-col\",\"children\":\"$L53\"}]}]]}]]}]}]\n"])

self.__next_f.push([1,"46:[\"$\",\"section\",\"9f14b7d65a6e,9f14b7d65a6e\",{\"className\":\"section bg-theme-bg text-theme-text section--flush\",\"id\":\"tab\",\"data-sanity\":\"id=acd960f1-42cb-4fbc-b238-3298c7b4c4ca;type=page;path=pageBuilder:9f14b7d65a6e;base=http%3A%2F%2Flocalhost%3A3333\",\"children\":[\"$\",\"div\",null,{\"className\":\"container mb-g1\",\"children\":[\"$\",\"div\",null,{\"className\":\"grid grid-rows-[auto_1fr]\",\"children\":[[\"$\",\"div\",null,{\"className\":\"card card--large grid-cursor col-span-full row-span-full gap-y-0 max-lg:grid-rows-subgrid\",\"children\":[[\"$\",\"div\",null,{\"className\":\"col-span-full row-start-1 row-end-2 grid lg:row-start-1 lg:row-end-3 lg:items-center lg:col-start-1 lg:col-end-9 lg:pl-g0.25 lg:pr-g3\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-prose w-full lg:justify-self-start\",\"children\":[[\"$\",\"div\",null,{\"className\":\"type-base grow-1\",\"children\":[[\"$\",\"h3\",null,{\"className\":\"type-base md:type-md text-pretty\",\"children\":\"Tab\"}],[\"$\",\"div\",null,{\"className\":\"type-base md:type-md text-theme-text-sec text-pretty\",\"children\":[[\"$\",\"p\",\"d6ba645f3ed8\",{\"children\":[\"Our custom autocomplete model predicts your next actions.\"]}]]}],null]}],false]}]}],[\"$\",\"div\",null,{\"className\":\"max-lg:mt-g1.75 col-span-full row-start-2 row-end-3 grid cursor-default items-end lg:row-start-1 lg:row-end-3 lg:items-center lg:col-start-9 lg:col-end-25\"}]]}],[\"$\",\"div\",null,{\"className\":\"grid-cursor p-g1.75 col-span-full row-span-full gap-y-0 max-lg:grid-rows-subgrid\",\"children\":[[\"$\",\"div\",null,{\"className\":\"col-span-full row-start-1 row-end-2 grid lg:row-start-1 lg:row-end-2 lg:items-center lg:col-start-1 lg:col-end-9 lg:pl-g0.25 lg:pr-g3\"}],[\"$\",\"div\",null,{\"className\":\"max-lg:pt-v1 col-span-full row-start-2 row-end-3 grid cursor-default items-end lg:row-start-2 lg:row-end-3 lg:items-center lg:col-start-9 lg:col-end-25\",\"children\":\"$L54\"}]]}]]}]}]}]\n"])

self.__next_f.push([1,"47:[\"$\",\"section\",\"6c011cb33f14,6c011cb33f14\",{\"className\":\"section bg-theme-bg text-theme-text section--flush\",\"id\":\"=\",\"data-sanity\":\"id=acd960f1-42cb-4fbc-b238-3298c7b4c4ca;type=page;path=pageBuilder:6c011cb33f14;base=http%3A%2F%2Flocalhost%3A3333\",\"children\":[\"$\",\"div\",null,{\"className\":\"container my-v2\",\"children\":[false,[\"$\",\"div\",null,{\"className\":\"grid gap-g1 grid-cols-1 md:grid-cols-2 lg:grid-cols-3 items-stretch mb-g1\",\"children\":[[\"$\",\"div\",\"card-0\",{\"className\":\"undefined h-full\",\"children\":[\"$\",\"div\",null,{\"className\":\"card flex h-full grow-1 flex-col\",\"children\":\"$L55\"}]}],[\"$\",\"div\",\"card-1\",{\"className\":\"undefined h-full\",\"children\":[\"$\",\"div\",null,{\"className\":\"card flex h-full grow-1 flex-col\",\"children\":\"$L56\"}]}],[\"$\",\"div\",\"card-2\",{\"className\":\"undefined h-full\",\"children\":[\"$\",\"div\",null,{\"className\":\"card flex h-full grow-1 flex-col\",\"children\":\"$L57\"}]}]]}]]}]}]\n"])

self.__next_f.push([1,"48:[\"$\",\"section\",\"b59b0619eff7,b59b0619eff7\",{\"className\":\"section bg-theme-bg text-theme-text section--flush\",\"id\":\"ecosystem\",\"data-sanity\":\"id=acd960f1-42cb-4fbc-b238-3298c7b4c4ca;type=page;path=pageBuilder:b59b0619eff7;base=http%3A%2F%2Flocalhost%3A3333\",\"children\":[\"$\",\"div\",null,{\"className\":\"container mb-g1\",\"children\":[\"$\",\"div\",null,{\"className\":\"grid grid-rows-[auto_1fr]\",\"children\":[[\"$\",\"$L2b\",null,{\"link\":\"$41:props:pageData:pageBuilder:3:sectionBuilder:0:link\",\"className\":\"card card--large card--feature grid-cursor col-span-full row-span-full gap-y-0 max-lg:grid-rows-subgrid\",\"children\":[[\"$\",\"div\",null,{\"className\":\"col-span-full row-start-1 row-end-2 grid lg:row-start-1 lg:row-end-3 lg:items-center lg:col-start-1 lg:col-end-9 lg:pl-g0.25 lg:pr-g3\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-prose w-full lg:justify-self-start\",\"children\":[[\"$\",\"div\",null,{\"className\":\"type-base grow-1\",\"children\":[[\"$\",\"h3\",null,{\"className\":\"type-base md:type-md text-pretty\",\"children\":\"Across the entire development process\"}],[\"$\",\"div\",null,{\"className\":\"type-base md:type-md text-theme-text-sec text-pretty\",\"children\":[[\"$\",\"p\",\"4df8eb57dc30\",{\"children\":[\"Start tasks from Slack, issue tracker, mobile and more. Finish off in the IDE.\"]}]]}],null]}],[\"$\",\"div\",null,{\"className\":\"mt-v8/12\",\"children\":[\"$\",\"span\",null,{\"className\":\"btn-tertiary\",\"children\":\"Try agents on web and mobile ‚Üó\"}]}]]}]}],[\"$\",\"div\",null,{\"className\":\"max-lg:mt-g1.75 col-span-full row-start-2 row-end-3 grid cursor-default items-end lg:row-start-1 lg:row-end-3 lg:items-center lg:col-start-9 lg:col-end-25\"}]]}],[\"$\",\"div\",null,{\"className\":\"grid-cursor p-g1.75 col-span-full row-span-full gap-y-0 max-lg:grid-rows-subgrid\",\"children\":[[\"$\",\"div\",null,{\"className\":\"col-span-full row-start-1 row-end-2 grid lg:row-start-1 lg:row-end-2 lg:items-center lg:col-start-1 lg:col-end-9 lg:pl-g0.25 lg:pr-g3\"}],[\"$\",\"div\",null,{\"className\":\"max-lg:pt-v1 col-span-full row-start-2 row-end-3 grid cursor-default items-end lg:row-start-2 lg:row-end-3 lg:items-center lg:col-start-9 lg:col-end-25\",\"children\":\"$L58\"}]]}]]}]}]}]\n"])

self.__next_f.push([1,"49:[\"$\",\"section\",\"5431cb0a0703,5431cb0a0703\",{\"className\":\"section bg-theme-bg text-theme-text section--flush\",\"id\":\"$undefined\",\"data-sanity\":\"id=acd960f1-42cb-4fbc-b238-3298c7b4c4ca;type=page;path=pageBuilder:5431cb0a0703;base=http%3A%2F%2Flocalhost%3A3333\",\"children\":[\"$\",\"div\",null,{\"className\":\"container my-v2\",\"children\":[false,[\"$\",\"div\",null,{\"className\":\"grid gap-g1 grid-cols-1 md:grid-cols-2 lg:grid-cols-3 items-stretch mb-g1\",\"children\":[[\"$\",\"div\",\"card-0\",{\"className\":\"undefined h-full\",\"children\":[\"$\",\"$L2b\",null,{\"link\":\"$41:props:pageData:pageBuilder:3:sectionBuilder:1:featureCards:0:link\",\"className\":\"card card--feature flex h-full grow-1 flex-col\",\"children\":\"$L59\"}]}],[\"$\",\"div\",\"card-1\",{\"className\":\"undefined h-full\",\"children\":[\"$\",\"$L2b\",null,{\"link\":\"$41:props:pageData:pageBuilder:3:sectionBuilder:1:featureCards:1:link\",\"className\":\"card card--feature flex h-full grow-1 flex-col\",\"children\":\"$L5a\"}]}],[\"$\",\"div\",\"card-2\",{\"className\":\"undefined h-full\",\"children\":[\"$\",\"$L2b\",null,{\"link\":\"$41:props:pageData:pageBuilder:3:sectionBuilder:1:featureCards:2:link\",\"className\":\"card card--feature flex h-full grow-1 flex-col\",\"children\":\"$L5b\"}]}]]}]]}]}]\n"])

self.__next_f.push([1,"50:[\"$\",\"div\",null,{\"className\":\"media-border-container relative grid grid-cols-1 grid-rows-1 bg-[image:var(--color-theme-card-03)]\",\"children\":[[\"$\",\"div\",null,{\"className\":\"relative z-1 col-span-full row-span-full overflow-hidden\",\"children\":[[\"$\",\"picture\",null,{\"className\":\"absolute inset-0 scale-[1.1] transform wallpaper-brightness-dark\",\"children\":[[\"$\",\"source\",null,{\"media\":\"(min-width: 901px)\",\"srcSet\":\"$60\"}],\"$undefined\",[\"$\",\"img\",null,{\"className\":\"object-cover h-full w-full\",\"alt\":\"\",\"src\":\"https://cdn.sanity.io/images/2hv88549/production/1ffde036387b7242c29496bd7b1009f2218bce43-3266x2324.jpg?auto=format\",\"loading\":\"lazy\",\"decoding\":\"async\",\"width\":\"$undefined\",\"height\":\"$undefined\",\"fetchPriority\":\"auto\"}]]}],\"$undefined\"]}],[\"$\",\"div\",null,{\"className\":\"z-20 col-span-full row-span-full\",\"children\":\"$L61\"}],false,false]}]\n"])

self.__next_f.push([1,"54:[\"$\",\"div\",null,{\"className\":\"media-border-container relative grid grid-cols-1 grid-rows-1 bg-[image:var(--color-theme-card-03)]\",\"children\":[[\"$\",\"div\",null,{\"className\":\"relative z-1 col-span-full row-span-full overflow-hidden\",\"children\":[[\"$\",\"picture\",null,{\"className\":\"absolute inset-0 scale-[1.1] transform wallpaper-brightness-dark\",\"children\":[[\"$\",\"source\",null,{\"media\":\"(min-width: 901px)\",\"srcSet\":\"$62\"}],\"$undefined\",[\"$\",\"img\",null,{\"className\":\"object-cover h-full w-full\",\"alt\":\"\",\"src\":\"https://cdn.sanity.io/images/2hv88549/production/6a23c94721e22f5c31f2ef72ccd7cdf9fecd9e12-1995x1330.jpg?auto=format\",\"loading\":\"lazy\",\"decoding\":\"async\",\"width\":\"$undefined\",\"height\":\"$undefined\",\"fetchPriority\":\"auto\"}]]}],\"$undefined\"]}],[\"$\",\"div\",null,{\"className\":\"z-20 col-span-full row-span-full\",\"children\":\"$L63\"}],false,false]}]\n"])

self.__next_f.push([1,"58:[\"$\",\"div\",null,{\"className\":\"media-border-container relative grid grid-cols-1 grid-rows-1 bg-[image:var(--color-theme-card-03)]\",\"children\":[[\"$\",\"div\",null,{\"className\":\"relative z-1 col-span-full row-span-full overflow-hidden\",\"children\":[[\"$\",\"picture\",null,{\"className\":\"absolute inset-0 scale-[1.1] transform wallpaper-brightness-dark\",\"children\":[[\"$\",\"source\",null,{\"media\":\"(min-width: 901px)\",\"srcSet\":\"$64\"}],\"$undefined\",[\"$\",\"img\",null,{\"className\":\"object-cover h-full w-full\",\"alt\":\"\",\"src\":\"https://cdn.sanity.io/images/2hv88549/production/00a586c62c8782e65c0affe6363a43ed6bdbc1fd-3139x2093.jpg?auto=format\",\"loading\":\"lazy\",\"decoding\":\"async\",\"width\":\"$undefined\",\"height\":\"$undefined\",\"fetchPriority\":\"auto\"}]]}],\"$undefined\"]}],[\"$\",\"div\",null,{\"className\":\"z-20 col-span-full row-span-full\",\"children\":\"$L65\"}],false,false]}]\n"])

self.__next_f.push([1,"5c:[\"$\",\"div\",null,{\"className\":\"media-border-container relative grid grid-cols-1 grid-rows-1 bg-[image:var(--color-theme-card-03)]\",\"children\":[[\"$\",\"div\",null,{\"className\":\"relative z-1 col-span-full row-span-full overflow-hidden\",\"children\":[[\"$\",\"picture\",null,{\"className\":\"absolute inset-0 media-light\",\"children\":[[\"$\",\"source\",null,{\"media\":\"(min-width: 901px)\",\"srcSet\":\"$66\"}],\"$undefined\",[\"$\",\"img\",null,{\"className\":\"object-cover h-full w-full\",\"alt\":\"\",\"src\":\"https://cdn.sanity.io/images/2hv88549/production/dd6ee0f449998ddf62957db43119c6b9ffed08c4-4000x4000.png?auto=format\",\"loading\":\"lazy\",\"decoding\":\"async\",\"width\":\"$undefined\",\"height\":\"$undefined\",\"fetchPriority\":\"auto\"}]]}],[\"$\",\"picture\",null,{\"className\":\"absolute inset-0 media-dark\",\"children\":[[\"$\",\"source\",null,{\"media\":\"(min-width: 901px)\",\"srcSet\":\"$67\"}],\"$undefined\",\"$L68\"]}]]}],null,\"$L69\",false]}]\n"])

self.__next_f.push([1,"5d:[\"$\",\"div\",null,{\"className\":\"media-border-container relative grid grid-cols-1 grid-rows-1 bg-theme-card-03-hex\",\"children\":[[\"$\",\"div\",null,{\"className\":\"relative z-1 col-span-full row-span-full overflow-hidden\",\"children\":[[\"$\",\"picture\",null,{\"className\":\"absolute inset-0 media-light\",\"children\":[[\"$\",\"source\",null,{\"media\":\"(min-width: 901px)\",\"srcSet\":\"$6a\"}],\"$undefined\",[\"$\",\"img\",null,{\"className\":\"object-cover h-full w-full\",\"alt\":\"\",\"src\":\"https://cdn.sanity.io/images/2hv88549/production/50ffc1473a64f3b6ee74395e0eb9b49425599b7b-3200x3200.png?auto=format\",\"loading\":\"lazy\",\"decoding\":\"async\",\"width\":\"$undefined\",\"height\":\"$undefined\",\"fetchPriority\":\"auto\"}]]}],[\"$\",\"picture\",null,{\"className\":\"absolute inset-0 media-dark\",\"children\":[[\"$\",\"source\",null,{\"media\":\"(min-width: 901px)\",\"srcSet\":\"$6b\"}],\"$undefined\",\"$L6c\"]}]]}],null,\"$L6d\",false]}]\n"])

self.__next_f.push([1,"5e:[\"$\",\"div\",null,{\"className\":\"media-border-container relative grid grid-cols-1 grid-rows-1 bg-[image:var(--color-theme-card-03)]\",\"children\":[null,[\"$\",\"div\",null,{\"className\":\"z-20 col-span-full row-span-full\",\"children\":\"$L6e\"}],false,false]}]\n5f:[\"$\",\"div\",null,{\"className\":\"media-border-container relative grid grid-cols-1 grid-rows-1 bg-[image:var(--color-theme-card-03)]\",\"children\":[null,[\"$\",\"div\",null,{\"className\":\"z-20 col-span-full row-span-full\",\"children\":\"$L6f\"}],false,false]}]\n51:[[\"$\",\"div\",null,{\"className\":\"type-base flex max-w-prose grow-1 flex-col justify-between\",\"children\":[[\"$\",\"div\",null,{\"children\":[[\"$\",\"h2\",null,{\"className\":\"$undefined\",\"children\":\"Codebase understanding\"}],[\"$\",\"div\",null,{\"className\":\"text-theme-text-sec text-pretty\",\"children\":[[\"$\",\"p\",\"d3bb720376a4\",{\"children\":[\"Cursor‚Äôs codebase embedding model gives Agent deep understanding and recall.\"]}]]}]]}],false]}],[\"$\",\"figure\",null,{\"className\":\"pt-g1.75\",\"children\":\"$L70\"}]]\n52:[[\"$\",\"div\",null,{\"className\":\"type-base flex max-w-prose grow-1 flex-col justify-between\",\"children\":[[\"$\",\"div\",null,{\"children\":[[\"$\",\"h2\",null,{\"className\":\"$undefined\",\"children\":\"Top model access\"}],[\"$\",\"div\",null,{\"className\":\"text-theme-text-sec text-pretty\",\"children\":[[\"$\",\"p\",\"1102dba46986\",{\"children\":[\"Choose freely between frontier models from OpenAI, Anthropic, Gemini, and xAI.\"]}]]}]]}],false]}],[\"$\",\"figure\",null,{\"className\":\"pt-g1.75\",\"children\":\"$L71\"}]]\n53:[[\"$\",\"div\",null,{\"className\":\"type-base flex max-w-prose grow-1 flex-col justify-between\",\"children\":[[\"$\",\"div\",null,{\"children\":[[\"$\",\"h2\",null,{\"className\":\"$undefined\",\"children\":\"Scoped changes\"}],[\"$\",\"div\",null,{\"className\":\"text-theme-text-sec text-pretty\",\"children\":[[\"$\",\"p\",\"d046f2ab3522\",{\"children\":[\"Make targeted edits or run terminal commands with natural language.\"]}]]}]]}],false]}],[\"$\",\"figure\",null,{\"className\":\"pt-g1.75\",\"children\":\"$L72\"}]]\n55:[[\"$\",\"div\",null,{\"className\":\"type-base flex max-w-prose grow-1 flex-col justify-between\",\"children\":[[\""])

self.__next_f.push([1,"$\",\"div\",null,{\"children\":[[\"$\",\"h2\",null,{\"className\":\"$undefined\",\"children\":\"Multi-line edits\"}],[\"$\",\"div\",null,{\"className\":\"text-theme-text-sec text-pretty\",\"children\":[[\"$\",\"p\",\"ce25153fe594\",{\"children\":[\"Get suggested edits across multiple lines.\"]}]]}]]}],false]}],[\"$\",\"figure\",null,{\"className\":\"pt-g1.75\",\"children\":\"$L73\"}]]\n56:[[\"$\",\"div\",null,{\"className\":\"type-base flex max-w-prose grow-1 flex-col justify-between\",\"children\":[[\"$\",\"div\",null,{\"children\":[[\"$\",\"h2\",null,{\"className\":\"$undefined\",\"children\":\"Smart rewrites\"}],[\"$\",\"div\",null,{\"className\":\"text-theme-text-sec text-pretty\",\"children\":[[\"$\",\"p\",\"555b3562f029\",{\"children\":[\"Type naturally, Cursor will finish your thought.\"]}]]}]]}],false]}],[\"$\",\"figure\",null,{\"className\":\"pt-g1.75\",\"children\":\"$L74\"}]]\n57:[[\"$\",\"div\",null,{\"className\":\"type-base flex max-w-prose grow-1 flex-col justify-between\",\"children\":[[\"$\",\"div\",null,{\"children\":[[\"$\",\"h2\",null,{\"className\":\"$undefined\",\"children\":\"Tab, Tab, Tab\"}],[\"$\",\"div\",null,{\"className\":\"text-theme-text-sec text-pretty\",\"children\":[[\"$\",\"p\",\"d3bb720376a4\",{\"children\":[\"Fly through edits at your cursor and across files.\"]}]]}]]}],false]}],[\"$\",\"figure\",null,{\"className\":\"pt-g1.75\",\"children\":\"$L75\"}]]\n59:[[\"$\",\"div\",null,{\"className\":\"type-base flex max-w-prose grow-1 flex-col justify-between\",\"children\":[[\"$\",\"div\",null,{\"children\":[[\"$\",\"h2\",null,{\"className\":\"$undefined\",\"children\":\"IDE\"}],[\"$\",\"div\",null,{\"className\":\"text-theme-text-sec text-pretty\",\"children\":[[\"$\",\"p\",\"0e72422c5a4c\",{\"children\":[\"Manual to agentic coding, in one familiar editor.\"]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"mt-v8/12\",\"children\":[\"$\",\"span\",null,{\"className\":\"btn-tertiary\",\"children\":\"Download ‚Üí\"}]}]]}],[\"$\",\"figure\",null,{\"className\":\"pt-g1.75\",\"children\":\"$L76\"}]]\n5a:[[\"$\",\"div\",null,{\"className\":\"type-base flex max-w-prose grow-1 flex-col justify-between\",\"children\":[[\"$\",\"div\",null,{\"children\":[[\"$\",\"h2\",null,{\"className\":\"$undefined\",\"children\":\"CLI\"}],[\"$\",\"div\",null,{\"className\":\"text-theme-text-sec "])

self.__next_f.push([1,"70:[\"$\",\"div\",null,{\"className\":\"media-border-container relative grid grid-cols-1 grid-rows-1 bg-[image:var(--color-theme-card-03)]\",\"children\":[[\"$\",\"div\",null,{\"className\":\"relative z-1 col-span-full row-span-full overflow-hidden\",\"children\":[[\"$\",\"picture\",null,{\"className\":\"absolute inset-0 scale-[1.1] transform wallpaper-brightness-dark media-light\",\"children\":[[\"$\",\"source\",null,{\"media\":\"(min-width: 901px)\",\"srcSet\":\"$7b\"}],\"$undefined\",[\"$\",\"img\",null,{\"className\":\"object-cover h-full w-full\",\"alt\":\"\",\"src\":\"https://cdn.sanity.io/images/2hv88549/production/fd9b3b4cd7d670f9f7d89ef54a9d83eedc7eb8cc-2560x1440.png?auto=format\",\"loading\":\"lazy\",\"decoding\":\"async\",\"width\":\"$undefined\",\"height\":\"$undefined\",\"fetchPriority\":\"auto\"}]]}],[\"$\",\"picture\",null,{\"className\":\"absolute inset-0 scale-[1.1] transform wallpaper-brightness-dark media-dark\",\"children\":[[\"$\",\"source\",null,{\"media\":\"(min-width: 901px)\",\"srcSet\":\"$7c\"}],\"$undefined\",\"$L7d\"]}]]}],\"$L7e\",false,false]}]\n"])

self.__next_f.push([1,"71:[\"$\",\"div\",null,{\"className\":\"media-border-container relative grid grid-cols-1 grid-rows-1 bg-[image:var(--color-theme-card-03)]\",\"children\":[[\"$\",\"div\",null,{\"className\":\"relative z-1 col-span-full row-span-full overflow-hidden\",\"children\":[[\"$\",\"picture\",null,{\"className\":\"absolute inset-0 scale-[1.1] transform wallpaper-brightness-dark media-light\",\"children\":[[\"$\",\"source\",null,{\"media\":\"(min-width: 901px)\",\"srcSet\":\"$7f\"}],\"$undefined\",[\"$\",\"img\",null,{\"className\":\"object-cover h-full w-full\",\"alt\":\"\",\"src\":\"https://cdn.sanity.io/images/2hv88549/production/0e44f2d6b59d6d3ab8ea68a301f92e079d0e7e89-2560x1440.png?auto=format\",\"loading\":\"lazy\",\"decoding\":\"async\",\"width\":\"$undefined\",\"height\":\"$undefined\",\"fetchPriority\":\"auto\"}]]}],[\"$\",\"picture\",null,{\"className\":\"absolute inset-0 scale-[1.1] transform wallpaper-brightness-dark media-dark\",\"children\":[[\"$\",\"source\",null,{\"media\":\"(min-width: 901px)\",\"srcSet\":\"$80\"}],\"$undefined\",\"$L81\"]}]]}],\"$L82\",false,false]}]\n"])

self.__next_f.push([1,"72:[\"$\",\"div\",null,{\"className\":\"media-border-container relative grid grid-cols-1 grid-rows-1 bg-[image:var(--color-theme-card-03)]\",\"children\":[[\"$\",\"div\",null,{\"className\":\"relative z-1 col-span-full row-span-full overflow-hidden\",\"children\":[[\"$\",\"picture\",null,{\"className\":\"absolute inset-0 scale-[1.1] transform wallpaper-brightness-dark media-light\",\"children\":[[\"$\",\"source\",null,{\"media\":\"(min-width: 901px)\",\"srcSet\":\"$83\"}],\"$undefined\",[\"$\",\"img\",null,{\"className\":\"object-cover h-full w-full\",\"alt\":\"\",\"src\":\"https://cdn.sanity.io/images/2hv88549/production/214fe44b44ebb3a38c3c7e6bde246a032d1bdc57-2560x1440.png?auto=format\",\"loading\":\"lazy\",\"decoding\":\"async\",\"width\":\"$undefined\",\"height\":\"$undefined\",\"fetchPriority\":\"auto\"}]]}],[\"$\",\"picture\",null,{\"className\":\"absolute inset-0 scale-[1.1] transform wallpaper-brightness-dark media-dark\",\"children\":[[\"$\",\"source\",null,{\"media\":\"(min-width: 901px)\",\"srcSet\":\"$84\"}],\"$undefined\",\"$L85\"]}]]}],\"$L86\",false,false]}]\n"])

self.__next_f.push([1,"73:[\"$\",\"div\",null,{\"className\":\"media-border-container relative grid grid-cols-1 grid-rows-1 bg-[image:var(--color-theme-card-03)]\",\"children\":[[\"$\",\"div\",null,{\"className\":\"relative z-1 col-span-full row-span-full overflow-hidden\",\"children\":[[\"$\",\"picture\",null,{\"className\":\"absolute inset-0 scale-[1.1] transform wallpaper-brightness-dark media-light\",\"children\":[[\"$\",\"source\",null,{\"media\":\"(min-width: 901px)\",\"srcSet\":\"$87\"}],\"$undefined\",[\"$\",\"img\",null,{\"className\":\"object-cover h-full w-full\",\"alt\":\"\",\"src\":\"https://cdn.sanity.io/images/2hv88549/production/fd9b3b4cd7d670f9f7d89ef54a9d83eedc7eb8cc-2560x1440.png?auto=format\",\"loading\":\"lazy\",\"decoding\":\"async\",\"width\":\"$undefined\",\"height\":\"$undefined\",\"fetchPriority\":\"auto\"}]]}],[\"$\",\"picture\",null,{\"className\":\"absolute inset-0 scale-[1.1] transform wallpaper-brightness-dark media-dark\",\"children\":[[\"$\",\"source\",null,{\"media\":\"(min-width: 901px)\",\"srcSet\":\"$88\"}],\"$undefined\",\"$L89\"]}]]}],\"$L8a\",false,false]}]\n"])

self.__next_f.push([1,"74:[\"$\",\"div\",null,{\"className\":\"media-border-container relative grid grid-cols-1 grid-rows-1 bg-[image:var(--color-theme-card-03)]\",\"children\":[[\"$\",\"div\",null,{\"className\":\"relative z-1 col-span-full row-span-full overflow-hidden\",\"children\":[[\"$\",\"picture\",null,{\"className\":\"absolute inset-0 scale-[1.1] transform wallpaper-brightness-dark media-light\",\"children\":[[\"$\",\"source\",null,{\"media\":\"(min-width: 901px)\",\"srcSet\":\"$8b\"}],\"$undefined\",[\"$\",\"img\",null,{\"className\":\"object-cover h-full w-full\",\"alt\":\"\",\"src\":\"https://cdn.sanity.io/images/2hv88549/production/fd9b3b4cd7d670f9f7d89ef54a9d83eedc7eb8cc-2560x1440.png?auto=format\",\"loading\":\"lazy\",\"decoding\":\"async\",\"width\":\"$undefined\",\"height\":\"$undefined\",\"fetchPriority\":\"auto\"}]]}],[\"$\",\"picture\",null,{\"className\":\"absolute inset-0 scale-[1.1] transform wallpaper-brightness-dark media-dark\",\"children\":[[\"$\",\"source\",null,{\"media\":\"(min-width: 901px)\",\"srcSet\":\"$8c\"}],\"$undefined\",\"$L8d\"]}]]}],\"$L8e\",false,false]}]\n"])

self.__next_f.push([1,"75:[\"$\",\"div\",null,{\"className\":\"media-border-container relative grid grid-cols-1 grid-rows-1 bg-[image:var(--color-theme-card-03)]\",\"children\":[[\"$\",\"div\",null,{\"className\":\"relative z-1 col-span-full row-span-full overflow-hidden\",\"children\":[[\"$\",\"picture\",null,{\"className\":\"absolute inset-0 scale-[1.1] transform wallpaper-brightness-dark media-light\",\"children\":[[\"$\",\"source\",null,{\"media\":\"(min-width: 901px)\",\"srcSet\":\"$8f\"}],\"$undefined\",[\"$\",\"img\",null,{\"className\":\"object-cover h-full w-full\",\"alt\":\"\",\"src\":\"https://cdn.sanity.io/images/2hv88549/production/fd9b3b4cd7d670f9f7d89ef54a9d83eedc7eb8cc-2560x1440.png?auto=format\",\"loading\":\"lazy\",\"decoding\":\"async\",\"width\":\"$undefined\",\"height\":\"$undefined\",\"fetchPriority\":\"auto\"}]]}],[\"$\",\"picture\",null,{\"className\":\"absolute inset-0 scale-[1.1] transform wallpaper-brightness-dark media-dark\",\"children\":[[\"$\",\"source\",null,{\"media\":\"(min-width: 901px)\",\"srcSet\":\"$90\"}],\"$undefined\",\"$L91\"]}]]}],\"$L92\",false,false]}]\n"])

self.__next_f.push([1,"76:[\"$\",\"div\",null,{\"className\":\"media-border-container relative grid grid-cols-1 grid-rows-1 bg-theme-card-02-hex\",\"children\":[[\"$\",\"div\",null,{\"className\":\"relative z-1 col-span-full row-span-full overflow-hidden\",\"children\":[[\"$\",\"picture\",null,{\"className\":\"absolute inset-0 scale-[1.1] transform wallpaper-brightness-dark media-light\",\"children\":[[\"$\",\"source\",null,{\"media\":\"(min-width: 901px)\",\"srcSet\":\"$93\"}],\"$undefined\",[\"$\",\"img\",null,{\"className\":\"object-cover h-full w-full\",\"alt\":\"\",\"src\":\"https://cdn.sanity.io/images/2hv88549/production/214fe44b44ebb3a38c3c7e6bde246a032d1bdc57-2560x1440.png?auto=format\",\"loading\":\"lazy\",\"decoding\":\"async\",\"width\":\"$undefined\",\"height\":\"$undefined\",\"fetchPriority\":\"auto\"}]]}],[\"$\",\"picture\",null,{\"className\":\"absolute inset-0 scale-[1.1] transform wallpaper-brightness-dark media-dark\",\"children\":[[\"$\",\"source\",null,{\"media\":\"(min-width: 901px)\",\"srcSet\":\"$94\"}],\"$undefined\",\"$L95\"]}]]}],\"$L96\",false,false]}]\n"])

self.__next_f.push([1,"77:[\"$\",\"div\",null,{\"className\":\"media-border-container relative grid grid-cols-1 grid-rows-1 bg-theme-card-02-hex\",\"children\":[[\"$\",\"div\",null,{\"className\":\"relative z-1 col-span-full row-span-full overflow-hidden\",\"children\":[[\"$\",\"picture\",null,{\"className\":\"absolute inset-0 scale-[1.1] transform wallpaper-brightness-dark\",\"children\":[[\"$\",\"source\",null,{\"media\":\"(min-width: 901px)\",\"srcSet\":\"$97\"}],\"$undefined\",[\"$\",\"img\",null,{\"className\":\"object-cover h-full w-full\",\"alt\":\"\",\"src\":\"https://cdn.sanity.io/images/2hv88549/production/7fe2629468a1b9598c66a47f5a8d6fbdc946f0b4-3093x2320.jpg?auto=format\",\"loading\":\"lazy\",\"decoding\":\"async\",\"width\":\"$undefined\",\"height\":\"$undefined\",\"fetchPriority\":\"auto\"}]]}],\"$undefined\"]}],[\"$\",\"div\",null,{\"className\":\"z-20 col-span-full row-span-full\",\"children\":\"$L98\"}],false,false]}]\n"])

self.__next_f.push([1,"78:[\"$\",\"div\",null,{\"className\":\"media-border-container relative grid grid-cols-1 grid-rows-1 bg-theme-card-02-hex\",\"children\":[[\"$\",\"div\",null,{\"className\":\"relative z-1 col-span-full row-span-full overflow-hidden\",\"children\":[[\"$\",\"picture\",null,{\"className\":\"absolute inset-0 scale-[1.1] transform wallpaper-brightness-dark media-light\",\"children\":[[\"$\",\"source\",null,{\"media\":\"(min-width: 901px)\",\"srcSet\":\"$99\"}],\"$undefined\",[\"$\",\"img\",null,{\"className\":\"object-cover h-full w-full\",\"alt\":\"\",\"src\":\"https://cdn.sanity.io/images/2hv88549/production/e6375f83012e4a76ff15411ce87937362c411153-2560x1440.png?auto=format\",\"loading\":\"lazy\",\"decoding\":\"async\",\"width\":\"$undefined\",\"height\":\"$undefined\",\"fetchPriority\":\"auto\"}]]}],[\"$\",\"picture\",null,{\"className\":\"absolute inset-0 scale-[1.1] transform wallpaper-brightness-dark media-dark\",\"children\":[[\"$\",\"source\",null,{\"media\":\"(min-width: 901px)\",\"srcSet\":\"$9a\"}],\"$undefined\",\"$L9b\"]}]]}],\"$L9c\",false,false]}]\n"])

self.__next_f.push([1,"import logging\nfrom typing import Dict\n\nimport pandas as pd\n\nlogger = logging.getLogger(__name__)\n\n\ndef focus_share(\n events: pd.DataFrame,\n feature_col: str = \"interaction_type\",\n user_col: str = \"user_id\",\n) -\u003e pd.DataFrame:\n if events.empty:\n return pd.DataFrame(\n columns=[feature_col, \"events\", \"unique_users\", \"share_of_events\", \"share_of_users\"],\n )\n\n missing = {feature_col, user_col} - set(events.columns)\n if missing:\n raise ValueError(f\"Missing required columns: {sorted(missing)}\")\n\n grouped = (\n events.groupby(feature_col)\n .agg(events=(feature_col, \"size\"), unique_users=(user_col, \"nunique\"))\n .reset_index()\n )\n\n total_events = grouped[\"events\"].sum()\n total_users = grouped[\"unique_users\"].sum()\n\n grouped[\"share_of_events\"] = grouped[\"events\"] / total_events if total_events else 0.0\n grouped[\"share_of_users\"] = grouped[\"unique_users\"] / total_users if total_users else 0.0\n return grouped.sort_values(\"events\", ascending=False).reset_index(drop=True)\n\n\ndef switch_summary(\n events: pd.DataFrame,\n feature_col: str = \"interaction_type\",\n user_col: str = \"user_id\",\n ts_col: str = \"timestamp\",\n) -\u003e Dict[str, float]:\n defaults = {\n \"total_switches\": 0,\n \"agent_entry_rate\": 0.0,\n \"tab_return_rate\": 0.0,\n \"avg_seconds_between_switches\": 0.0,\n }\n\n if events.empty:\n return defaults\n\n required = {feature_col, user_col, ts_col}\n missing = required - set(events.columns)\n if missing:\n raise ValueError(f\"Missing required columns: {sorted(missing)}\")\n\n df = events.copy()\n df[ts_col] = pd.to_datetime(df[ts_col], utc=True, errors=\"coerce\")\n df = df.dropna(subset=[ts_col]).sort_values([user_col, ts_col])\n if df.empty:\n return defaults\n\n df[\"prev_feature\"] = df.groupby(user_col)[feature_col].shift(1)\n df[\"prev_timestamp\"] = df.groupby(user_col)[ts_col].shift(1)\n switches = df[df[\"prev_feature\"].notna() \u0026 (df[feature_col] != df[\"prev_feature\"])]\n total = int(switches.shape[0])\n if total == 0:\n return defaults\n\n agent_entries = switches[switches[feature_col] == \"agent\"]\n tab_returns = switches[(switches[feature_col] == \"tab\") \u0026 (switches[\"prev_feature\"] != \"tab\")]\n durations = (switches[ts_col] - switches[\"prev_timestamp\"]).dt.total_seconds()\n\n return {\n \"total_switches\": total,\n \"agent_entry_rate\": float(agent_entries.shape[0] / total),\n \"tab_return_rate\": float(tab_returns.shape[0] / total),\n \"avg_seconds_between_switches\": float(durations.mean()) if not durations.empty else 0.0,\n }\n\n\ndef rolling_focus_share(\n events: pd.DataFrame,\n ts_col: str = \"timestamp\",\n feature_col: str = \"interaction_type\",\n freq: str = \"1D\",\n window: int = 7,\n) -\u003e pd.DataFrame:\n if events.empty:\n return pd.DataFrame()\n\n required = {ts_col, feature_col}\n missing = required - set(events.columns)\n if missing:\n raise ValueError(f\"Missing required columns: {sorted(missing)}\")\n\n df = events.copy()\n df[ts_col] = pd.to_datetime(df[ts_col], utc=True, errors=\"coerce\")\n df = df.dropna(subset=[ts_col])\n if df.empty:\n return pd.DataFrame()\n\n daily = (\n df.groupby([pd.Grouper(key=ts_col, freq=freq), feature_col])\n .size()\n .unstack(fill_value=0)\n .sort_index()\n )\n\n if daily.empty:\n return pd.DataFrame()\n\n totals = daily.sum(axis=1).replace(0, pd.NA)\n shares = daily.div(totals, axis=0).fillna(0.0)\n return shares.rolling(window, min_periods=1).mean()\n"])

self.__next_f.push([1,"from typing import Dict, Optional\n\nimport pandas as pd\n\nALIAS_MAP: Dict[str, str] = {\n \"tabs\": \"tab\",\n \"tab_view\": \"tab\",\n \"assistants\": \"agent\",\n \"assistant_panel\": \"agent\",\n}\n\n\ndef normalize_interactions(\n events: pd.DataFrame,\n *,\n feature_col: str = \"interaction_type\",\n mapping: Optional[Dict[str, str]] = None,\n) -\u003e pd.DataFrame:\n mapping = mapping or ALIAS_MAP\n\n df = events.copy()\n df[feature_col] = df[feature_col].astype(str).str.strip().str.lower()\n df[feature_col] = df[feature_col].replace(mapping)\n return df\n\n\ndef tag_sessions(\n events: pd.DataFrame,\n *,\n user_col: str = \"user_id\",\n ts_col: str = \"timestamp\",\n session_gap_minutes: int = 30,\n) -\u003e pd.DataFrame:\n if events.empty:\n out = events.copy()\n out[\"session_id\"] = pd.Series(dtype=str)\n return out\n\n df = events.copy()\n df[ts_col] = pd.to_datetime(df[ts_col], utc=True, errors=\"coerce\")\n df = df.dropna(subset=[ts_col]).sort_values([user_col, ts_col])\n if df.empty:\n df[\"session_id\"] = pd.Series(dtype=str)\n return df\n\n df[\"prev_timestamp\"] = df.groupby(user_col)[ts_col].shift(1)\n gap = pd.Timedelta(minutes=session_gap_minutes)\n df[\"new_session\"] = (df[\"prev_timestamp\"].isna()) | ((df[ts_col] - df[\"prev_timestamp\"]) \u003e gap)\n df[\"session_index\"] = df.groupby(user_col)[\"new_session\"].cumsum()\n df[\"session_id\"] = df[user_col].astype(str) + \"-\" + df[\"session_index\"].astype(str)\n return df.drop(columns=[\"prev_timestamp\", \"new_session\", \"session_index\"])\n"])

self.__next_f.push([1,"import pandas as pd\n\nfrom usage.report import build_usage_report\nfrom usage.segmentation import normalize_interactions, tag_sessions\nfrom usage.summary import focus_share, switch_summary\n\n\ndef sample_events() -\u003e pd.DataFrame:\n return pd.DataFrame(\n [\n {\"user_id\": \"u1\", \"timestamp\": \"2024-03-01T09:00:00Z\", \"interaction_type\": \"Tab\"},\n {\"user_id\": \"u1\", \"timestamp\": \"2024-03-01T09:05:00Z\", \"interaction_type\": \"agent\"},\n {\"user_id\": \"u1\", \"timestamp\": \"2024-03-01T09:20:00Z\", \"interaction_type\": \"tab\"},\n {\"user_id\": \"u2\", \"timestamp\": \"2024-03-01T10:00:00Z\", \"interaction_type\": \"assistant_panel\"},\n {\"user_id\": \"u2\", \"timestamp\": \"2024-03-01T10:10:00Z\", \"interaction_type\": \"tab\"},\n ]\n )\n\n\ndef test_focus_share_orders_features() -\u003e None:\n events = normalize_interactions(sample_events())\n summary = focus_share(events)\n assert list(summary[\"interaction_type\"]) == [\"tab\", \"agent\"]\n\n\ndef test_switch_summary_counts_transitions() -\u003e None:\n events = normalize_interactions(sample_events())\n metrics = switch_summary(events)\n assert metrics[\"total_switches\"] == 3\n assert 0.0 \u003c= metrics[\"agent_entry_rate\"] \u003c= 1.0\n\n\ndef test_tag_sessions_assigns_ids() -\u003e None:\n tagged = tag_sessions(sample_events(), session_gap_minutes=5)\n assert tagged[\"session_id\"].nunique() \u003e= 2\n\n\ndef test_usage_report_returns_expected_sections() -\u003e None:\n report = build_usage_report(sample_events())\n assert {\"focus_share\", \"switch_summary\", \"rolling_focus_share\", \"session_count\"} \u003c= report.keys()\n"])

self.__next_f.push([1,"98:[\"$\",\"$La5\",null,{\"disableInteractions\":true,\"innerPaddingPx\":24,\"aspectRatio\":{\"base\":\"1/1\",\"md\":\"1/1\"},\"height\":\"378px\",\"windows\":[{\"id\":\"agent-ide\",\"title\":\"\",\"x\":50,\"y\":50,\"widthPx\":680,\"heightPx\":330,\"zIndex\":10,\"shadowless\":true,\"content\":[\"$\",\"$Lab\",null,{\"scenario\":{\"id\":\"startup-analytics\",\"title\":\"Workspace Feature Adoption Insights:eyIkX2hhc2giOiJhNzEyMDMzZTNkNjc5NThjIiwiJF9zb3VyY2UiOiJXb3Jrc3BhY2UgRmVhdHVyZSBBZG9wdGlvbiBJbnNpZ2h0cyJ9\",\"repoName\":\"cursor-usage-research\",\"chatTitle\":\"Analyze Tab vs Agent Usage Patterns:eyIkX2hhc2giOiJlZjUzMGEyNTcwZTU1MDYzIiwiJF9zb3VyY2UiOiJBbmFseXplIFRhYiB2cyBBZ2VudCBVc2FnZSBQYXR0ZXJucyJ9\",\"showInAgentsSidebar\":true,\"files\":[{\"id\":\"usage/summary.py\",\"name\":\"usage/summary.py\",\"language\":\"python\",\"content\":\"$ac\"},{\"id\":\"usage/segmentation.py\",\"name\":\"usage/segmentation.py\",\"language\":\"python\",\"content\":\"$ad\"},{\"id\":\"usage/report.py\",\"name\":\"usage/report.py\",\"language\":\"python\",\"content\":\"from typing import Dict\\n\\nimport pandas as pd\\n\\nfrom .segmentation import normalize_interactions, tag_sessions\\nfrom .summary import focus_share, rolling_focus_share, switch_summary\\n\\n\\ndef build_usage_report(events: pd.DataFrame) -\u003e Dict[str, object]:\\n normalized = normalize_interactions(events)\\n sessions = tag_sessions(normalized)\\n\\n return {\\n \\\"focus_share\\\": focus_share(normalized),\\n \\\"switch_summary\\\": switch_summary(normalized),\\n \\\"rolling_focus_share\\\": rolling_focus_share(normalized),\\n \\\"session_count\\\": sessions[\\\"session_id\\\"].nunique() if not sessions.empty else 0,\\n }\\n\"},{\"id\":\"tests/test_usage.py\",\"name\":\"tests/test_usage.py\",\"language\":\"python\",\"content\":\"$ae\"},{\"id\":\"requirements.txt\",\"name\":\"requirements.txt\",\"language\":\"text\",\"content\":\"pandas\u003e=2.2.0\\npytest\u003e=7.0.0\"}],\"messages\":[{\"id\":\"sa1\",\"role\":\"user\",\"text\":\"Help me understand how teams split their focus between the tab view and the agents panel across our workspaces.:eyIkX2hhc2giOiI3NDU3YjcwMTFhZGVjYzJhIiwiJF9zb3VyY2UiOiJIZWxwIG1lIHVuZGVyc3RhbmQgaG93IHRlYW1zIHNwbGl0IHRoZWlyIGZvY3VzIGJldHdlZW4gdGhlIHRhYiB2aWV3IGFuZCB0aGUgYWdlbnRzIHBhbmVsIGFjcm9zcyBvdXIgd29ya3NwYWNlcy4ifQ==\"},{\"id\":\"sa2\",\"role\":\"thinking\",\"text\":\"Thought 7s:eyIkX2hhc2giOiIwMjZlMDEzM2RkYzViMjQwIiwiJF9zb3VyY2UiOiJUaG91Z2h0IDdzIn0=\"},{\"id\":\"sa3\",\"role\":\"read\",\"text\":\"Reviewed workspace usage exports and historical engagement notes:eyIkX2hhc2giOiJhMTc0MTIyZmY5YzY2N2I4IiwiJF9zb3VyY2UiOiJSZXZpZXdlZCB3b3Jrc3BhY2UgdXNhZ2UgZXhwb3J0cyBhbmQgaGlzdG9yaWNhbCBlbmdhZ2VtZW50IG5vdGVzIn0=\"},{\"id\":\"sa4\",\"role\":\"assistant\",\"text\":\"I'll build an analytics toolkit that highlights tab versus agent adoption and switching behavior::eyIkX2hhc2giOiI2ZWJlNGEzNjk5NDgxNTk4IiwiJF9zb3VyY2UiOiJJJ2xsIGJ1aWxkIGFuIGFuYWx5dGljcyB0b29sa2l0IHRoYXQgaGlnaGxpZ2h0cyB0YWIgdmVyc3VzIGFnZW50IGFkb3B0aW9uIGFuZCBzd2l0Y2hpbmcgYmVoYXZpb3I6In0=\"},{\"id\":\"sa5\",\"role\":\"code\",\"text\":\"summary.py +150 -0\"},{\"id\":\"sa6\",\"role\":\"assistant\",\"text\":\"Next I'll normalize the interaction labels and add session tagging so product can filter specific cohorts::eyIkX2hhc2giOiIwMDQzZWFhMDM1ODg0MGM1IiwiJF9zb3VyY2UiOiJOZXh0IEknbGwgbm9ybWFsaXplIHRoZSBpbnRlcmFjdGlvbiBsYWJlbHMgYW5kIGFkZCBzZXNzaW9uIHRhZ2dpbmcgc28gcHJvZHVjdCBjYW4gZmlsdGVyIHNwZWNpZmljIGNvaG9ydHM6In0=\"},{\"id\":\"sa7\",\"role\":\"code\",\"text\":\"segmentation.py +94 -0\"},{\"id\":\"sa8\",\"role\":\"assistant\",\"text\":\"Time to stitch everything together with a report helper that combines the key metrics::eyIkX2hhc2giOiI2MzY5NTVkNzlhMGJjODUzIiwiJF9zb3VyY2UiOiJUaW1lIHRvIHN0aXRjaCBldmVyeXRoaW5nIHRvZ2V0aGVyIHdpdGggYSByZXBvcnQgaGVscGVyIHRoYXQgY29tYmluZXMgdGhlIGtleSBtZXRyaWNzOiJ9\"},{\"id\":\"sa9\",\"role\":\"code\",\"text\":\"report.py +40 -0\"},{\"id\":\"sa10\",\"role\":\"assistant\",\"text\":\"I'll add tests to lock in the behaviour using a representative sample from last week's export::eyIkX2hhc2giOiI2YWVlMTg2MDVmMjY5ZWE4IiwiJF9zb3VyY2UiOiJJJ2xsIGFkZCB0ZXN0cyB0byBsb2NrIGluIHRoZSBiZWhhdmlvdXIgdXNpbmcgYSByZXByZXNlbnRhdGl2ZSBzYW1wbGUgZnJvbSBsYXN0IHdlZWsncyBleHBvcnQ6In0=\"},{\"id\":\"sa11\",\"role\":\"code\",\"text\":\"test_usage.py created +90 lines\"},{\"id\":\"sa12\",\"role\":\"assistant\",\"text\":\"All set! We now track focus share, switching rates, and rolling engagement so PMs can compare tab-first and agent-first workflows in seconds.:eyIkX2hhc2giOiI4NGY0YTIwNGVlMzk2NjZiIiwiJF9zb3VyY2UiOiJBbGwgc2V0ISBXZSBub3cgdHJhY2sgZm9jdXMgc2hhcmUsIHN3aXRjaGluZyByYXRlcywgYW5kIHJvbGxpbmcgZW5nYWdlbWVudCBzbyBQTXMgY2FuIGNvbXBhcmUgdGFiLWZpcnN0IGFuZCBhZ2VudC1maXJzdCB3b3JrZmxvd3MgaW4gc2Vjb25kcy4ifQ==\"}],\"openFileIds\":[\"usage/summary.py\",\"usage/report.py\",\"tests/test_usage.py\"],\"activeFileId\":\"usage/summary.py\",\"diffSummary\":{\"added\":220,\"removed\":80}},\"embedded\":true,\"streamInitialMessages\":false,\"title\":\"cursor-agent\",\"disableInteractions\":true}]}]}]\n"])

self.__next_f.push([1,"9d:[\"$\",\"$Laf\",null,{\"aspectRatio\":{\"base\":\"1/1\",\"md\":\"1/1\"},\"height\":\"378px\"}]\n9e:[\"$\",\"$Lb0\",null,{\"currentModel\":\"GPT-5\",\"aspectRatio\":{\"base\":\"1/1\",\"md\":\"1/1\"},\"height\":\"378px\"}]\n9f:[\"$\",\"$La5\",null,{\"disableInteractions\":true,\"height\":\"378px\",\"innerPaddingPx\":24,\"windows\":[{\"id\":\"cursor-ide\",\"title\":\"\",\"x\":50,\"y\":50,\"widthPx\":680,\"heightPx\":330,\"shadowless\":true,\"zIndex\":10,\"ctaHref\":\"/download\",\"ctaLabel\":\"Get Cursor\",\"content\":[\"$\",\"$La6\",null,{\"layoutPreset\":\"ide\",\"scenario\":\"biotech\",\"showSidebar\":false,\"showChat\":false,\"mode\":{\"type\":\"cmdk\",\"autoplay\":true,\"selectLines\":{\"start\":7,\"end\":8},\"selectionDelay\":800},\"embedded\":true}]}]}]\na0:[\"$\",\"$La5\",null,{\"disableInteractions\":true,\"height\":\"378px\",\"innerPaddingPx\":24,\"windows\":[{\"id\":\"cursor-ide\",\"title\":\"\",\"x\":50,\"y\":50,\"widthPx\":680,\"heightPx\":330,\"shadowless\":true,\"zIndex\":10,\"ctaHref\":\"/download\",\"ctaLabel\":\"Get Cursor\",\"content\":[\"$\",\"$La6\",null,{\"layoutPreset\":\"ide\",\"scenario\":\"changeManagement\",\"showSidebar\":false,\"showChat\":false,\"mode\":{\"type\":\"tab-static\",\"multiHighlight\":[6,7,13],\"caretLine\":2},\"embedded\":true}]}]}]\na1:[\"$\",\"$La5\",null,{\"disableInteractions\":true,\"height\":\"378px\",\"innerPaddingPx\":24,\"windows\":[{\"id\":\"cursor-ide\",\"title\":\"\",\"x\":50,\"y\":50,\"widthPx\":680,\"heightPx\":330,\"shadowless\":true,\"zIndex\":10,\"ctaHref\":\"/download\",\"ctaLabel\":\"Get Cursor\",\"content\":[\"$\",\"$La6\",null,{\"layoutPreset\":\"ide\",\"scenario\":\"pseudoCode\",\"showSidebar\":false,\"showChat\":false,\"mode\":{\"type\":\"tab-static\",\"suggestAfterLine\":7,\"caretLine\":7},\"embedded\":true}]}]}]\na2:[\"$\",\"$La5\",null,{\"disableInteractions\":true,\"height\":\"378px\",\"innerPaddingPx\":24,\"windows\":[{\"id\":\"cursor-ide\",\"title\":\"\",\"x\":50,\"y\":50,\"widthPx\":341,\"heightPx\":330,\"shadowless\":true,\"zIndex\":10,\"ctaHref\":\"/download\",\"ctaLabel\":\"Get Cursor\",\"content\":[\"$\",\"$La6\",null,{\"layoutPreset\":\"ide\",\"scenario\":\"changeManagement\",\"showSidebar\":false,\"showChat\":false,\"mode\":{\"type\":\"tab-static\",\"bottomPillLabel\":\"Changelog.tsx\"},\"embedded\":true}]}]}]\na3:[\"$\",\"$La5\",null,{\"disableInteractions\":true,\"innerPadd"])

## 1.5x faster MoE training with custom MXFP8 kernels ¬∑ Cursor

### 1.5x faster MoE training with custom MXFP8 kernels

### 1.5x faster MoE training with custom MXFP8 kernels

### A quick introduction to the Microscaling (MX) data formats

### 1. Tensor memory and CUDA cores kill the dequantization vibe

### 2. Death by a thousand quantizations

### Choosing the right low-precision recipe

### Embracing tcgen05 MXFP8 block-scaled matrix multiplication

### Expanding to MXFP8 grouped matrix multiplications

### Building the fastest MXFP8 quantization kernel ever

### A quick introduction to the Microscaling (MX) data formats

### 1. Tensor memory and CUDA cores kill the dequantization vibe

### 2. Death by a thousand quantizations

### Choosing the right low-precision recipe

### Embracing tcgen05 MXFP8 block-scaled matrix multiplication

### Expanding to MXFP8 grouped matrix multiplications

### Building the fastest MXFP8 quantization kernel ever

### L2 cache optimization via expert-wise supergrouping

### Grouped matrix multiplication benchmarks

### L2 cache optimization via expert-wise supergrouping

### Grouped matrix multiplication benchmarks

{"@context":"https://schema.org","@type":"Organization","@id":"https://cursor.com/#org","name":"Cursor","description":"Cursor is the best way to code with AI.","url":"https://cursor.com","logo":{"@type":"ImageObject","url":"https://cursor.com/public/opengraph-image.png","width":200,"height":60},"sameAs":["https://twitter.com/cursor_ai","https://www.linkedin.com/company/cursorai"],"contactPoint":{"@type":"ContactPoint","contactType":"customer service","url":"https://cursor.com/contact-sales"},"foundingDate":"2023"}

{"@context":"https://schema.org","@type":"WebSite","@id":"https://cursor.com/#website","name":"Cursor","description":"Cursor is the best way to code with AI.","url":"https://cursor.com","publisher":{"@type":"Organization","@id":"https://cursor.com/#org"}}

{"@context":"https://schema.org","@type":"BlogPosting","@id":"https://cursor.com/blog/kernels","headline":"1.5x faster MoE training with custom MXFP8 kernels","description":"Achieving a 3.5x MoE layer speedup with a complete rebuild for Blackwell GPUs.","datePublished":"2025-08-29T02:55:25.007Z","dateModified":"2025-08-29T02:55:25.007Z","author":{"@type":"Person","name":"Stuart Sul"},"publisher":{"@type":"Organization","@id":"https://cursor.com/#org"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://cursor.com/blog/kernels"},"url":"https://cursor.com/blog/kernels","inLanguage":"en-US","isPartOf":{"@type":"Blog","name":"Cursor Blog","url":"https://cursor.com/blog"},"articleSection":"research","wordCount":3731}

We want to build the world‚Äôs best AI coding models, but training large language models can be expensive. For instance, our largest internal models can take weeks to train on tens of thousands of GPUs. This is not only computationally expensive, but it also slows the pace at which improvements reach our users.

We recently began upgrading from Hopper GPUs (H100s) to Blackwell GPUs (B200s) and saw this as an opportunity to deeply optimize our training workloads. Profiling revealed that the main bottleneck was the Mixture-of-Experts (MoE) layer, implemented with MegaBlocks, which accounted for nearly 53% of forward-pass time and 27% of backward-pass time.

That‚Äôs why, over the past few weeks, we rewrote the entire MoE layer from scratch at the GPU kernel level with zero dependencies on any CUDA libraries. Instead, we used pure, good old CUDA and PTX, with a few bits of ThunderKittens sprinkled in. As a result, we achieved a 3.5x improvement in MoE layer performance for both the forward and backward passes, translating to a 1.5x end-to-end training speedup on Blackwell and a 2x speedup compared to our original Hopper setup. We believe our stack is faster than any combination of open-source alternatives available today.

Most of this improvement came from transitioning from BF16 to MXFP8 which we achieved with nearly zero loss in training quality. But this also taught us that going low precision is easier said than done. If not done carefully, MXFP8 training may offer minimal performance improvement over BF16 due to various kernel overheads. The MXFP8 training recipe is also not widely shared, meaning you have to discover the right approach yourself.

A common way to reduce the computational cost of large deep learning models is to use lower-precision activations and weights. However, converting them to narrow bit-width formats (e.g., 8 or fewer bits) introduces unacceptable rounding error unless the values are scaled appropriately. For example, some weights in a large model might be 0.0001, 0.0005, or ‚Äì0.0007, but naively converting these to FP8 would round them all to the same number: zero, since the smallest positive value representable in FP8E4M3 is around 0.0019.

To address this, it is common to apply a per-tensor scaling factor, which rescales the tensor while keeping its values within the representable range of the target data format. This ensures that the available dynamic range is fully utilized. For example, if a tensor‚Äôs values are all between ‚Äì0.0001 and 0.0001, its scaling factor could be set to 4,480,000. This would cause the tensor‚Äôs values after scaling to be in the range [‚Äì448, 448], which corresponds to the representable bounds of FP8E4M3.

Microscaling goes a step further by applying scaling to fine-grained sub-blocks of a tensor, rather than using a single scale factor for the entire tensor. The Microscaling (MX) format standardizes this approach by defining a set of low-precision, micro-scaled data formats. Full details are provided in its specification, which defines the following concrete MX-compliant formats:

Microscaling goes a step further by applying scaling to fine-grained sub-blocks of a tensor, rather than using a single scale factor for the entire tensor. The Microscaling (MX) format standardizes this approach by defining a set of low-precision, micro-scaled data formats. Full details are provided in its specification, which defines the following concrete MX-compliant formats:

Although microscaling can unlock significant performance gains, applying it in practice introduces several challenges and depends heavily on the underlying hardware. Let‚Äôs first look at why applying microscaling on NVIDIA Blackwell GPUs can be particularly challenging.

This method naturally works well on the Hopper architecture (on which DSV3 was trained), because (1) the results of the tensor core matrix multiplies (via wgmma instruction) are accumulated in registers, and (2) you can pipeline matrix multiplies, asynchronously launching other tensor core matrix multiplies while performing dequantization with CUDA cores. Because everything is accumulated in the registers, no additional data movement is required between the matrix multiplies.

This is no longer the case on Blackwell GPUs due to tensor memory (TMEM). TMEM is a new set of on-chip, SM-local memory added on Blackwell GPUs. Unlike Hopper GPUs, where the results of tensor core matrix multiplies accumulate directly in registers, Blackwell GPUs accumulate them in TMEM (via tcgen05.mma instruction). To perform custom arithmetic on the accumulators, you must transfer the results from TMEM to registers, process them with CUDA cores, write them back to TMEM, and then wait for all of these instructions to complete because the data movements are asynchronous. Although TMEM is faster to access than shared memory (SMEM), this still kills tensor core occupancy.

In grouped matrix multiplications for MoE training, it‚Äôs common for MM to be very large compared to KK and NN. So let‚Äôs take:

In total, this is approximately 2.9 GB worth of High Bandwidth Memory (HBM) reads and writes. Assuming a sustainable HBM throughput of 6.5 TB/s on B200s, an optimized FP8 quantization kernel would take:

Even though FP8 matrix multiplication is theoretically 2x faster than BF16 matrix multiplication, quantization time can truly kill the performance gain. The above analysis is also optimistic because it assumes a block-scaled FP8 matrix multiplication running at 3,300 TFLOP/s, which is much higher than what is typically achieved in end-to-end MoE training. In the worst case, the quantization could take longer than the FP8 matrix multiplication, thus making the overall FP8 computation slower than BF16.

Even though FP8 matrix multiplication is theoretically 2x faster than BF16 matrix multiplication, quantization time can truly kill the performance gain. The above analysis is also optimistic because it assumes a block-scaled FP8 matrix multiplication running at 3,300 TFLOP/s, which is much higher than what is typically achieved in end-to-end MoE training. In the worst case, the quantization could take longer than the FP8 matrix multiplication, thus making the overall FP8 computation slower than BF16.

Furthermore, NVIDIA‚Äôs MXFP8 tcgen05.mma instruction, the PTX instruction for tensor core matrix multiplication, requires a slightly unintuitive scale factor layout. With 32-block scaling, TransformerEngine or TorchAO quantization kernels return the scale factor in a naive M x N / 32 layout. This must then be reshaped, either in PyTorch or by fusing the reshaping logic into other kernels, both of which negatively affect performance. In practice, you really don‚Äôt want to process scale factors inside the matrix multiply kernels. The fastest way to load them is by taking the HBM ‚Üí SMEM (cp.async.bulk) path and then the SMEM ‚Üí TMEM (tcgen05.cp) path; the moment a scale detours through the register tile, the tensor vibe is dead.

Next, we explain how we addressed the aforementioned challenges, starting with our approach to quantization.

To match the training quality of BF16, we ran a series of low-precision experiments, measuring how much each recipe diverged from BF16. From these, we identified the one that yields training loss convergence nearly identical to that of BF16 for our workloads.

Finally, tcgen05.mma instruction requires the scale factor to reside in TMEM. However, there is no direct method to load the scales from HBM into TMEM. The fastest approach is to first load the data from HBM into on-chip SMEM using the cp.async.bulk.tensor instruction (leveraging the Tensor Memory Accelerator, or TMA), and then transfer it from SMEM to TMEM using the tcgen05.cp instruction. For this to work, scale factors must be stored in the layout expected by tcgen05.mma, as explained later in this post.

With this setup, we design a pipeline where certain warps continuously load input tiles and scale from HBM to SMEM, others move scales from SMEM to TMEM, others launch the MMAs, and some occasionally load the TMEM accumulator into registers, store it in SMEM, and TMA-store it back to HBM.

if (warpgroup_id < 2) { for (int i = 0; i < num_tiles; i++) { mbarrier_wait_for_final_matmul_completion(); // mbarrier.try_wait async_load_from_TMEM(reg, TMEM); // tcgen05.ld wait_for_load_completion(); // tcgen05.wait // This is iterated in the actual implementation to save SMEM store_to_SMEM(SMEM, reg); TMA_async_store(HBM, SMEM); // cp.async.bulk.tensor } } else { if (warp_id == 0) { for (int i = 0; i < num_tiles; i++) { for (int j = 0; j < num_iters; j++) { mbarrier_wait_for_matmul_completion(); // load input tiles TMA_async_load(SMEM, HBM); } } } else if (warp_id == 1) { for (int i = 0; i < num_tiles; i++) { for (int j = 0; j < num_iters; j++) { mbarrier_wait_for_tcgen05_cp_completion(); // load scales (HBM -> SMEM) TMA_load(SMEM, HBM); } } } else if (warp_id == 2) { for (int i = 0; i < num_tiles; i++) { for (int j = 0; j < num_iters; j++) { mbarrier_wait_for_matmul_completion(); mbarrier_wait_for_scale_SMEM_load(); // load scales (SMEM -> TMEM) load_to_TMEM(TMEM, SMEM); // tcgen05.cp } } } else if (cta_rank == 0) { // 2-CTA MMA is launched by a single CTA for (int i = 0; i < num_tiles; i++) { mbarrier_wait_for_TMEM_clear(); for (int j = 0; j < num_iters; j++) { mbarrier_wait_for_input_SMEM_load(); mbarrier_wait_for_scale_TMEM_load(); // tcgen05.mma.cta_group::2.mxf8f6f4.block_scale launch_matmul(SMEM, SMEM, TMEM); } } } }

When scale factors must also reside in TMEM, however, we can only execute a single 256x32x256 tcgen05.mma instruction at a time using just a 128x256 region of TMEM. As a result, performance degradation is unavoidable. For example, the throughput of a 16,384x16,384x16,384 FP8 matrix multiplication drops from 3,200 TFLOP/s to 3,040 TFLOP/s under this constraint.

These throughput numbers apply only to pure FP8 matrix multiplication. With MXFP8 block scaling, throughput inevitably decreases further due to TMEM pipelining overhead. In practice, we achieve around 2,750 TFLOP/s with L2 cache clearance for block-scaled MXFP8 matrix multiplication kernels. Even so, this remains ~1.83x faster than standard BF16 matrix multiplication, which typically reaches 1,500~1,550 TFLOP/s on optimal shapes. Not too bad of a start!

A standalone MXFP8 matrix multiplication kernel is a useful first step, but during MXFP8 MoE training, its applications are limited (e.g., shared expert scenarios). To fully support MoE in MXFP8, we need grouped matrix multiplication kernels, specifically:

Note that these variants are part of the reason we are building these kernels from scratch. To date, we have not found any open-source alternative that fully supports MXFP8 MoE training with 32-block scaling.

With this abstraction, implementing the grouped MXFP8 matrix multiplication variants reduces to designing the appropriate loop structures and assigning indices correctly before launching the above abstraction. However, naive looping can significantly hurt performance due to poor L2 cache utilization.

Maintaining high L2 cache utilization is critical. In our benchmarks, inefficient HBM access patterns could reduce performance by nearly 50% in grouped matrix multiplication kernels. To address this, we applied supergrouping, a heuristic from ThunderKittens kernels that maximizes L2 reuse by ensuring that the region of the output matrix computed by all 148 SMs at any given time is as square as possible. You can learn more about this by reading the linked kernel code above.

The closest open-source alternative on Blackwell for grouped Fprop/Dgrad/Wgrad is DeepSeek‚Äôs DeepGEMM. DeepGEMM differs slightly in that it uses 1x128 and 128x128 scale blocks for the A and B matrices, which comes at the cost of reduced accuracy. Still, it is the only alternative available, so we integrated DeepGEMM into our internal model training and profiled its performance. While DeepGEMM excelled on certain input shapes in our microbenchmarks, our end-to-end benchmarks showed that our kernels outperformed it for our workloads.

It is also important to note that the above benchmarks exclude quantization time. DeepGEMM does not provide optimized quantization kernels, and without them, end-to-end performance can quickly degrade. In the worst case, it can even be slower than BF16 training.

With these optimizations, we implemented an MXFP8 quantization kernel that sustains 6.2+ TB/s while producing the scale matrix in a layout directly compatible with tcgen05.mma. To the best of our knowledge, this is the fastest MXFP8 quantization kernel available for MoE training.

With these optimizations, we implemented an MXFP8 quantization kernel that sustains 6.2+ TB/s while producing the scale matrix in a layout directly compatible with tcgen05.mma. To the best of our knowledge, this is the fastest MXFP8 quantization kernel available for MoE training.

3.5x faster MoE layer execution for both the forward and backward passes while maintaining the same training quality

With all the optimizations described above, we achieved 3.5x faster MoE layer execution for both the forward and backward passes while maintaining the same training quality. When tested on one of our internal models, this translated into a 1.5x end-to-end training speedup on Blackwell and a 2x speedup compared to our original Hopper setup, measured in tokens per second per GPU (TPS/GPU). We believe that our stack runs MXFP8 MoE training faster than any combination of open-source alternatives.

We still have plenty of work ahead and many kernels left to optimize. Our current efforts focus on building more efficient multi-GPU communication, improving custom attention kernels, and preparing to transition to FP4 for future MoE training runs.

If you are interested in writing and optimizing high-performance kernels, training large coding models, or our work broadly, we‚Äôd love to hear from you. Reach out to us at hiring@anysphere.co.

{"@context":"https://schema.org","@type":"BlogPosting","@id":"https://cursor.com/blog/kernels","headline":"1.5x faster MoE training with custom MXFP8 kernels","description":"Achieving a 3.5x MoE layer speedup with a complete rebuild for Blackwell GPUs.","datePublished":"2025-08-29T02:55:25.007Z","dateModified":"2025-08-29T02:55:25.007Z","author":{"@type":"Person","name":"Stuart Sul"},"publisher":{"@type":"Organization","@id":"https://cursor.com/#org"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://cursor.com/blog/kernels"},"url":"https://cursor.com/blog/kernels","inLanguage":"en-US","isPartOf":{"@type":"Blog","name":"Cursor Blog","url":"https://cursor.com/blog"},"articleSection":"research","wordCount":3731}

We want to build the world‚Äôs best AI coding models, but training large language models can be expensive. For instance, our largest internal models can take weeks to train on tens of thousands of GPUs. This is not only computationally expensive, but it also slows the pace at which improvements reach our users.

We recently began upgrading from Hopper GPUs (H100s) to Blackwell GPUs (B200s) and saw this as an opportunity to deeply optimize our training workloads. Profiling revealed that the main bottleneck was the Mixture-of-Experts (MoE) layer, implemented with MegaBlocks, which accounted for nearly 53% of forward-pass time and 27% of backward-pass time.

That‚Äôs why, over the past few weeks, we rewrote the entire MoE layer from scratch at the GPU kernel level with zero dependencies on any CUDA libraries. Instead, we used pure, good old CUDA and PTX, with a few bits of ThunderKittens sprinkled in. As a result, we achieved a 3.5x improvement in MoE layer performance for both the forward and backward passes, translating to a 1.5x end-to-end training speedup on Blackwell and a 2x speedup compared to our original Hopper setup. We believe our stack is faster than any combination of open-source alternatives available today.

Most of this improvement came from transitioning from BF16 to MXFP8 which we achieved with nearly zero loss in training quality. But this also taught us that going low precision is easier said than done. If not done carefully, MXFP8 training may offer minimal performance improvement over BF16 due to various kernel overheads. The MXFP8 training recipe is also not widely shared, meaning you have to discover the right approach yourself.

A common way to reduce the computational cost of large deep learning models is to use lower-precision activations and weights. However, converting them to narrow bit-width formats (e.g., 8 or fewer bits) introduces unacceptable rounding error unless the values are scaled appropriately. For example, some weights in a large model might be 0.0001, 0.0005, or ‚Äì0.0007, but naively converting these to FP8 would round them all to the same number: zero, since the smallest positive value representable in FP8E4M3 is around 0.0019.

To address this, it is common to apply a per-tensor scaling factor, which rescales the tensor while keeping its values within the representable range of the target data format. This ensures that the available dynamic range is fully utilized. For example, if a tensor‚Äôs values are all between ‚Äì0.0001 and 0.0001, its scaling factor could be set to 4,480,000. This would cause the tensor‚Äôs values after scaling to be in the range [‚Äì448, 448], which corresponds to the representable bounds of FP8E4M3.

Microscaling goes a step further by applying scaling to fine-grained sub-blocks of a tensor, rather than using a single scale factor for the entire tensor. The Microscaling (MX) format standardizes this approach by defining a set of low-precision, micro-scaled data formats. Full details are provided in its specification, which defines the following concrete MX-compliant formats:

Microscaling goes a step further by applying scaling to fine-grained sub-blocks of a tensor, rather than using a single scale factor for the entire tensor. The Microscaling (MX) format standardizes this approach by defining a set of low-precision, micro-scaled data formats. Full details are provided in its specification, which defines the following concrete MX-compliant formats:

Although microscaling can unlock significant performance gains, applying it in practice introduces several challenges and depends heavily on the underlying hardware. Let‚Äôs first look at why applying microscaling on NVIDIA Blackwell GPUs can be particularly challenging.

This method naturally works well on the Hopper architecture (on which DSV3 was trained), because (1) the results of the tensor core matrix multiplies (via wgmma instruction) are accumulated in registers, and (2) you can pipeline matrix multiplies, asynchronously launching other tensor core matrix multiplies while performing dequantization with CUDA cores. Because everything is accumulated in the registers, no additional data movement is required between the matrix multiplies.

This is no longer the case on Blackwell GPUs due to tensor memory (TMEM). TMEM is a new set of on-chip, SM-local memory added on Blackwell GPUs. Unlike Hopper GPUs, where the results of tensor core matrix multiplies accumulate directly in registers, Blackwell GPUs accumulate them in TMEM (via tcgen05.mma instruction). To perform custom arithmetic on the accumulators, you must transfer the results from TMEM to registers, process them with CUDA cores, write them back to TMEM, and then wait for all of these instructions to complete because the data movements are asynchronous. Although TMEM is faster to access than shared memory (SMEM), this still kills tensor core occupancy.

In grouped matrix multiplications for MoE training, it‚Äôs common for MM to be very large compared to KK and NN. So let‚Äôs take:

In total, this is approximately 2.9 GB worth of High Bandwidth Memory (HBM) reads and writes. Assuming a sustainable HBM throughput of 6.5 TB/s on B200s, an optimized FP8 quantization kernel would take:

Even though FP8 matrix multiplication is theoretically 2x faster than BF16 matrix multiplication, quantization time can truly kill the performance gain. The above analysis is also optimistic because it assumes a block-scaled FP8 matrix multiplication running at 3,300 TFLOP/s, which is much higher than what is typically achieved in end-to-end MoE training. In the worst case, the quantization could take longer than the FP8 matrix multiplication, thus making the overall FP8 computation slower than BF16.

Even though FP8 matrix multiplication is theoretically 2x faster than BF16 matrix multiplication, quantization time can truly kill the performance gain. The above analysis is also optimistic because it assumes a block-scaled FP8 matrix multiplication running at 3,300 TFLOP/s, which is much higher than what is typically achieved in end-to-end MoE training. In the worst case, the quantization could take longer than the FP8 matrix multiplication, thus making the overall FP8 computation slower than BF16.

Furthermore, NVIDIA‚Äôs MXFP8 tcgen05.mma instruction, the PTX instruction for tensor core matrix multiplication, requires a slightly unintuitive scale factor layout. With 32-block scaling, TransformerEngine or TorchAO quantization kernels return the scale factor in a naive M x N / 32 layout. This must then be reshaped, either in PyTorch or by fusing the reshaping logic into other kernels, both of which negatively affect performance. In practice, you really don‚Äôt want to process scale factors inside the matrix multiply kernels. The fastest way to load them is by taking the HBM ‚Üí SMEM (cp.async.bulk) path and then the SMEM ‚Üí TMEM (tcgen05.cp) path; the moment a scale detours through the register tile, the tensor vibe is dead.

Next, we explain how we addressed the aforementioned challenges, starting with our approach to quantization.

To match the training quality of BF16, we ran a series of low-precision experiments, measuring how much each recipe diverged from BF16. From these, we identified the one that yields training loss convergence nearly identical to that of BF16 for our workloads.

Finally, tcgen05.mma instruction requires the scale factor to reside in TMEM. However, there is no direct method to load the scales from HBM into TMEM. The fastest approach is to first load the data from HBM into on-chip SMEM using the cp.async.bulk.tensor instruction (leveraging the Tensor Memory Accelerator, or TMA), and then transfer it from SMEM to TMEM using the tcgen05.cp instruction. For this to work, scale factors must be stored in the layout expected by tcgen05.mma, as explained later in this post.

With this setup, we design a pipeline where certain warps continuously load input tiles and scale from HBM to SMEM, others move scales from SMEM to TMEM, others launch the MMAs, and some occasionally load the TMEM accumulator into registers, store it in SMEM, and TMA-store it back to HBM.

if (warpgroup_id < 2) { for (int i = 0; i < num_tiles; i++) { mbarrier_wait_for_final_matmul_completion(); // mbarrier.try_wait async_load_from_TMEM(reg, TMEM); // tcgen05.ld wait_for_load_completion(); // tcgen05.wait // This is iterated in the actual implementation to save SMEM store_to_SMEM(SMEM, reg); TMA_async_store(HBM, SMEM); // cp.async.bulk.tensor } } else { if (warp_id == 0) { for (int i = 0; i < num_tiles; i++) { for (int j = 0; j < num_iters; j++) { mbarrier_wait_for_matmul_completion(); // load input tiles TMA_async_load(SMEM, HBM); } } } else if (warp_id == 1) { for (int i = 0; i < num_tiles; i++) { for (int j = 0; j < num_iters; j++) { mbarrier_wait_for_tcgen05_cp_completion(); // load scales (HBM -> SMEM) TMA_load(SMEM, HBM); } } } else if (warp_id == 2) { for (int i = 0; i < num_tiles; i++) { for (int j = 0; j < num_iters; j++) { mbarrier_wait_for_matmul_completion(); mbarrier_wait_for_scale_SMEM_load(); // load scales (SMEM -> TMEM) load_to_TMEM(TMEM, SMEM); // tcgen05.cp } } } else if (cta_rank == 0) { // 2-CTA MMA is launched by a single CTA for (int i = 0; i < num_tiles; i++) { mbarrier_wait_for_TMEM_clear(); for (int j = 0; j < num_iters; j++) { mbarrier_wait_for_input_SMEM_load(); mbarrier_wait_for_scale_TMEM_load(); // tcgen05.mma.cta_group::2.mxf8f6f4.block_scale launch_matmul(SMEM, SMEM, TMEM); } } } }

When scale factors must also reside in TMEM, however, we can only execute a single 256x32x256 tcgen05.mma instruction at a time using just a 128x256 region of TMEM. As a result, performance degradation is unavoidable. For example, the throughput of a 16,384x16,384x16,384 FP8 matrix multiplication drops from 3,200 TFLOP/s to 3,040 TFLOP/s under this constraint.

These throughput numbers apply only to pure FP8 matrix multiplication. With MXFP8 block scaling, throughput inevitably decreases further due to TMEM pipelining overhead. In practice, we achieve around 2,750 TFLOP/s with L2 cache clearance for block-scaled MXFP8 matrix multiplication kernels. Even so, this remains ~1.83x faster than standard BF16 matrix multiplication, which typically reaches 1,500~1,550 TFLOP/s on optimal shapes. Not too bad of a start!

A standalone MXFP8 matrix multiplication kernel is a useful first step, but during MXFP8 MoE training, its applications are limited (e.g., shared expert scenarios). To fully support MoE in MXFP8, we need grouped matrix multiplication kernels, specifically:

Note that these variants are part of the reason we are building these kernels from scratch. To date, we have not found any open-source alternative that fully supports MXFP8 MoE training with 32-block scaling.

With this abstraction, implementing the grouped MXFP8 matrix multiplication variants reduces to designing the appropriate loop structures and assigning indices correctly before launching the above abstraction. However, naive looping can significantly hurt performance due to poor L2 cache utilization.

Maintaining high L2 cache utilization is critical. In our benchmarks, inefficient HBM access patterns could reduce performance by nearly 50% in grouped matrix multiplication kernels. To address this, we applied supergrouping, a heuristic from ThunderKittens kernels that maximizes L2 reuse by ensuring that the region of the output matrix computed by all 148 SMs at any given time is as square as possible. You can learn more about this by reading the linked kernel code above.

The closest open-source alternative on Blackwell for grouped Fprop/Dgrad/Wgrad is DeepSeek‚Äôs DeepGEMM. DeepGEMM differs slightly in that it uses 1x128 and 128x128 scale blocks for the A and B matrices, which comes at the cost of reduced accuracy. Still, it is the only alternative available, so we integrated DeepGEMM into our internal model training and profiled its performance. While DeepGEMM excelled on certain input shapes in our microbenchmarks, our end-to-end benchmarks showed that our kernels outperformed it for our workloads.

It is also important to note that the above benchmarks exclude quantization time. DeepGEMM does not provide optimized quantization kernels, and without them, end-to-end performance can quickly degrade. In the worst case, it can even be slower than BF16 training.

With these optimizations, we implemented an MXFP8 quantization kernel that sustains 6.2+ TB/s while producing the scale matrix in a layout directly compatible with tcgen05.mma. To the best of our knowledge, this is the fastest MXFP8 quantization kernel available for MoE training.

With these optimizations, we implemented an MXFP8 quantization kernel that sustains 6.2+ TB/s while producing the scale matrix in a layout directly compatible with tcgen05.mma. To the best of our knowledge, this is the fastest MXFP8 quantization kernel available for MoE training.

3.5x faster MoE layer execution for both the forward and backward passes while maintaining the same training quality

With all the optimizations described above, we achieved 3.5x faster MoE layer execution for both the forward and backward passes while maintaining the same training quality. When tested on one of our internal models, this translated into a 1.5x end-to-end training speedup on Blackwell and a 2x speedup compared to our original Hopper setup, measured in tokens per second per GPU (TPS/GPU). We believe that our stack runs MXFP8 MoE training faster than any combination of open-source alternatives.

We still have plenty of work ahead and many kernels left to optimize. Our current efforts focus on building more efficient multi-GPU communication, improving custom attention kernels, and preparing to transition to FP4 for future MoE training runs.

If you are interested in writing and optimizing high-performance kernels, training large coding models, or our work broadly, we‚Äôd love to hear from you. Reach out to us at hiring@anysphere.co.

self.__next_f.push([1,"5:[\"$\",\"html\",null,{\"lang\":\"en-US\",\"suppressHydrationWarning\":true,\"className\":\"__variable_81079b __variable_58c592\",\"children\":[\"$\",\"body\",null,{\"className\":\"bg-theme-bg text-theme-text min-h-screen pt-[var(--site-header-height)]\",\"children\":[[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"Organization\\\",\\\"@id\\\":\\\"https://cursor.com/#org\\\",\\\"name\\\":\\\"Cursor\\\",\\\"description\\\":\\\"Cursor is the best way to code with AI.\\\",\\\"url\\\":\\\"https://cursor.com\\\",\\\"logo\\\":{\\\"@type\\\":\\\"ImageObject\\\",\\\"url\\\":\\\"https://cursor.com/public/opengraph-image.png\\\",\\\"width\\\":200,\\\"height\\\":60},\\\"sameAs\\\":[\\\"https://twitter.com/cursor_ai\\\",\\\"https://www.linkedin.com/company/cursorai\\\"],\\\"contactPoint\\\":{\\\"@type\\\":\\\"ContactPoint\\\",\\\"contactType\\\":\\\"customer service\\\",\\\"url\\\":\\\"https://cursor.com/contact-sales\\\"},\\\"foundingDate\\\":\\\"2023\\\"}\"}}],[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"WebSite\\\",\\\"@id\\\":\\\"https://cursor.com/#website\\\",\\\"name\\\":\\\"Cursor\\\",\\\"description\\\":\\\"Cursor is the best way to code with AI.\\\",\\\"url\\\":\\\"https://cursor.com\\\",\\\"publisher\\\":{\\\"@type\\\":\\\"Organization\\\",\\\"@id\\\":\\\"https://cursor.com/#org\\\"}}\"}}],\"$L11\"]}]}]\n"])

self.__next_f.push([1,"11:[\"$\",\"$L12\",null,{\"dictionary\":{\"next\":\"Next\",\"previous\":\"Previous\",\"back\":\"Back\",\"home\":\"Home\",\"blog\":\"Blog\",\"changelog\":\"Changelog\",\"topics\":\"Topics\",\"older posts\":\"Older posts\",\"newer posts\":\"Newer posts\",\"previous post\":\"Previous post\",\"next post\":\"Next post\",\"view more posts\":\"View more posts\",\"view all changelog entries\":\"See what‚Äôs new in Cursor\",\"by\":\"by\",\"in\":\"in\",\"author\":\"Author\",\"filed under\":\"Filed under\",\"submit\":\"Submit\",\"submitting...\":\"Submitting...\",\"company name\":\"Company name\",\"company size (engineers)\":\"Company size (Engineers)\",\"select company size\":\"Select company size\",\"additional details? (optional)\":\"Additional details? (Optional)\",\"how are you looking to use cursor?\":\"How are you looking to use Cursor?\",\"recommended\":\"Recommended\",\"features\":\"Features\",\"feature\":\"Feature\",\"download\":\"Download\",\"upgrade\":\"Upgrade\",\"pricing\":\"Pricing\",\"in progress\":\"In progress\",\"loading\":\"Loading\",\"error\":\"Error\",\"success\":\"Success\",\"reset\":\"Reset\",\"disable\":\"Disable\",\"enable\":\"Enable\",\"save\":\"Save\",\"cancel\":\"Cancel\",\"edit\":\"Edit\",\"delete\":\"Delete\",\"yes\":\"Yes\",\"no\":\"No\",\"all\":\"All\",\"none\":\"None\",\"select\":\"Select\",\"search\":\"Search\",\"menu\":\"Menu\",\"close\":\"Close\",\"open\":\"Open\",\"language\":\"Language\",\"english\":\"English\",\"japanese\":\"Japanese\",\"chinese\":\"Chinese\",\"all posts\":\"All Posts\",\"company\":\"Company\",\"news\":\"News\",\"product\":\"Product\",\"research\":\"Research\",\"stories\":\"Stories\",\"soc 2 certified\":\"SOC 2 Certified\",\"latest from cursor\":\"Latest from Cursor\"},\"dictionaryTranslations\":{\"next\":\"Next\",\"previous\":\"Previous\",\"back\":\"Back\",\"home\":\"Home\",\"blog\":\"Blog\",\"changelog\":\"Changelog\",\"topics\":\"Topics\",\"older posts\":\"Older posts\",\"newer posts\":\"Newer posts\",\"previous post\":\"Previous post\",\"next post\":\"Next post\",\"view more posts\":\"View more posts\",\"view all changelog entries\":\"See what‚Äôs new in Cursor\",\"by\":\"by\",\"in\":\"in\",\"author\":\"Author\",\"filed under\":\"Filed under\",\"submit\":\"Submit\",\"submitting...\":\"Submitting...\",\"company name\":\"Company name\",\"company size (engineers)\":\"Company size (Engineers)\",\"select company size\":\"Select company size\",\"additional details? (optional)\":\"Additional details? (Optional)\",\"how are you looking to use cursor?\":\"How are you looking to use Cursor?\",\"recommended\":\"Recommended\",\"features\":\"Features\",\"feature\":\"Feature\",\"download\":\"Download\",\"upgrade\":\"Upgrade\",\"pricing\":\"Pricing\",\"in progress\":\"In progress\",\"loading\":\"Loading\",\"error\":\"Error\",\"success\":\"Success\",\"reset\":\"Reset\",\"disable\":\"Disable\",\"enable\":\"Enable\",\"save\":\"Save\",\"cancel\":\"Cancel\",\"edit\":\"Edit\",\"delete\":\"Delete\",\"yes\":\"Yes\",\"no\":\"No\",\"all\":\"All\",\"none\":\"None\",\"select\":\"Select\",\"search\":\"Search\",\"menu\":\"Menu\",\"close\":\"Close\",\"open\":\"Open\",\"language\":\"Language\",\"english\":\"English\",\"japanese\":\"Japanese\",\"chinese\":\"Chinese\",\"all posts\":\"All Posts\",\"company\":\"Company\",\"news\":\"News\",\"product\":\"Product\",\"research\":\"Research\",\"stories\":\"Stories\",\"soc 2 certified\":\"SOC 2 Certified\",\"latest from cursor\":\"Latest from Cursor\"},\"translations\":{},\"locale\":\"en-US\",\"locales\":[\"en-US\",\"cn\",\"ja\",\"zh-Hant\"],\"defaultLocale\":\"en-US\",\"translationRequired\":false,\"dialectTranslationRequired\":false,\"region\":\"$undefined\",\"gtServicesEnabled\":true,\"projectId\":\"prj_jdh3imw75wenlfdi2jf1n41k\",\"translationEnabled\":true,\"runtimeUrl\":\"https://runtime2.gtx.dev\",\"devApiKey\":\"$undefined\",\"dictionaryEnabled\":true,\"renderSettings\":{\"method\":\"default\",\"timeout\":8000},\"developmentApiEnabled\":false,\"localeRoutingEnabledCookieName\":\"generaltranslation.locale-routing-enabled\",\"referrerLocaleCookieName\":\"generaltranslation.referrer-locale\",\"localeCookieName\":\"generaltranslation.locale\",\"resetLocaleCookieName\":\"generaltranslation.locale-reset\",\"customMapping\":{\"cn\":{\"code\":\"zh\"}},\"children\":[\"$\",\"$L13\",null,{\"enableSystem\":true,\"defaultTheme\":\"system\",\"disableTransitionOnChange\":true,\"storageKey\":\"marketing-theme\",\"children\":[\"$\",\"$L14\",null,{\"children\":[[\"$\",\"$L15\",null,{}],[\"$\",\"$e\",null,{\"fallback\":null,\"children\":\"$L16\"}],\"$L17\",[\"$\",\"$L18\",null,{}],\"$L19\",[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[\"$L1a\",[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/marketing-static/_next/static/css/8739a6a13754aafd.css?dpl=dpl_9sfZMY2911UstTVCyjNMFa1nrAJW\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"/marketing-static/_next/static/css/400864fd90c96a10.css?dpl=dpl_9sfZMY2911UstTVCyjNMFa1nrAJW\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}],\"$L1b\",\"$L1c\",\"$L1d\"]}]}]}]\n"])

self.__next_f.push([1,"1a:[\"$\",\"$L22\",null,{\"latestVersion\":{\"linux\":[{\"downloadUrl\":\"https://api2.cursor.sh/updates/download/golden/linux-arm64-deb/cursor/1.7\",\"label\":\"Linux .deb (ARM64)\"},{\"downloadUrl\":\"https://api2.cursor.sh/updates/download/golden/linux-x64-deb/cursor/1.7\",\"label\":\"Linux .deb (x64)\"},{\"downloadUrl\":\"https://api2.cursor.sh/updates/download/golden/linux-arm64-rpm/cursor/1.7\",\"label\":\"Linux RPM (ARM64)\"},{\"downloadUrl\":\"https://api2.cursor.sh/updates/download/golden/linux-x64-rpm/cursor/1.7\",\"label\":\"Linux RPM (x64)\"},{\"downloadUrl\":\"https://api2.cursor.sh/updates/download/golden/linux-arm64/cursor/1.7\",\"label\":\"Linux AppImage (ARM64)\"},{\"downloadUrl\":\"https://api2.cursor.sh/updates/download/golden/linux-x64/cursor/1.7\",\"label\":\"Linux AppImage (x64)\"}],\"macOS\":[{\"downloadUrl\":\"https://api2.cursor.sh/updates/download/golden/darwin-arm64/cursor/1.7\",\"label\":\"Mac (ARM64)\"},{\"downloadUrl\":\"https://api2.cursor.sh/updates/download/golden/darwin-x64/cursor/1.7\",\"label\":\"Mac (x64)\"},{\"downloadUrl\":\"https://api2.cursor.sh/updates/download/golden/darwin-universal/cursor/1.7\",\"label\":\"Mac Universal\"}],\"versionNumber\":\"1.7\",\"windows\":[{\"downloadUrl\":\"https://api2.cursor.sh/updates/download/golden/win32-x64/cursor/1.7\",\"label\":\"Windows (x64) (System)\"},{\"downloadUrl\":\"https://api2.cursor.sh/updates/download/golden/win32-x64-user/cursor/1.7\",\"label\":\"Windows (x64) (User)\"},{\"downloadUrl\":\"https://api2.cursor.sh/updates/download/golden/win32-arm64/cursor/1.7\",\"label\":\"Windows (ARM64) (System)\"},{\"downloadUrl\":\"https://api2.cursor.sh/updates/download/golden/win32-arm64-user/cursor/1.7\",\"label\":\"Windows (ARM64) (User)\"}]},\"children\":[\"$\",\"$L23\",null,{\"blogData\":[{\"_id\":\"5da81d1d-3524-44c0-84d2-c848bb69bd6b\",\"category\":\"product\",\"date\":\"2025-10-07T17:00:00.000Z\",\"excerpt\":\"Cursor can now create plans, research your codebase, and run agents for significantly longer.\",\"externalTitle\":null,\"externalUrl\":null,\"language\":\"en-US\",\"slug\":\"plan-mode\",\"thumbnailImage\":null,\"title\":\"Introducing Plan Mode\"},{\"_id\":\"b14b4ccd-b904-42d8-a224-578ab1aa6605\",\"category\":\"product\",\"date\":\"2025-10-01T17:20:00.000Z\",\"excerpt\":\"We‚Äôre speeding up the Java Language Server Protocol (LSP) and VS Code ecosystem.\",\"externalTitle\":null,\"externalUrl\":null,\"language\":\"en-US\",\"slug\":\"java\",\"thumbnailImage\":{\"_type\":\"image\",\"alt\":null,\"asset\":{\"metadata\":{\"dimensions\":null,\"isLandscape\":null,\"isPortrait\":null,\"isSquare\":true},\"url\":null},\"darkImage\":{\"metadata\":{\"dimensions\":null,\"isLandscape\":null,\"isPortrait\":null,\"isSquare\":true},\"url\":null},\"darkMobileImage\":{\"metadata\":{\"dimensions\":null},\"url\":null},\"mobileImage\":{\"metadata\":{\"dimensions\":null},\"url\":null},\"thumbnailType\":\"large\"},\"title\":\"Improving Java support in Cursor\"},{\"_id\":\"28698eca-aada-4254-ba86-bb7a1b5f43b8\",\"category\":\"research\",\"date\":\"2025-09-12T01:16:00.000Z\",\"excerpt\":\"Our new Tab model makes 21% fewer suggestions while having 28% higher accept rate.\",\"externalTitle\":null,\"externalUrl\":null,\"language\":\"en-US\",\"slug\":\"tab-rl\",\"thumbnailImage\":null,\"title\":\"Improving Cursor Tab with online RL\"},{\"_id\":\"9968b97c-6865-47af-970b-c644716aabe2\",\"category\":\"research\",\"date\":\"2025-08-29T02:55:25.007Z\",\"excerpt\":\"Achieving a 3.5x MoE layer speedup with a complete rebuild for Blackwell GPUs.\",\"externalTitle\":null,\"externalUrl\":null,\"language\":null,\"slug\":\"kernels\",\"thumbnailImage\":null,\"title\":\"1.5x faster MoE training with custom MXFP8 kernels\"}],\"children\":[\"$\",\"$L24\",null,{\"changelogData\":[{\"_id\":\"853c90ae-0e66-4ce0-9766-6e406e98a083\",\"date\":\"2025-09-29T05:08:58.822Z\",\"language\":\"en-US\",\"slug\":\"1-7\",\"title\":\"Browser Controls, Plan Mode, and Hooks\",\"version\":\"1.7\"},{\"_id\":\"8fc8a135-b0d9-4ad7-9db7-957768e793e6\",\"date\":\"2025-09-12T01:25:00.000Z\",\"language\":null,\"slug\":\"1-6\",\"title\":\"Slash commands, summarization, and improved Agent terminal\",\"version\":\"1.6\"},{\"_id\":\"04da9023-36ee-44e7-ae5c-15fea1269749\",\"date\":\"2025-08-21T19:54:00.000Z\",\"language\":\"en-US\",\"slug\":\"1-5\",\"title\":\"Linear integration, improved Agent terminal, and OS notifications\",\"version\":\"1.5\"},{\"_id\":\"d9e5960a-631a-4140-a2df-efe79921b56e\",\"date\":\"2025-08-06T19:44:00.000Z\",\"language\":null,\"slug\":\"1-4\",\"title\":\"Improved Agent tools, steerability, and usage visibility\",\"version\":\"1.4\"}],\"children\":\"$L25\"}]}]}]\n"])

self.__next_f.push([1,"25:[\"$\",\"main\",null,{\"id\":\"main\",\"data-sanity\":\"id=notFoundPage;type=page;path=pageBuilder;base=http%3A%2F%2Flocalhost%3A3333\",\"children\":[\"$L26\"]}]\n"])

self.__next_f.push([1,"26:[\"$\",\"section\",\"c6a92b6344f6,c6a92b6344f6\",{\"className\":\"section bg-theme-bg text-theme-text section--flush-y\",\"id\":\"$undefined\",\"data-sanity\":\"id=notFoundPage;type=page;path=pageBuilder:c6a92b6344f6;base=http%3A%2F%2Flocalhost%3A3333\",\"children\":[\"$\",\"div\",null,{\"className\":\"container\",\"children\":[\"$\",\"div\",null,{\"className\":\"text-left max-w-prose\",\"children\":[false,\"$undefined\",[\"$\",\"h1\",null,{\"className\":\"type-md-lg text-balance \",\"children\":\"Page not found (404)\"}],[\"$\",\"div\",null,{\"className\":\"flex justify-start mb-v1\",\"children\":[\"$\",\"div\",null,{\"className\":\"type-md-lg text-theme-text-sec stack text-balance\",\"children\":[[\"$\",\"p\",\"9f1e4b93d80d\",{\"children\":[\"Tabbed into the void.\"]}]]}]}],[\"$\",\"div\",null,{\"className\":\"gap-x-g1 flex items-center justify-start \",\"children\":[\"$L27\"]}]]}]}]}]\n"])

self.__next_f.push([1,"19:[\"$\",\"header\",null,{\"className\":\"bg-theme-bg px-g2 fixed top-0 left-0 z-50 w-full\",\"children\":[\"$\",\"div\",null,{\"className\":\"relative z-2 container grid h-[var(--site-header-height)] grid-cols-[1fr_auto_auto] items-center lg:grid-cols-[auto_1fr_auto]\",\"children\":[\"$L29\",[\"$\",\"div\",null,{\"className\":\"col-start-1 col-end-2 row-start-1 row-end-2\",\"children\":[\"$\",\"$L2a\",null,{\"title\":\"Cursor\"}]}],[\"$\",\"$L2b\",null,{\"mainNavigation\":{\"_createdAt\":\"2025-06-25T17:34:29Z\",\"_id\":\"6d03b6cc-bfae-4884-abf9-c8fcae321aa8\",\"_rev\":\"kjFDLJNvlIZ5LZwJBGfqJ8\",\"_system\":{\"base\":{\"id\":\"6d03b6cc-bfae-4884-abf9-c8fcae321aa8\",\"rev\":\"3JcS3A5GjOK7pB9VbWt8ez\"}},\"_type\":\"mainNavigation\",\"_updatedAt\":\"2025-10-03T17:54:03Z\",\"items\":[{\"children\":null,\"link\":{\"_type\":\"link\",\"file\":null,\"href\":null,\"label\":\"Features\",\"linkType\":\"page\",\"openInNewTab\":null,\"page\":\"features\",\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"subnavColumns\":\"one\",\"title\":null,\"type\":\"link\"},{\"children\":null,\"link\":{\"_type\":\"link\",\"file\":null,\"href\":null,\"label\":\"Enterprise\",\"linkType\":\"page\",\"openInNewTab\":false,\"page\":\"enterprise\",\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"subnavColumns\":null,\"title\":null,\"type\":\"link\"},{\"children\":null,\"link\":{\"_type\":\"link\",\"file\":null,\"href\":\"/pricing\",\"label\":\"Pricing\",\"linkType\":\"href\",\"openInNewTab\":false,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"subnavColumns\":null,\"title\":null,\"type\":\"link\"},{\"children\":[{\"_key\":\"a3696f9e6cdb\",\"_type\":\"link\",\"file\":null,\"href\":\"/changelog\",\"label\":\"Changelog\",\"linkType\":\"href\",\"openInNewTab\":false,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},{\"_key\":\"b647cebb79d2\",\"_type\":\"link\",\"file\":null,\"href\":\"/blog\",\"label\":\"Blog\",\"linkType\":\"href\",\"openInNewTab\":false,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},{\"_key\":\"af349d1d2f5f\",\"_type\":\"link\",\"file\":null,\"href\":\"/docs\",\"label\":\"Docs\",\"linkType\":\"href\",\"openInNewTab\":false,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},{\"_key\":\"447badf81268\",\"_type\":\"link\",\"file\":null,\"href\":null,\"label\":\"Community\",\"linkType\":\"page\",\"openInNewTab\":false,\"page\":\"community\",\"post\":null,\"postLanguage\":null,\"simplePage\":null},{\"_key\":\"a064252bc419\",\"_type\":\"link\",\"file\":null,\"href\":\"https://forum.cursor.com/\",\"label\":\"Forum\",\"linkType\":\"href\",\"openInNewTab\":true,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},{\"_key\":\"deea9a680819\",\"_type\":\"link\",\"file\":null,\"href\":\"https://anysphere.inc/\",\"label\":\"Careers\",\"linkType\":\"href\",\"openInNewTab\":true,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null}],\"link\":{\"_type\":\"link\",\"file\":null,\"href\":\"/changelog\",\"label\":\"Resources\",\"linkType\":\"href\",\"openInNewTab\":false,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"subnavColumns\":\"two\",\"title\":\"Resources\",\"type\":\"link\"}],\"language\":\"en-US\",\"navigationCta1\":{\"icon\":null,\"link\":{\"_type\":\"link\",\"file\":null,\"href\":\"https://cursor.com/dashboard\",\"label\":\"Sign in\",\"linkType\":\"href\",\"openInNewTab\":true,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"size\":\"small\",\"variant\":\"ghost\"},\"navigationCta2\":{\"icon\":null,\"link\":{\"_type\":\"link\",\"file\":null,\"href\":\"/download\",\"label\":\"Download\",\"linkType\":\"href\",\"openInNewTab\":null,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"size\":\"small\",\"variant\":\"primary\"}},\"currentLocale\":\"en-US\"}],[\"$\",\"div\",null,{\"className\":\"gap-g0.75 col-start-2 col-end-3 row-start-1 row-end-2 flex items-center justify-self-end lg:col-start-3 lg:col-end-[-1]\",\"children\":[\"$L2c\",\"$L2d\"]}]]}]}]\n"])

self.__next_f.push([1,"1b:[\"$\",\"footer\",null,{\"className\":\"pt-v3 pb-v3 md:pb-g3 px-g2 bg-theme-card-hex relative\",\"children\":[[\"$\",\"div\",null,{\"className\":\"mb-v4.5 container\",\"children\":[\"$\",\"nav\",null,{\"children\":[\"$\",\"div\",null,{\"className\":\"gap-x-g1 gap-y-v2 grid grid-cols-2 md:grid-cols-5\",\"children\":[[\"$\",\"div\",\"0\",{\"children\":[[\"$\",\"h3\",null,{\"className\":\"type-base md:type-sm text-theme-text-sec pb-v2.5/12\",\"children\":\"Product\"}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",\"0\",{\"children\":[\"$\",\"$L28\",null,{\"link\":{\"_key\":\"bd3849c1404f\",\"_type\":\"link\",\"file\":null,\"href\":null,\"label\":\"Features\",\"linkType\":\"page\",\"openInNewTab\":false,\"page\":\"features\",\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"className\":\"type-base md:type-sm py-v2.5/12 footer-link inline-block\",\"currentLocale\":\"en-US\",\"children\":[\"Features\",\"$undefined\"]}]}],[\"$\",\"li\",\"1\",{\"children\":[\"$\",\"$L28\",null,{\"link\":{\"_key\":\"059fd2fbd5cc\",\"_type\":\"link\",\"file\":null,\"href\":null,\"label\":\"Enterprise\",\"linkType\":\"page\",\"openInNewTab\":false,\"page\":\"enterprise\",\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"className\":\"type-base md:type-sm py-v2.5/12 footer-link inline-block\",\"currentLocale\":\"en-US\",\"children\":[\"Enterprise\",\"$undefined\"]}]}],[\"$\",\"li\",\"2\",{\"children\":[\"$\",\"$L28\",null,{\"link\":{\"_key\":\"d7ed5ceda74a\",\"_type\":\"link\",\"file\":null,\"href\":null,\"label\":\"Bugbot\",\"linkType\":\"page\",\"openInNewTab\":false,\"page\":\"bugbot\",\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"className\":\"type-base md:type-sm py-v2.5/12 footer-link inline-block\",\"currentLocale\":\"en-US\",\"children\":[\"Bugbot\",\"$undefined\"]}]}],[\"$\",\"li\",\"3\",{\"children\":[\"$\",\"$L28\",null,{\"link\":{\"_key\":\"2d8d3619c635\",\"_type\":\"link\",\"file\":null,\"href\":null,\"label\":\"CLI\",\"linkType\":\"page\",\"openInNewTab\":false,\"page\":\"cli\",\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"className\":\"type-base md:type-sm py-v2.5/12 footer-link inline-block\",\"currentLocale\":\"en-US\",\"children\":[\"CLI\",\"$undefined\"]}]}],[\"$\",\"li\",\"4\",{\"children\":[\"$\",\"$L28\",null,{\"link\":{\"_key\":\"4a9c5bbe338c\",\"_type\":\"link\",\"file\":null,\"href\":\"/pricing\",\"label\":\"Pricing\",\"linkType\":\"href\",\"openInNewTab\":false,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"className\":\"type-base md:type-sm py-v2.5/12 footer-link inline-block\",\"currentLocale\":\"en-US\",\"children\":[\"Pricing\",\"$undefined\"]}]}]]}]]}],[\"$\",\"div\",\"1\",{\"children\":[[\"$\",\"h3\",null,{\"className\":\"type-base md:type-sm text-theme-text-sec pb-v2.5/12\",\"children\":\"Resources\"}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",\"0\",{\"children\":[\"$\",\"$L28\",null,{\"link\":{\"_key\":\"88cade86b69f\",\"_type\":\"link\",\"file\":null,\"href\":\"/download\",\"label\":\"Download\",\"linkType\":\"href\",\"openInNewTab\":false,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"className\":\"type-base md:type-sm py-v2.5/12 footer-link inline-block\",\"currentLocale\":\"en-US\",\"children\":[\"Download\",\"$undefined\"]}]}],[\"$\",\"li\",\"1\",{\"children\":[\"$\",\"$L28\",null,{\"link\":{\"_key\":\"ac54cf6b8655\",\"_type\":\"link\",\"file\":null,\"href\":\"/agents\",\"label\":\"Web Agents\",\"linkType\":\"href\",\"openInNewTab\":true,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"className\":\"type-base md:type-sm py-v2.5/12 footer-link inline-block\",\"currentLocale\":\"en-US\",\"children\":[\"Web Agents\",\"$undefined\"]}]}],[\"$\",\"li\",\"2\",{\"children\":[\"$\",\"$L28\",null,{\"link\":{\"_key\":\"437357adbec9\",\"_type\":\"link\",\"file\":null,\"href\":\"/changelog\",\"label\":\"Changelog\",\"linkType\":\"href\",\"openInNewTab\":false,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"className\":\"type-base md:type-sm py-v2.5/12 footer-link inline-block\",\"currentLocale\":\"en-US\",\"children\":[\"Changelog\",\"$undefined\"]}]}],[\"$\",\"li\",\"3\",{\"children\":[\"$\",\"$L28\",null,{\"link\":{\"_key\":\"02984c67264f\",\"_type\":\"link\",\"file\":null,\"href\":\"/docs\",\"label\":\"Docs\",\"linkType\":\"href\",\"openInNewTab\":false,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"className\":\"type-base md:type-sm py-v2.5/12 footer-link inline-block\",\"currentLocale\":\"en-US\",\"children\":[\"Docs\",\"$undefined\"]}]}],[\"$\",\"li\",\"4\",{\"children\":[\"$\",\"$L28\",null,{\"link\":{\"_key\":\"162f532a26d5\",\"_type\":\"link\",\"file\":null,\"href\":\"https://forum.cursor.com/\",\"label\":\"Forum\",\"linkType\":\"href\",\"openInNewTab\":true,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"className\":\"type-base md:type-sm py-v2.5/12 footer-link inline-block\",\"currentLocale\":\"en-US\",\"children\":[\"Forum\",[\"$\",\"span\",null,{\"className\":\"footer-link__icon\",\"children\":\" ‚Üó\"}]]}]}],[\"$\",\"li\",\"5\",{\"children\":[\"$\",\"$L28\",null,{\"link\":{\"_key\":\"15adb9498c9b\",\"_type\":\"link\",\"file\":null,\"href\":\"https://status.cursor.com/\",\"label\":\"Status\",\"linkType\":\"href\",\"openInNewTab\":true,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"className\":\"type-base md:type-sm py-v2.5/12 footer-link inline-block\",\"currentLocale\":\"en-US\",\"children\":[\"Status\",[\"$\",\"span\",null,{\"className\":\"footer-link__icon\",\"children\":\" ‚Üó\"}]]}]}]]}]]}],[\"$\",\"div\",\"2\",{\"children\":[[\"$\",\"h3\",null,{\"className\":\"type-base md:type-sm text-theme-text-sec pb-v2.5/12\",\"children\":\"Company\"}],[\"$\",\"ul\",null,{\"children\":[\"$L2e\",\"$L2f\",\"$L30\",\"$L31\",\"$L32\"]}]]}],\"$L33\",\"$L34\"]}]}]}],\"$L35\"]}]\n"])

self.__next_f.push([1,"a:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"1.5x faster MoE training with custom MXFP8 kernels ¬∑ Cursor\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Achieving a 3.5x MoE layer speedup with a complete rebuild for Blackwell GPUs.\"}],[\"$\",\"meta\",\"2\",{\"name\":\"author\",\"content\":\"Stuart Sul\"}],[\"$\",\"link\",\"3\",{\"rel\":\"manifest\",\"href\":\"/marketing-static/manifest.webmanifest\",\"crossOrigin\":\"$undefined\"}],[\"$\",\"meta\",\"4\",{\"name\":\"preconnect\",\"content\":\"https://cdn.sanity.io\"}],[\"$\",\"link\",\"5\",{\"rel\":\"canonical\",\"href\":\"https://cursor.com/blog/kernels\"}],[\"$\",\"link\",\"6\",{\"rel\":\"alternate\",\"hrefLang\":\"en-US\",\"href\":\"https://cursor.com/blog/kernels\"}],[\"$\",\"link\",\"7\",{\"rel\":\"alternate\",\"hrefLang\":\"cn\",\"href\":\"https://cursor.com/cn/blog/kernels\"}],[\"$\",\"link\",\"8\",{\"rel\":\"alternate\",\"hrefLang\":\"ja\",\"href\":\"https://cursor.com/ja/blog/kernels\"}],[\"$\",\"link\",\"9\",{\"rel\":\"alternate\",\"hrefLang\":\"zh-Hant\",\"href\":\"https://cursor.com/zh-Hant/blog/kernels\"}],[\"$\",\"meta\",\"10\",{\"property\":\"og:title\",\"content\":\"1.5x faster MoE training with custom MXFP8 kernels ¬∑ Cursor\"}],[\"$\",\"meta\",\"11\",{\"property\":\"og:description\",\"content\":\"Built to make you extraordinarily productive, Cursor is the best way to code with AI.\"}],[\"$\",\"meta\",\"12\",{\"property\":\"og:url\",\"content\":\"https://cursor.com/blog/kernels\"}],[\"$\",\"meta\",\"13\",{\"property\":\"og:site_name\",\"content\":\"Cursor\"}],[\"$\",\"meta\",\"14\",{\"property\":\"og:locale\",\"content\":\"en-US\"}],[\"$\",\"meta\",\"15\",{\"property\":\"og:image\",\"content\":\"https://cdn.sanity.io/images/2hv88549/production/581af892e88b9b7e17a19773730e5d9907c03cec-2401x1260.png?auto=format\u0026w=1200\u0026h=627\u0026fit=crop\"}],[\"$\",\"meta\",\"16\",{\"property\":\"og:image:width\",\"content\":\"1200\"}],[\"$\",\"meta\",\"17\",{\"property\":\"og:image:height\",\"content\":\"627\"}],[\"$\",\"meta\",\"18\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"19\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"20\",{\"name\":\"twitter:title\",\"content\":\"1.5x faster MoE training with custom MXFP8 kernels ¬∑ Cursor\"}],[\"$\",\"meta\",\"21\",{\"name\":\"twitter:description\",\"content\":\"Built to make you extraordinarily productive, Cursor is the best way to code with AI.\"}],[\"$\",\"meta\",\"22\",{\"name\":\"twitter:image\",\"content\":\"https://cdn.sanity.io/images/2hv88549/production/581af892e88b9b7e17a19773730e5d9907c03cec-2401x1260.png?auto=format\u0026w=1200\u0026h=627\u0026fit=crop\"}],[\"$\",\"meta\",\"23\",{\"name\":\"twitter:image:width\",\"content\":\"1200\"}],[\"$\",\"meta\",\"24\",{\"name\":\"twitter:image:height\",\"content\":\"627\"}],[\"$\",\"link\",\"25\",{\"rel\":\"apple-touch-icon\",\"href\":\"/marketing-static/apple-touch-icon.png\"}],[\"$\",\"link\",\"26\",{\"rel\":\"icon\",\"href\":\"/marketing-static/icon-192x192.png\",\"sizes\":\"192x192\",\"type\":\"image/png\"}],[\"$\",\"link\",\"27\",{\"rel\":\"icon\",\"href\":\"/marketing-static/icon-512x512.png\",\"sizes\":\"512x512\",\"type\":\"image/png\"}],[\"$\",\"$L36\",\"28\",{}]],\"error\":null,\"digest\":\"$undefined\"}\n"])

self.__next_f.push([1,"34:[\"$\",\"div\",\"4\",{\"children\":[[\"$\",\"h3\",null,{\"className\":\"type-base md:type-sm text-theme-text-sec pb-v2.5/12\",\"children\":\"Connect\"}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",\"0\",{\"children\":[\"$\",\"$L28\",null,{\"link\":{\"_key\":\"a47878555d9f\",\"_type\":\"link\",\"file\":null,\"href\":\"https://x.com/cursor_ai\",\"label\":\"X\",\"linkType\":\"href\",\"openInNewTab\":true,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"className\":\"type-base md:type-sm py-v2.5/12 footer-link inline-block\",\"currentLocale\":\"en-US\",\"children\":[\"X\",[\"$\",\"span\",null,{\"className\":\"footer-link__icon\",\"children\":\" ‚Üó\"}]]}]}],[\"$\",\"li\",\"1\",{\"children\":[\"$\",\"$L28\",null,{\"link\":{\"_key\":\"04d633aeb0d5\",\"_type\":\"link\",\"file\":null,\"href\":\"https://www.linkedin.com/company/cursorai\",\"label\":\"LinkedIn\",\"linkType\":\"href\",\"openInNewTab\":true,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"className\":\"type-base md:type-sm py-v2.5/12 footer-link inline-block\",\"currentLocale\":\"en-US\",\"children\":[\"LinkedIn\",[\"$\",\"span\",null,{\"className\":\"footer-link__icon\",\"children\":\" ‚Üó\"}]]}]}],[\"$\",\"li\",\"2\",{\"children\":[\"$\",\"$L28\",null,{\"link\":{\"_key\":\"79a67a3532b1\",\"_type\":\"link\",\"file\":null,\"href\":\"https://www.youtube.com/@cursor_ai\",\"label\":\"YouTube\",\"linkType\":\"href\",\"openInNewTab\":true,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"className\":\"type-base md:type-sm py-v2.5/12 footer-link inline-block\",\"currentLocale\":\"en-US\",\"children\":[\"YouTube\",[\"$\",\"span\",null,{\"className\":\"footer-link__icon\",\"children\":\" ‚Üó\"}]]}]}]]}]]}]\n"])

self.__next_f.push([1,"35:[\"$\",\"div\",null,{\"className\":\"gap-v2 container flex flex-col items-start justify-between md:flex-row md:items-center\",\"children\":[[\"$\",\"div\",null,{\"className\":\"text-theme-text-sec gap-g1.5 flex items-center\",\"children\":[[\"$\",\"small\",null,{\"className\":\"type-base md:type-sm\",\"children\":[\"¬© \",2025,\" \",[\"$\",\"a\",null,{\"href\":\"https://anysphere.inc\",\"className\":\"hover:text-theme-text active:text-theme-text\",\"children\":\"Anysphere, Inc.\"}]]}],[\"$\",\"small\",null,{\"className\":\"type-base md:type-sm\",\"children\":[\"üõ° \",[\"$\",\"$L28\",null,{\"link\":{\"linkType\":\"href\",\"href\":\"/security\"},\"currentLocale\":\"en-US\",\"className\":\"hover:text-theme-text active:text-theme-text inline\",\"children\":\"$L37\"}]]}]]}],[\"$\",\"div\",null,{\"className\":\"gap-g1.5 flex items-center\",\"children\":[[\"$\",\"$L38\",null,{}],[\"$\",\"$L39\",null,{}]]}]]}]\n"])

self.__next_f.push([1,"29:[\"$\",\"a\",null,{\"href\":\"#main\",\"className\":\"skipnav btn btn--sm\",\"children\":\"Skip to content\",\"data-_gt\":\"$undefined\"}]\n2c:[\"$\",\"$L28\",null,{\"link\":\"$19:props:children:props:children:2:props:mainNavigation:navigationCta1:link\",\"className\":\"btn--ghost btn--sm\",\"ctaTracking\":\"$undefined\",\"download\":\"$undefined\",\"onClick\":\"$undefined\",\"children\":[\"Sign in\",null]}]\n2d:[\"$\",\"$L28\",null,{\"link\":\"$19:props:children:props:children:2:props:mainNavigation:navigationCta2:link\",\"className\":\"max-sm:hidden btn btn--sm\",\"ctaTracking\":\"$undefined\",\"download\":\"$undefined\",\"onClick\":\"$undefined\",\"children\":[\"Download\",null]}]\n"])

self.__next_f.push([1,"6:[\"$\",\"main\",null,{\"className\":\"section section--longform\",\"id\":\"main\",\"children\":[[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"BlogPosting\\\",\\\"@id\\\":\\\"https://cursor.com/blog/kernels\\\",\\\"headline\\\":\\\"1.5x faster MoE training with custom MXFP8 kernels\\\",\\\"description\\\":\\\"Achieving a 3.5x MoE layer speedup with a complete rebuild for Blackwell GPUs.\\\",\\\"datePublished\\\":\\\"2025-08-29T02:55:25.007Z\\\",\\\"dateModified\\\":\\\"2025-08-29T02:55:25.007Z\\\",\\\"author\\\":{\\\"@type\\\":\\\"Person\\\",\\\"name\\\":\\\"Stuart Sul\\\"},\\\"publisher\\\":{\\\"@type\\\":\\\"Organization\\\",\\\"@id\\\":\\\"https://cursor.com/#org\\\"},\\\"mainEntityOfPage\\\":{\\\"@type\\\":\\\"WebPage\\\",\\\"@id\\\":\\\"https://cursor.com/blog/kernels\\\"},\\\"url\\\":\\\"https://cursor.com/blog/kernels\\\",\\\"inLanguage\\\":\\\"en-US\\\",\\\"isPartOf\\\":{\\\"@type\\\":\\\"Blog\\\",\\\"name\\\":\\\"Cursor Blog\\\",\\\"url\\\":\\\"https://cursor.com/blog\\\"},\\\"articleSection\\\":\\\"research\\\",\\\"wordCount\\\":3731}\"}}],[\"$\",\"section\",null,{\"className\":\"container\",\"children\":[[\"$\",\"article\",null,{\"className\":\"grid-cursor border-theme-border-02 mb-v3 gap-y-0\",\"children\":[[\"$\",\"div\",null,{\"className\":\"col-span-full xl:col-start-1 xl:col-end-7 max-xl:hidden\",\"children\":[\"$\",\"div\",null,{\"className\":\"top-[var(--site-sticky-top)] lg:sticky\",\"children\":[\"$\",\"div\",null,{\"className\":\"type-base mb-v2/12 relative z-1\",\"children\":[\"$L3a\",[\"$\",\"span\",null,{\"className\":\"text-theme-text-sec\",\"children\":\"/\"}],\" \",[\"$\",\"$L3b\",null,{\"className\":\"text-theme-text-sec hover:text-theme-text active:text-theme-text-sec capitalize\",\"href\":\"/blog/topic/research\",\"children\":\"Research\"}]]}]}]}],[\"$\",\"div\",null,{\"className\":\"grid grid-cols-subgrid *:col-span-full col-span-full md:col-start-1 md:col-end-19 lg:col-start-1 lg:col-end-17 xl:col-start-7 xl:col-end-19\",\"children\":[[\"$\",\"header\",null,{\"className\":\"mb-v2\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"type-lg text-balance\",\"children\":\"1.5x faster MoE training with custom MXFP8 kernels\"}],[\"$\",\"div\",null,{\"className\":\"type-base text-theme-text-sec mt-v4/12\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2025-08-29T02:55:25.007Z\",\"className\":\"$undefined\",\"children\":\"$L3c\"}],\" by \",[\"$\",\"span\",null,{\"children\":\"Stuart Sul\"}],[\"$\",\"span\",null,{\"className\":\"xl:hidden\",\"children\":[\"$L3d\",[\"$\",\"$L3b\",null,{\"className\":\"text-theme-text-sec hover:text-theme-text active:text-theme-text-sec capitalize\",\"href\":\"/blog/topic/research\",\"children\":\"Research\"}]]}]]}],[\"$\",\"p\",null,{\"className\":\"type-md-sm mt-v1 text-pretty\",\"children\":\"Achieving a 3.5x MoE layer speedup with a complete rebuild for Blackwell GPUs.\"}],\"$undefined\"]}],[\"$\",\"nav\",null,{\"id\":\"contents\",\"className\":\"card card--text mb-v2 type-sm\",\"children\":[\"$L3e\",[\"$\",\"div\",null,{\"className\":\"grid grid-cols-1 sm:grid-cols-2 gap-x-g1.5\",\"children\":[[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",\"be112b934010\",{\"children\":[\"$\",\"a\",null,{\"href\":\"#a-quick-introduction-to-the-microscaling-mx-data-formats\",\"className\":\"py-v1/12 hover:text-theme-text-sec block \",\"children\":\"A quick introduction to the Microscaling (MX) data formats\"}]}],[\"$\",\"li\",\"d718817ebe1e\",{\"children\":[\"$\",\"a\",null,{\"href\":\"#1-tensor-memory-and-cuda-cores-kill-the-dequantization-vibe\",\"className\":\"py-v1/12 hover:text-theme-text-sec block \",\"children\":\"1. Tensor memory and CUDA cores kill the dequantization vibe\"}]}],[\"$\",\"li\",\"a7a91e3ae229\",{\"children\":[\"$\",\"a\",null,{\"href\":\"#2-death-by-a-thousand-quantizations\",\"className\":\"py-v1/12 hover:text-theme-text-sec block \",\"children\":\"2. Death by a thousand quantizations\"}]}],[\"$\",\"li\",\"dd0736e4dbdc\",{\"children\":[\"$\",\"a\",null,{\"href\":\"#choosing-the-right-low-precision-recipe\",\"className\":\"py-v1/12 hover:text-theme-text-sec block \",\"children\":\"Choosing the right low-precision recipe\"}]}],[\"$\",\"li\",\"2b22a63f842f\",{\"children\":[\"$\",\"a\",null,{\"href\":\"#embracing-tcgen05-mxfp8-block-scaled-matrix-multiplication\",\"className\":\"py-v1/12 hover:text-theme-text-sec block \",\"children\":\"Embracing tcgen05 MXFP8 block-scaled matrix multiplication\"}]}],[\"$\",\"li\",\"e0015fc7a153\",{\"children\":\"$L3f\"}]]}],\"$L40\"]}]]}],\"$L41\",\"$L42\"]}],null]}],\"$L43\"]}]]}]\n"])

self.__next_f.push([1,"41:[\"$\",\"div\",null,{\"data-sanity\":\"id=9968b97c-6865-47af-970b-c644716aabe2;type=post;path=content;base=%2F\",\"children\":[\"$\",\"div\",null,{\"className\":\"prose prose--block col-span-full!\",\"children\":[[\"$\",\"p\",\"5fda9647f4a9\",{\"children\":[\"We want to build the world‚Äôs best AI coding models, but training large language models can be expensive. For instance, our largest internal models can take weeks to train on tens of thousands of GPUs. This is not only computationally expensive, but it also slows the pace at which improvements reach our users.\"]}],[\"$\",\"p\",\"9c67c0f9f77d\",{\"children\":[\"We recently began upgrading from Hopper GPUs (H100s) to Blackwell GPUs (B200s) and saw this as an opportunity to deeply optimize our training workloads. Profiling revealed that the main bottleneck was the Mixture-of-Experts (MoE) layer, implemented with \",[\"$\",\"$L28\",\"a43e9fdb0088\",{\"link\":{\"_key\":\"0d23b5e8c683\",\"_type\":\"link\",\"file\":null,\"href\":\"https://github.com/databricks/megablocks\",\"label\":null,\"linkType\":null,\"openInNewTab\":null,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"children\":[\"MegaBlocks\"]}],\", which accounted for nearly 53% of forward-pass time and 27% of backward-pass time.\"]}],[\"$\",\"p\",\"9e9e22853b37\",{\"children\":[\"That‚Äôs why, over the past few weeks, \",[\"$\",\"strong\",\"6e75c09dbcd6\",{\"children\":[\"we rewrote the entire MoE layer from scratch at the GPU kernel level\"]}],\" with zero dependencies on any CUDA libraries. Instead, we used pure, good old CUDA and PTX, with a few bits of \",[\"$\",\"$L28\",\"e256e91486ac\",{\"link\":{\"_key\":\"f4319d4c3950\",\"_type\":\"link\",\"file\":null,\"href\":\"https://github.com/HazyResearch/ThunderKittens\",\"label\":null,\"linkType\":null,\"openInNewTab\":null,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"children\":[\"ThunderKittens\"]}],\" sprinkled in. As a result, we achieved a \",[\"$\",\"strong\",\"16b40e67bd6b\",{\"children\":[\"3.5x improvement in MoE layer performance for both the forward and backward passes\"]}],\", translating to a 1.5x end-to-end training speedup on Blackwell and a 2x speedup compared to our original Hopper setup. We believe our stack is faster than any combination of open-source alternatives available today.\"]}],[\"$\",\"figure\",\"4c8ada893f4e\",{\"className\":\"stack\",\"children\":[[\"$\",\"div\",null,{\"className\":\"media-border-container overflow-hidden\",\"children\":[[\"$\",\"picture\",null,{\"className\":\"block h-full w-full\",\"children\":[[\"$\",\"source\",null,{\"media\":\"(min-width: 661px)\",\"srcSet\":\"$44\"}],\"$undefined\",\"$L45\"]}],\"$undefined\"]}],\"$L46\"]}],\"$L47\",\"$L48\",\"$L49\",\"$L4a\",\"$L4b\",\"$L4c\",\"$L4d\",\"$L4e\",\"$L4f\",\"$L50\",\"$L51\",\"$L52\",\"$L53\",\"$L54\",\"$L55\",\"$L56\",\"$L57\",\"$L58\",\"$L59\",\"$L5a\",\"$L5b\",\"$L5c\",\"$L5d\",\"$L5e\",\"$L5f\",\"$L60\",\"$L61\",\"$L62\",\"$L63\",\"$L64\",\"$L65\",\"$L66\",\"$L67\",\"$L68\",\"$L69\",\"$L6a\",\"$L6b\",\"$L6c\",\"$L6d\",\"$L6e\",\"$L6f\",\"$L70\",\"$L71\",\"$L72\",\"$L73\",\"$L74\",\"$L75\",\"$L76\",\"$L77\",\"$L78\",\"$L79\",\"$L7a\",\"$L7b\",\"$L7c\",\"$L7d\",\"$L7e\",\"$L7f\",\"$L80\",\"$L81\",\"$L82\",\"$L83\",\"$L84\",\"$L85\",\"$L86\",\"$L87\",\"$L88\",\"$L89\",\"$L8a\",\"$L8b\",\"$L8c\",\"$L8d\",\"$L8e\",\"$L8f\",\"$L90\",\"$L91\",\"$L92\",\"$L93\",\"$L94\",\"$L95\",\"$L96\",\"$L97\",\"$L98\",\"$L99\",\"$L9a\",\"$L9b\",\"$L9c\",\"$L9d\",\"$L9e\",\"$L9f\",\"$La0\",\"$La1\",\"$La2\",\"$La3\",\"$La4\",\"$La5\",\"$La6\",\"$La7\",\"$La8\",\"$La9\",\"$Laa\",\"$Lab\",\"$Lac\",\"$Lad\",\"$Lae\",\"$Laf\",\"$Lb0\",\"$Lb1\",\"$Lb2\",\"$Lb3\",\"$Lb4\",\"$Lb5\",\"$Lb6\",\"$Lb7\",\"$Lb8\",\"$Lb9\"]}]}]\n"])

self.__next_f.push([1,"47:[\"$\",\"p\",\"0f2a61daabf8\",{\"children\":[\"Most of this improvement came from transitioning from BF16 to \",[\"$\",\"$L28\",\"31d1ced08103\",{\"link\":{\"_key\":\"97897c578c04\",\"_type\":\"link\",\"file\":null,\"href\":\"https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf\",\"label\":null,\"linkType\":null,\"openInNewTab\":null,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"children\":[\"MXFP8\"]}],\" which we achieved with nearly zero loss in training quality. But this also taught us that going low precision is easier said than done. If not done carefully, MXFP8 training may offer minimal performance improvement over BF16 due to various kernel overheads. The MXFP8 training recipe is also not widely shared, meaning you have to discover the right approach yourself.\"]}]\n"])

self.__next_f.push([1,"48:[\"$\",\"p\",\"789832f69457\",{\"children\":[\"The following is our recipe, and a look at the ML work we do at Cursor.\"]}]\n49:[\"$\",\"h2\",\"be112b934010\",{\"id\":\"a-quick-introduction-to-the-microscaling-mx-data-formats\",\"children\":[\"$\",\"a\",null,{\"href\":\"#a-quick-introduction-to-the-microscaling-mx-data-formats\",\"className\":\"hover:opacity-90\",\"children\":[\"A quick introduction to the Microscaling (MX) data formats\"]}]}]\n4a:[\"$\",\"p\",\"024437313f03\",{\"children\":[\"A common way to reduce the computational cost of large deep learning models is to use lower-precision activations and weights. However, converting them to narrow bit-width formats (e.g., 8 or fewer bits) introduces unacceptable rounding error unless the values are scaled appropriately. For example, some weights in a large model might be 0.0001, 0.0005, or ‚Äì0.0007, but naively converting these to FP8 would round them all to the same number: zero, since the smallest positive value representable in FP8E4M3 is around 0.0019.\"]}]\n4b:[\"$\",\"p\",\"c1f5a0ebc9b1\",{\"children\":[\"To address this, it is common to apply a per-tensor scaling factor, which rescales the tensor while keeping its values within the representable range of the target data format. This ensures that the available dynamic range is fully utilized. For example, if a tensor‚Äôs values are all between ‚Äì0.0001 and 0.0001, its scaling factor could be set to 4,480,000. This would cause the tensor‚Äôs values after scaling to be in the range [‚Äì448, 448], which corresponds to the representable bounds of FP8E4M3.\"]}]\n"])

self.__next_f.push([1,"4c:[\"$\",\"p\",\"e6a993e484d5\",{\"children\":[\"Microscaling goes a step further by applying scaling to fine-grained sub-blocks of a tensor, rather than using a single scale factor for the entire tensor. The \",[\"$\",\"strong\",\"bb733d440d74\",{\"children\":[\"Microscaling (MX) format\"]}],\" standardizes this approach by defining a set of low-precision, micro-scaled data formats. Full details are provided in its \",[\"$\",\"$L28\",\"9c47fbc7b2ba\",{\"link\":{\"_key\":\"22ce403a8298\",\"_type\":\"link\",\"file\":null,\"href\":\"https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf\",\"label\":null,\"linkType\":null,\"openInNewTab\":null,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"children\":[\"specification\"]}],\", which defines the following concrete MX-compliant formats:\"]}]\n"])

self.__next_f.push([1,"4d:[\"$\",\"figure\",\"3ffd91973e97\",{\"className\":\"stack\",\"children\":[[\"$\",\"div\",null,{\"className\":\"media-border-container overflow-hidden\",\"children\":[[\"$\",\"picture\",null,{\"className\":\"block h-full w-full\",\"children\":[[\"$\",\"source\",null,{\"media\":\"(min-width: 661px)\",\"srcSet\":\"$bd\"}],\"$undefined\",[\"$\",\"img\",null,{\"className\":\"object-cover h-full w-full\",\"alt\":\"Table 1. Concrete MX-compliant formats.\",\"src\":\"https://cdn.sanity.io/images/2hv88549/production/15f3628154d637c898669518b69a2b765d05b429-1296x438.png?auto=format\",\"loading\":\"lazy\",\"decoding\":\"async\",\"width\":\"$undefined\",\"height\":\"$undefined\",\"fetchPriority\":\"auto\"}]]}],\"$undefined\"]}],[\"$\",\"figcaption\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"Table 1. Concrete MX-compliant formats.\"}}]]}]\n"])

self.__next_f.push([1,"4f:[\"$\",\"figure\",\"b8e9c7c8b34f\",{\"className\":\"stack\",\"children\":[[\"$\",\"div\",null,{\"className\":\"media-border-container overflow-hidden\",\"children\":[[\"$\",\"picture\",null,{\"className\":\"block h-full w-full\",\"children\":[[\"$\",\"source\",null,{\"media\":\"(min-width: 661px)\",\"srcSet\":\"$be\"}],\"$undefined\",[\"$\",\"img\",null,{\"className\":\"object-cover h-full w-full\",\"alt\":\"Figure 2. MXFP8 quantization example: Each 1x32 block shares a scale.\",\"src\":\"https://cdn.sanity.io/images/2hv88549/production/ebb87778c1965e02f192e5ba73cf9805be7a9b14-1600x707.png?auto=format\",\"loading\":\"lazy\",\"decoding\":\"async\",\"width\":\"$undefined\",\"height\":\"$undefined\",\"fetchPriority\":\"auto\"}]]}],\"$undefined\"]}],[\"$\",\"figcaption\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"Figure 2. MXFP8 quantization example: Each 1x32 block shares a scale.\"}}]]}]\n"])

self.__next_f.push([1,"50:[\"$\",\"p\",\"aef2fc1d4a56\",{\"children\":[\"Although microscaling can unlock significant performance gains, applying it in practice introduces several challenges and depends heavily on the underlying hardware. Let‚Äôs first look at why applying microscaling on NVIDIA Blackwell GPUs can be particularly challenging.\"]}]\n51:[\"$\",\"h2\",\"d718817ebe1e\",{\"id\":\"1-tensor-memory-and-cuda-cores-kill-the-dequantization-vibe\",\"children\":[\"$\",\"a\",null,{\"href\":\"#1-tensor-memory-and-cuda-cores-kill-the-dequantization-vibe\",\"className\":\"hover:opacity-90\",\"children\":[\"1. Tensor memory and CUDA cores kill the dequantization vibe\"]}]}]\n52:[\"$\",\"p\",\"a8afffb5ce95\",{\"children\":[\"In microscaled FP8 matrix multiplication, the computation is broken into smaller block-sized steps along the reduction dimension. After each blocked matrix multiplication, the partial result is \",[\"$\",\"strong\",\"cf43eceb7448\",{\"children\":[\"dequantized\"]}],\" using scale factors and then accumulated before proceeding to the next block.\"]}]\n"])

self.__next_f.push([1,"54:[\"$\",\"p\",\"3e89bcecebbb\",{\"children\":[\"1. Perform a 128-block matrix multiplication:\"]}]\n55:[\"$\",\"$Lbf\",\"f9157833cc1b\",{\"code\":\"C_block = A[:, k*128:(k+1)*128] @ B[k*128:(k+1)*128, :]\",\"language\":\"$undefined\"}]\n56:[\"$\",\"p\",\"79153a412daf\",{\"children\":[\"2. Apply dequantization using scale factors:\"]}]\n57:[\"$\",\"$Lbf\",\"88fe5eea94fb\",{\"code\":\"C_block = A_scale[:, k] * C_block * B_scale[k, :]\",\"language\":\"$undefined\"}]\n58:[\"$\",\"p\",\"06ae7eb47092\",{\"children\":[\"where \",[\"$\",\"code\",\"7b6041d3e7d9\",{\"children\":[\"A_scale\"]}],\" is broadcast across 128 elements along the K dimension and \",[\"$\",\"code\",\"c9127d5c5359\",{\"children\":[\"B_scale\"]}],\" is broadcast across 128 elements along both dimensions\"]}]\n59:[\"$\",\"p\",\"317e4a951733\",{\"children\":[\"3. Accumulate the dequantized block:\"]}]\n5a:[\"$\",\"$Lbf\",\"043da3c78a9d\",{\"code\":\"C += C_block\",\"language\":\"$undefined\"}]\n5b:[\"$\",\"p\",\"a26aecaab9fc\",{\"children\":[\"4. Proceed to the next block along the reduction dimension:\"]}]\n5c:[\"$\",\"$Lbf\",\"128e77dbfb63\",{\"code\":\"k += 1\",\"language\":\"$undefined\"}]\n5d:[\"$\",\"p\",\"ac3e8f34d463\",{\"children\":[\"This method naturally works well on the Hopper architecture (on which DSV3 was trained), because (1) the results of the tensor core matrix multiplies (via \",[\"$\",\"code\",\"24762b5ae353\",{\"children\":[\"wgmma\"]}],\" instruction) are accumulated in registers, and (2) you can pipeline matrix multiplies, asynchronously launching other tensor core matrix multiplies while performing dequantization with CUDA cores. Because everything is accumulated in the registers, no additional data movement is required between the matrix multiplies.\"]}]\n"])

self.__next_f.push([1,"5e:[\"$\",\"p\",\"279cbafcd1a4\",{\"children\":[\"This is no longer the case on Blackwell GPUs due to \",[\"$\",\"$L28\",\"464947b220be\",{\"link\":{\"_key\":\"cf00246bd85e\",\"_type\":\"link\",\"file\":null,\"href\":\"https://docs.nvidia.com/cuda/parallel-thread-execution/#tensor-memory\",\"label\":null,\"linkType\":null,\"openInNewTab\":null,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"children\":[\"tensor memory\"]}],\" (TMEM). TMEM is a new set of on-chip, SM-local memory added on Blackwell GPUs. Unlike Hopper GPUs, where the results of tensor core matrix multiplies accumulate directly in registers, Blackwell GPUs accumulate them in TMEM (via \",[\"$\",\"code\",\"4e0943d8cd93\",{\"children\":[\"tcgen05.mma\"]}],\" instruction). To perform custom arithmetic on the accumulators, you must transfer the results from TMEM to registers, process them with CUDA cores, write them back to TMEM, and then wait for all of these instructions to complete because the data movements are asynchronous. Although TMEM is faster to access than shared memory (SMEM), this still kills tensor core occupancy.\"]}]\n"])

self.__next_f.push([1,"5f:[\"$\",\"figure\",\"7602789a5c05\",{\"className\":\"stack\",\"children\":[[\"$\",\"div\",null,{\"className\":\"media-border-container overflow-hidden\",\"children\":[[\"$\",\"picture\",null,{\"className\":\"block h-full w-full\",\"children\":[[\"$\",\"source\",null,{\"media\":\"(min-width: 661px)\",\"srcSet\":\"$c0\"}],\"$undefined\",[\"$\",\"img\",null,{\"className\":\"object-cover h-full w-full\",\"alt\":\"Figure 3. Gantt chart taken from our custom Blackwell attention kernel. First row shows the tensor core activity (QK\u003csup\u003eT\u003c/sup\u003e). Second row shows the CUDA core activity (TMEM ‚Üí registers, then softmax). TMEM ‚Üí register latency causes the tensor core pipeline bubble.\",\"src\":\"https://cdn.sanity.io/images/2hv88549/production/c3cccca43b64e61118c66bfb5f283a6b9a610000-626x670.png?auto=format\",\"loading\":\"lazy\",\"decoding\":\"async\",\"width\":\"$undefined\",\"height\":\"$undefined\",\"fetchPriority\":\"auto\"}]]}],\"$undefined\"]}],[\"$\",\"figcaption\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"Figure 3. Gantt chart taken from our custom Blackwell attention kernel. First row shows the tensor core activity (QK\u003csup\u003eT\u003c/sup\u003e). Second row shows the CUDA core activity (TMEM ‚Üí registers, then softmax). TMEM ‚Üí register latency causes the tensor core pipeline bubble.\"}}]]}]\n"])

self.__next_f.push([1,"68:[\"$\",\"p\",\"9797f931acc6\",{\"children\":[\"In grouped matrix multiplications for MoE training, it‚Äôs common for \",[\"$\",\"span\",\"91dbe01d1ec7\",{\"className\":\"inline-block leading-none [\u0026_.katex]:[font-size:inherit]\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"katex\\\"\u003e\u003cmath xmlns=\\\"http://www.w3.org/1998/Math/MathML\\\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eM\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\\\"application/x-tex\\\"\u003eM\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\"}}],\" to be very large compared to \",[\"$\",\"span\",\"cc5a420930b6\",{\"className\":\"inline-block leading-none [\u0026_.katex]:[font-size:inherit]\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"katex\\\"\u003e\u003cmath xmlns=\\\"http://www.w3.org/1998/Math/MathML\\\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eK\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\\\"application/x-tex\\\"\u003eK\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\"}}],\" and \",[\"$\",\"span\",\"6a86bb27da70\",{\"className\":\"inline-block leading-none [\u0026_.katex]:[font-size:inherit]\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"katex\\\"\u003e\u003cmath xmlns=\\\"http://www.w3.org/1998/Math/MathML\\\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eN\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\\\"application/x-tex\\\"\u003eN\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\"}}],\". So let‚Äôs take:\"]}]\n"])

self.__next_f.push([1,"70:[\"$\",\"p\",\"314758de34ab\",{\"children\":[\"In total, this is approximately 2.9 GB worth of High Bandwidth Memory (HBM) reads and writes. Assuming a sustainable HBM throughput of 6.5 TB/s on B200s, an optimized FP8 quantization kernel would take:\"]}]\n71:[\"$\",\"div\",\"871029b016a5\",{\"className\":\"math-wrapper\",\"children\":[\"$\",\"span\",null,{\"className\":\"block prose-math leading-none [\u0026_.katex]:[font-size:inherit]\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"katex\\\"\u003e\u003cmath xmlns=\\\"http://www.w3.org/1998/Math/MathML\\\" display=\\\"block\\\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmn\u003e2.9\u003c/mn\u003e\u003cmtext\u003e GB \u003c/mtext\u003e\u003cmi mathvariant=\\\"normal\\\"\u003e/\u003c/mi\u003e\u003cmtext\u003e \u003c/mtext\u003e\u003cmn\u003e6.5\u003c/mn\u003e\u003cmtext\u003e TB/s\u003c/mtext\u003e\u003cmo\u003e‚âà\u003c/mo\u003e\u003cmn\u003e0.44\u003c/mn\u003e\u003cmtext\u003e ms\u003c/mtext\u003e\u003c/mrow\u003e\u003cannotation encoding=\\\"application/x-tex\\\"\u003e2.9 \\\\text{ GB } / \\\\text{ } 6.5 \\\\text{ TB/s} \\\\approx 0.44 \\\\text{ ms}\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\"}}]}]\n72:[\"$\",\"p\",\"aa203da6629e\",{\"children\":[\"That‚Äôs almost 40% of the matrix multiplication time. In a common scenario where we also transpose-quantize the A matrix for backward propagation, this doubles to 0.88 ms, \",[\"$\",\"strong\",\"a6638cdf16c7\",{\"children\":[\"about 76% of the matrix multiplication time\"]}],\"!\"]}]\n73:[\"$\",\"p\",\"10b02fc52dba\",{\"children\":[\"Even though FP8 matrix multiplication is theoretically 2x faster than BF16 matrix multiplication, quantization time can truly kill the performance gain. The above analysis is also \",[\"$\",\"em\",\"ccad6059ee3b\",{\"children\":[\"optimistic\"]}],\" because it assumes a block-scaled FP8 matrix multiplication running at 3,300 TFLOP/s, which is much higher than what is typically achieved in end-to-end MoE training. In the worst case, the quantization could take longer than the FP8 matrix multiplication, thus making the overall FP8 computation slower than BF16.\"]}]\n"])

self.__next_f.push([1,"74:[\"$\",\"p\",\"40d42f4abf89\",{\"children\":[\"There do exist some fast, open-source FP8 quantization kernels, namely from NVIDIA \",[\"$\",\"$L28\",\"42053c89ef8d\",{\"link\":{\"_key\":\"2950ffcde065\",\"_type\":\"link\",\"file\":null,\"href\":\"https://github.com/NVIDIA/TransformerEngine/blob/dd083bdfca296294322f2b5f8143ce54ee15db22/transformer_engine/common/util/cast_kernels.cuh#L1005\",\"label\":null,\"linkType\":null,\"openInNewTab\":null,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"children\":[\"TransformerEngine\"]}],\" or Pytorch \",[\"$\",\"$L28\",\"c4a96528a3a1\",{\"link\":{\"_key\":\"c379a4a1822f\",\"_type\":\"link\",\"file\":null,\"href\":\"https://github.com/pytorch/ao/blob/main/torchao/csrc/cuda/mx_kernels/mxfp8_quantize.cuh\",\"label\":null,\"linkType\":null,\"openInNewTab\":null,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"children\":[\"TorchAO\"]}],\". However, our microbenchmarks have shown that they leave bandwidth on the table, running at around 4.5 TB/s. Purely relying on intra-SM instructions (e.g., \",[\"$\",\"code\",\"f1e305af45b5\",{\"children\":[\"cp.async.bulk\"]}],\"), we know that Blackwell can easily run at 6~7 TB/s HBM bandwidth.\"]}]\n"])

self.__next_f.push([1,"75:[\"$\",\"p\",\"ca151e0c6513\",{\"children\":[\"Furthermore, NVIDIA‚Äôs MXFP8 \",[\"$\",\"code\",\"b626cb1b200e\",{\"children\":[\"tcgen05.mma\"]}],\" instruction, the PTX instruction for tensor core matrix multiplication, requires \",[\"$\",\"$L28\",\"1d6df61b2fe6\",{\"link\":{\"_key\":\"716f0ba152a1\",\"_type\":\"link\",\"file\":null,\"href\":\"https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-mma-scale-factor-a-layout-1x\",\"label\":null,\"linkType\":null,\"openInNewTab\":null,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"children\":[\"a slightly unintuitive\"]}],\" \",[\"$\",\"$L28\",\"2408a32d5f86\",{\"link\":{\"_key\":\"55610bebd03c\",\"_type\":\"link\",\"file\":null,\"href\":\"https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-mma-scale-factor-b\",\"label\":null,\"linkType\":null,\"openInNewTab\":null,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"children\":[\"scale factor layout\"]}],\". With 32-block scaling, TransformerEngine or TorchAO quantization kernels return the scale factor in a naive M x N / 32 layout. This must then be reshaped, either in PyTorch or by fusing the reshaping logic into other kernels, both of which negatively affect performance. In practice, you really don‚Äôt want to process scale factors inside the matrix multiply kernels. The fastest way to load them is by taking the HBM ‚Üí SMEM (\",[\"$\",\"code\",\"6f3812388b74\",{\"children\":[\"cp.async.bulk\"]}],\") path and then the SMEM ‚Üí TMEM (\",[\"$\",\"code\",\"7cbb802fb839\",{\"children\":[\"tcgen05.cp\"]}],\") path; the moment a scale detours through the register tile, the tensor vibe is dead.\"]}]\n"])

self.__next_f.push([1,"76:[\"$\",\"p\",\"83be78736bb2\",{\"children\":[\"Next, we explain how we addressed the aforementioned challenges, starting with our approach to quantization.\"]}]\n77:[\"$\",\"h2\",\"dd0736e4dbdc\",{\"id\":\"choosing-the-right-low-precision-recipe\",\"children\":[\"$\",\"a\",null,{\"href\":\"#choosing-the-right-low-precision-recipe\",\"className\":\"hover:opacity-90\",\"children\":[\"Choosing the right low-precision recipe\"]}]}]\n78:[\"$\",\"p\",\"2661c9c241ad\",{\"children\":[\"To match the training quality of BF16, we ran a series of low-precision experiments, measuring how much each recipe diverged from BF16. From these, we identified the one that yields training loss convergence nearly identical to that of BF16 for our workloads.\"]}]\n79:[\"$\",\"p\",\"f9f94f4de5e4\",{\"children\":[\"Specifically, we use the MXFP8 format with FP8E4M3 (4 exponent bits, 3 mantissa bits) as the element data type, FPE8M0 (8 exponent bits) as the scale data type, and a scaling block size of 32. We also adopt the MXFP8 quantization recipe from the paper, ‚Äú\",[\"$\",\"$L28\",\"be9032b738df\",{\"link\":{\"_key\":\"80e6e5b0253b\",\"_type\":\"link\",\"file\":null,\"href\":\"https://arxiv.org/pdf/2506.08027\",\"label\":null,\"linkType\":null,\"openInNewTab\":null,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"children\":[\"Recipes for Pre-training LLMs with MXFP8\"]}],\"‚Äù. Let:\"]}]\n"])

self.__next_f.push([1,"7f:[\"$\",\"p\",\"b509879ba24b\",{\"children\":[\"With this, our FP8 training loss matched the BF16 training loss:\"]}]\nc4:T5c8,"])

self.__next_f.push([1,"80:[\"$\",\"figure\",\"7c3862a09e5d\",{\"className\":\"stack\",\"children\":[[\"$\",\"div\",null,{\"className\":\"media-border-container overflow-hidden\",\"children\":[[\"$\",\"picture\",null,{\"className\":\"block h-full w-full\",\"children\":[[\"$\",\"source\",null,{\"media\":\"(min-width: 661px)\",\"srcSet\":\"$c4\"}],\"$undefined\",[\"$\",\"img\",null,{\"className\":\"object-cover h-full w-full\",\"alt\":\"Figure 4. BF16 vs MXFP8 Training Loss over 10k steps: nearly indistinguishable.\",\"src\":\"https://cdn.sanity.io/images/2hv88549/production/280679ef113ddd8ea6c6398f010ecbfe5582461d-1162x550.png?auto=format\",\"loading\":\"lazy\",\"decoding\":\"async\",\"width\":\"$undefined\",\"height\":\"$undefined\",\"fetchPriority\":\"auto\"}]]}],\"$undefined\"]}],[\"$\",\"figcaption\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"Figure 4. BF16 vs MXFP8 Training Loss over 10k steps: nearly indistinguishable.\"}}]]}]\n"])

self.__next_f.push([1,"81:[\"$\",\"figure\",\"d5d518d308e6\",{\"className\":\"stack\",\"children\":[[\"$\",\"div\",null,{\"className\":\"media-border-container overflow-hidden\",\"children\":[[\"$\",\"picture\",null,{\"className\":\"block h-full w-full\",\"children\":[[\"$\",\"source\",null,{\"media\":\"(min-width: 661px)\",\"srcSet\":\"$c5\"}],\"$undefined\",[\"$\",\"img\",null,{\"className\":\"object-cover h-full w-full\",\"alt\":\"Figure 5. BF16 vs MXFP8 Training Loss (9k‚Äì10k steps) with a specific data point at step 9832.\",\"src\":\"https://cdn.sanity.io/images/2hv88549/production/57d513f086a2c2a76d1a29cc930dfbc0640487d6-1148x522.png?auto=format\",\"loading\":\"lazy\",\"decoding\":\"async\",\"width\":\"$undefined\",\"height\":\"$undefined\",\"fetchPriority\":\"auto\"}]]}],\"$undefined\"]}],[\"$\",\"figcaption\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"Figure 5. BF16 vs MXFP8 Training Loss (9k‚Äì10k steps) with a specific data point at step 9832.\"}}]]}]\n"])

self.__next_f.push([1,"83:[\"$\",\"p\",\"b3293e556b69\",{\"children\":[\"On the NVIDIA Blackwell architecture, MX formats are baked into the tensor cores. Block scaling is invoked through the \",[\"$\",\"code\",\"009629f9bc5d\",{\"children\":[\"tcgen05.mma...block_scale\"]}],\" instruction and handled in hardware. This eliminates the need to move data out of TMEM for dequantization, since everything occurs during the tensor core matrix multiplication. Our MXFP8 matrix multiplication kernel design must therefore revolve around \",[\"$\",\"code\",\"5094ddf92e21\",{\"children\":[\"tcgen05.mma\"]}],\" and operate \",[\"$\",\"$L28\",\"64024b55cb6c\",{\"link\":{\"_key\":\"176f6bdc96ac\",\"_type\":\"link\",\"file\":null,\"href\":\"https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-kind-shapes\",\"label\":null,\"linkType\":null,\"openInNewTab\":null,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"children\":[\"within\"]}],\" \",[\"$\",\"$L28\",\"6f144595f5b5\",{\"link\":{\"_key\":\"5f5a8fe71c2b\",\"_type\":\"link\",\"file\":null,\"href\":\"https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-block-scaling\",\"label\":null,\"linkType\":null,\"openInNewTab\":null,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"children\":[\"its\"]}],\" \",[\"$\",\"$L28\",\"a00caf9f2ac8\",{\"link\":{\"_key\":\"a1077d1ff51f\",\"_type\":\"link\",\"file\":null,\"href\":\"https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-mma-instructions-mma\",\"label\":null,\"linkType\":null,\"openInNewTab\":null,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"children\":[\"constraints\"]}],\" to achieve maximum performance.\"]}]\n"])

self.__next_f.push([1,"87:[\"$\",\"p\",\"801165f79461\",{\"children\":[\"Finally, \",[\"$\",\"code\",\"ef3d7d294e6d\",{\"children\":[\"tcgen05.mma\"]}],\" instruction requires the scale factor to reside in TMEM. However, there is no direct method to load the scales from HBM into TMEM. The fastest approach is to first load the data from HBM into on-chip SMEM using the \",[\"$\",\"code\",\"d604311db580\",{\"children\":[\"cp.async.bulk.tensor\"]}],\" instruction (leveraging the Tensor Memory Accelerator, or TMA), and then transfer it from SMEM to TMEM using the \",[\"$\",\"code\",\"d22acb8f0364\",{\"children\":[\"tcgen05.cp\"]}],\" instruction. For this to work, scale factors must be stored in the layout expected by \",[\"$\",\"code\",\"b4c292d4cd2b\",{\"children\":[\"tcgen05.mma\"]}],\", as explained later in this post.\"]}]\n"])

self.__next_f.push([1,"8a:[\"$\",\"figure\",\"329ebde62cce\",{\"className\":\"stack\",\"children\":[[\"$\",\"div\",null,{\"className\":\"media-border-container overflow-hidden\",\"children\":[[\"$\",\"picture\",null,{\"className\":\"block h-full w-full\",\"children\":[[\"$\",\"source\",null,{\"media\":\"(min-width: 661px)\",\"srcSet\":\"$c6\"}],\"$undefined\",[\"$\",\"img\",null,{\"className\":\"object-cover h-full w-full\",\"alt\":\"Figure 6. Simplified TMEM allocation: accumulator region plus 5 slots each for A and B scales\",\"src\":\"https://cdn.sanity.io/images/2hv88549/production/646d1e04d975a6f4a25ac0d96b049e2103b93536-1600x791.png?auto=format\",\"loading\":\"lazy\",\"decoding\":\"async\",\"width\":\"$undefined\",\"height\":\"$undefined\",\"fetchPriority\":\"auto\"}]]}],\"$undefined\"]}],[\"$\",\"figcaption\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"Figure 6. Simplified TMEM allocation: accumulator region plus 5 slots each for A and B scales\"}}]]}]\n"])

self.__next_f.push([1,"8b:[\"$\",\"figure\",\"07d47d09d649\",{\"className\":\"stack\",\"children\":[[\"$\",\"div\",null,{\"className\":\"media-border-container overflow-hidden\",\"children\":[[\"$\",\"picture\",null,{\"className\":\"block h-full w-full\",\"children\":[[\"$\",\"source\",null,{\"media\":\"(min-width: 661px)\",\"srcSet\":\"$c7\"}],\"$undefined\",[\"$\",\"img\",null,{\"className\":\"object-cover h-full w-full\",\"alt\":\"Figure 7. Simplified SMEM allocation: 5 slots reserved for input tiles and scales\",\"src\":\"https://cdn.sanity.io/images/2hv88549/production/8e50ed0030b9181194a7aa8f7ffdb31254151d12-1530x804.png?auto=format\",\"loading\":\"lazy\",\"decoding\":\"async\",\"width\":\"$undefined\",\"height\":\"$undefined\",\"fetchPriority\":\"auto\"}]]}],\"$undefined\"]}],[\"$\",\"figcaption\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"Figure 7. Simplified SMEM allocation: 5 slots reserved for input tiles and scales\"}}]]}]\n"])

self.__next_f.push([1,"8c:[\"$\",\"p\",\"1eccded32c21\",{\"children\":[\"With this setup, we design a pipeline where certain warps continuously load input tiles and scale from HBM to SMEM, others move scales from SMEM to TMEM, others launch the MMAs, and some occasionally load the TMEM accumulator into registers, store it in SMEM, and TMA-store it back to HBM.\"]}]\nc8:T5c8,"])

self.__next_f.push([1,"8d:[\"$\",\"figure\",\"05e62b4736e8\",{\"className\":\"stack\",\"children\":[[\"$\",\"div\",null,{\"className\":\"media-border-container overflow-hidden\",\"children\":[[\"$\",\"picture\",null,{\"className\":\"block h-full w-full\",\"children\":[[\"$\",\"source\",null,{\"media\":\"(min-width: 661px)\",\"srcSet\":\"$c8\"}],\"$undefined\",[\"$\",\"img\",null,{\"className\":\"object-cover h-full w-full\",\"alt\":\"Figure 8. Simplified MXFP8 Matrix Multiplication Pipeline\",\"src\":\"https://cdn.sanity.io/images/2hv88549/production/e93dd5ec7f4779fbcd83932afb5b3b9c134494c8-1600x567.png?auto=format\",\"loading\":\"lazy\",\"decoding\":\"async\",\"width\":\"$undefined\",\"height\":\"$undefined\",\"fetchPriority\":\"auto\"}]]}],\"$undefined\"]}],[\"$\",\"figcaption\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"Figure 8. Simplified MXFP8 Matrix Multiplication Pipeline\"}}]]}]\n"])

self.__next_f.push([1,"if (warpgroup_id \u003c 2) {\n for (int i = 0; i \u003c num_tiles; i++) {\n mbarrier_wait_for_final_matmul_completion(); // mbarrier.try_wait\n async_load_from_TMEM(reg, TMEM); // tcgen05.ld\n wait_for_load_completion(); // tcgen05.wait\n // This is iterated in the actual implementation to save SMEM \n store_to_SMEM(SMEM, reg);\n TMA_async_store(HBM, SMEM); // cp.async.bulk.tensor\n }\n} else {\n if (warp_id == 0) {\n for (int i = 0; i \u003c num_tiles; i++) {\n for (int j = 0; j \u003c num_iters; j++) {\n mbarrier_wait_for_matmul_completion();\n // load input tiles\n TMA_async_load(SMEM, HBM);\n }\n }\n } else if (warp_id == 1) {\n for (int i = 0; i \u003c num_tiles; i++) {\n for (int j = 0; j \u003c num_iters; j++) {\n mbarrier_wait_for_tcgen05_cp_completion();\n // load scales (HBM -\u003e SMEM)\n TMA_load(SMEM, HBM);\n }\n }\n } else if (warp_id == 2) {\n for (int i = 0; i \u003c num_tiles; i++) {\n for (int j = 0; j \u003c num_iters; j++) {\n mbarrier_wait_for_matmul_completion();\n mbarrier_wait_for_scale_SMEM_load();\n // load scales (SMEM -\u003e TMEM)\n load_to_TMEM(TMEM, SMEM); // tcgen05.cp\n }\n }\n } else if (cta_rank == 0) { // 2-CTA MMA is launched by a single CTA\n for (int i = 0; i \u003c num_tiles; i++) {\n mbarrier_wait_for_TMEM_clear();\n for (int j = 0; j \u003c num_iters; j++) {\n mbarrier_wait_for_input_SMEM_load();\n mbarrier_wait_for_scale_TMEM_load();\n // tcgen05.mma.cta_group::2.mxf8f6f4.block_scale\n launch_matmul(SMEM, SMEM, TMEM);\n }\n }\n }\n}"])

self.__next_f.push([1,"90:[\"$\",\"$Lbf\",\"66cf6fa946f8\",{\"code\":\"$c9\",\"language\":\"csharp\"}]\n91:[\"$\",\"p\",\"5bb2d067e1f6\",{\"children\":[\"One unavoidable limitation in block-scaled matrix multiplication on Blackwell GPUs is the size of TMEM. Our microbenchmarks show that Blackwell tensor cores achieve the highest throughput when the full 128x512 TMEM is used as the accumulator. For 2-CTA FP8 matrix multiplication, this corresponds to consistently running two 256x32x256 \",[\"$\",\"code\",\"e0cf6fd04de1\",{\"children\":[\"tcgen05.mma\"]}],\" instructions. Each \",[\"$\",\"code\",\"054c63b9228c\",{\"children\":[\"tcgen05.mma\"]}],\" consumes a 128x256 region of TMEM per CTA, so two of them together fully occupy the 128x512 TMEM arrays across both CTAs.\"]}]\n92:[\"$\",\"p\",\"5d0e2f1290c2\",{\"children\":[\"When scale factors must also reside in TMEM, however, we can only execute a single 256x32x256 \",[\"$\",\"code\",\"12ed29c8a142\",{\"children\":[\"tcgen05.mma\"]}],\" instruction at a time using just a 128x256 region of TMEM. As a result, performance degradation is unavoidable. For example, the throughput of a 16,384x16,384x16,384 FP8 matrix multiplication drops from 3,200 TFLOP/s to 3,040 TFLOP/s under this constraint.\"]}]\n93:[\"$\",\"p\",\"cae34685be50\",{\"children\":[\"These throughput numbers apply only to pure FP8 matrix multiplication. With MXFP8 block scaling, throughput inevitably decreases further due to TMEM pipelining overhead. In practice, we achieve around 2,750 TFLOP/s with L2 cache clearance for block-scaled MXFP8 matrix multiplication kernels. Even so, this remains ~1.83x faster than standard BF16 matrix multiplication, which typically reaches 1,500~1,550 TFLOP/s on optimal shapes. Not too bad of a start!\"]}]\n94:[\"$\",\"h2\",\"e0015fc7a153\",{\"id\":\"expanding-to-mxfp8-grouped-matrix-multiplications\",\"children\":[\"$\",\"a\",null,{\"href\":\"#expanding-to-mxfp8-grouped-matrix-multiplications\",\"className\":\"hover:opacity-90\",\"children\":[\"Expanding to MXFP8 grouped matrix multiplications\"]}]}]\n95:[\"$\",\"p\",\"d27e82551fec\",{\"children\":[\"A standalone MXFP8 matrix multiplication kernel is a useful first"])

self.__next_f.push([1," step, but during MXFP8 MoE training, its applications are limited (e.g., shared expert scenarios). To fully support MoE in MXFP8, we need \",[\"$\",\"em\",\"7469248e07aa\",{\"children\":[\"grouped\"]}],\" matrix multiplication kernels, specifically:\"]}]\n96:[\"$\",\"ol\",\"57a347ce2b2f-parent\",{\"children\":[[\"$\",\"li\",\"57a347ce2b2f\",{\"children\":[\"Grouped forward propagation (Fprop) / data gradient (Dgrad)\"]}],[\"$\",\"li\",\"2d0c0fa25354\",{\"children\":[\"Grouped weight gradient (Wgrad)\"]}]]}]\n97:[\"$\",\"p\",\"17dceb408ecb\",{\"children\":[\"Note that these variants are part of the reason we are building these kernels from scratch. To date, we have not found any open-source alternative that fully supports MXFP8 MoE training with 32-block scaling.\"]}]\n98:[\"$\",\"p\",\"63f2a0dcd170\",{\"children\":[\"At the kernel level, grouped Fprop and Dgrad share the same structure. The only difference is that Dgrad requires accumulation, since the input tensor passes through both the up and gate projections. But this can be easily implemented by replacing the \",[\"$\",\"code\",\"f98ee55a042a\",{\"children\":[\"cp.async.bulk.tensor\"]}],\" instruction with \",[\"$\",\"code\",\"70ed60495746\",{\"children\":[\"cp.reduce.async.bulk.tensor\"]}],\", which can perform atomic add-store to HBM asynchronously.\"]}]\n99:[\"$\",\"p\",\"eee744d44581\",{\"children\":[\"Given the following matrices:\"]}]\n"])

self.__next_f.push([1," appropriate loop structures and assigning indices correctly before launching the above abstraction. However, naive looping can significantly hurt performance due to poor L2 cache utilization.\"]}]\na3:[\"$\",\"h3\",\"be6be2683b5f\",{\"id\":\"l2-cache-optimization-via-expert-wise-supergrouping\",\"children\":[\"$\",\"a\",null,{\"href\":\"#l2-cache-optimization-via-expert-wise-supergrouping\",\"className\":\"hover:opacity-90\",\"children\":[\"L2 cache optimization via expert-wise supergrouping\"]}]}]\n"])

self.__next_f.push([1,"a4:[\"$\",\"p\",\"e2d9ff88f51d\",{\"children\":[\"Maintaining high L2 cache utilization is critical. In our benchmarks, inefficient HBM access patterns could reduce performance by nearly 50% in grouped matrix multiplication kernels. To address this, we applied \",[\"$\",\"strong\",\"439a2af89d3c\",{\"children\":[\"supergrouping\"]}],\", \",[\"$\",\"$L28\",\"52adc4bb969e\",{\"link\":{\"_key\":\"881f99a12afa\",\"_type\":\"link\",\"file\":null,\"href\":\"https://github.com/HazyResearch/ThunderKittens/blob/6c27e28c8115d1839d9eeeb530913c184a75fc87/kernels/matmul/FP8/matmul.cu#L37\",\"label\":null,\"linkType\":null,\"openInNewTab\":null,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"children\":[\"a heuristic from ThunderKittens kernels\"]}],\" that maximizes L2 reuse by ensuring that the region of the output matrix computed by all 148 SMs at any given time is as square as possible. You can learn more about this by reading the linked kernel code above.\"]}]\n"])

self.__next_f.push([1,"a5:[\"$\",\"p\",\"a4274948edc0\",{\"children\":[\"A key enhancement for our grouped matrix multiplication kernels was applying supergrouping \",[\"$\",\"em\",\"9add6fc36f7b\",{\"children\":[\"per expert\"]}],\", considering only the submatrix belonging to the current expert rather than the entire output matrix. This proved especially effective for grouped Wgrad, where the reduction axis is often narrow due to expert partitioning. A narrow reduction axis leads to lower tensor core utilization, making memory bandwidth the primary bottleneck.\"]}]\na6:[\"$\",\"p\",\"132f7f63dd38\",{\"children\":[\"With the right matrix multiplication kernel abstraction and expert-wise L2 cache optimization, we achieved ~2,650 TFLOP/s with grouped MXFP8 matrix multiplication kernels ‚Äî only a 4% drop compared to the non-grouped version. Great!\"]}]\na7:[\"$\",\"h3\",\"a621c48de8a1\",{\"id\":\"grouped-matrix-multiplication-benchmarks\",\"children\":[\"$\",\"a\",null,{\"href\":\"#grouped-matrix-multiplication-benchmarks\",\"className\":\"hover:opacity-90\",\"children\":[\"Grouped matrix multiplication benchmarks\"]}]}]\na8:[\"$\",\"p\",\"4c04a451de94\",{\"children\":[\"The closest open-source alternative on Blackwell for grouped Fprop/Dgrad/Wgrad is DeepSeek‚Äôs DeepGEMM. DeepGEMM differs slightly in that it uses 1x128 and 128x128 scale blocks for the A and B matrices, which comes at the cost of reduced accuracy. Still, it is the only alternative available, so we integrated DeepGEMM into our internal model training and profiled its performance. While DeepGEMM excelled on certain input shapes in our microbenchmarks, our end-to-end benchmarks showed that our kernels outperformed it for our workloads.\"]}]\n"])

self.__next_f.push([1,"a9:[\"$\",\"div\",\"f29a67d580ef\",{\"className\":\"table-wrapper \",\"children\":[[\"$\",\"table\",null,{\"className\":\"w-full\",\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",\"0\",{\"className\":\"pb-v5/12 pr-g1 border-b border-theme-border-02 text-left text-sm font-bold whitespace-pre [\u0026:last-of-type]:pr-0\",\"children\":\"$undefined\"}],[\"$\",\"th\",\"1\",{\"className\":\"pb-v5/12 pr-g1 border-b border-theme-border-02 text-left text-sm font-bold whitespace-pre [\u0026:last-of-type]:pr-0\",\"children\":\"DeepSeek DeepGEMM\"}],[\"$\",\"th\",\"2\",{\"className\":\"pb-v5/12 pr-g1 border-b border-theme-border-02 text-left text-sm font-bold whitespace-pre [\u0026:last-of-type]:pr-0\",\"children\":\"Ours\"}]]}]}],[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",\"0\",{\"className\":\"border-b border-theme-border-02 last:border-b-0\",\"children\":[[\"$\",\"td\",\"0\",{\"className\":\"pt-v5/12 pr-g1 md:pt-v7/12 text-left align-top text-sm whitespace-pre-wrap [\u0026:last-of-type]:pr-0 \",\"children\":[[\"$\",\"p\",\"906fdd8beaac\",{\"children\":[\"Grouped Fprop / Dgrad\"]}]]}],[\"$\",\"td\",\"1\",{\"className\":\"pt-v5/12 pr-g1 md:pt-v7/12 text-left align-top text-sm whitespace-pre-wrap [\u0026:last-of-type]:pr-0 \",\"children\":[[\"$\",\"p\",\"b8c064e8e6d3\",{\"children\":[\"0.67 ms\"]}]]}],[\"$\",\"td\",\"2\",{\"className\":\"pt-v5/12 pr-g1 md:pt-v7/12 text-left align-top text-sm whitespace-pre-wrap [\u0026:last-of-type]:pr-0 \",\"children\":[[\"$\",\"p\",\"02eeff80b653\",{\"children\":[[\"$\",\"strong\",\"0c6aec82dbbd\",{\"children\":[\"0.43 ms\"]}]]}]]}]]}],[\"$\",\"tr\",\"1\",{\"className\":\"border-b border-theme-border-02 last:border-b-0\",\"children\":[[\"$\",\"td\",\"0\",{\"className\":\"pt-v5/12 pr-g1 md:pt-v7/12 text-left align-top text-sm whitespace-pre-wrap [\u0026:last-of-type]:pr-0 \",\"children\":[[\"$\",\"p\",\"f2b03f9cc748\",{\"children\":[\"Grouped Wgrad\"]}]]}],[\"$\",\"td\",\"1\",{\"className\":\"pt-v5/12 pr-g1 md:pt-v7/12 text-left align-top text-sm whitespace-pre-wrap [\u0026:last-of-type]:pr-0 \",\"children\":[[\"$\",\"p\",\"c88ba4c5bacd\",{\"children\":[\"0.71 ms\"]}]]}],[\"$\",\"td\",\"2\",{\"className\":\"pt-v5/12 pr-g1 md:pt-v7/12 text-left align-top text-sm whitespace-pre-wrap [\u0026:last-of-type]:pr-0 \",\"children\":[[\"$\",\"p\",\"e2a7e09be656\",{\"children\":[[\"$\",\"strong\",\"54735b5f5a86\",{\"children\":[\"0.65 ms\"]}]]}]]}]]}]]}]]}],[\"$\",\"p\",null,{\"className\":\"mt-v1 text-sm text-theme-text-sec\",\"children\":\"Table 3. Average latency for grouped Fprop/Dgrad and Wgrad kernels during internal model training.\"}]]}]\n"])

self.__next_f.push([1,"aa:[\"$\",\"p\",\"cfb016ac2a50\",{\"children\":[\"It is also important to note that the above benchmarks \",[\"$\",\"strong\",\"39b5cc4f3e37\",{\"children\":[\"exclude quantization time\"]}],\". DeepGEMM does not provide optimized quantization kernels, and without them, end-to-end performance can quickly degrade. In the worst case, it can even be slower than BF16 training.\"]}]\nab:[\"$\",\"h2\",\"9c46f2b8fd91\",{\"id\":\"building-the-fastest-mxfp8-quantization-kernel-ever\",\"children\":[\"$\",\"a\",null,{\"href\":\"#building-the-fastest-mxfp8-quantization-kernel-ever\",\"className\":\"hover:opacity-90\",\"children\":[\"Building the fastest MXFP8 quantization kernel ever\"]}]}]\nac:[\"$\",\"p\",\"3d6640f4f3c4\",{\"children\":[\"As noted earlier, existing MXFP8 quantization kernels are not only suboptimal but also require reshaping to match \",[\"$\",\"code\",\"77906d2902f8\",{\"children\":[\"tcgen05.mma\"]}],\" scale factor layout, incurring extra runtime overhead. Our goal, therefore, was to design a quantization kernel that:\"]}]\n"])

self.__next_f.push([1,"af:[\"$\",\"p\",\"2c7c7d3bfaa8\",{\"children\":[\"With these optimizations, we implemented an MXFP8 quantization kernel that sustains 6.2+ TB/s while producing the scale matrix in a layout directly compatible with \",[\"$\",\"code\",\"c2a7a0406867\",{\"children\":[\"tcgen05.mma\"]}],\". To the best of our knowledge, this is the fastest MXFP8 quantization kernel available for MoE training.\"]}]\n"])

self.__next_f.push([1,"b0:[\"$\",\"div\",\"f6725d29becc\",{\"className\":\"table-wrapper \",\"children\":[[\"$\",\"table\",null,{\"className\":\"w-full\",\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",\"0\",{\"className\":\"pb-v5/12 pr-g1 border-b border-theme-border-02 text-left text-sm font-bold whitespace-pre [\u0026:last-of-type]:pr-0\",\"children\":\"$undefined\"}],[\"$\",\"th\",\"1\",{\"className\":\"pb-v5/12 pr-g1 border-b border-theme-border-02 text-left text-sm font-bold whitespace-pre [\u0026:last-of-type]:pr-0\",\"children\":\"NVIDIA TransformerEngine\"}],[\"$\",\"th\",\"2\",{\"className\":\"pb-v5/12 pr-g1 border-b border-theme-border-02 text-left text-sm font-bold whitespace-pre [\u0026:last-of-type]:pr-0\",\"children\":\"PyTorch TorchAO\"}],[\"$\",\"th\",\"3\",{\"className\":\"pb-v5/12 pr-g1 border-b border-theme-border-02 text-left text-sm font-bold whitespace-pre [\u0026:last-of-type]:pr-0\",\"children\":\"Ours\"}]]}]}],[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",\"0\",{\"className\":\"border-b border-theme-border-02 last:border-b-0\",\"children\":[[\"$\",\"td\",\"0\",{\"className\":\"pt-v5/12 pr-g1 md:pt-v7/12 text-left align-top text-sm whitespace-pre-wrap [\u0026:last-of-type]:pr-0 \",\"children\":[[\"$\",\"p\",\"8df8114c53ad\",{\"children\":[\"Naive\"]}]]}],[\"$\",\"td\",\"1\",{\"className\":\"pt-v5/12 pr-g1 md:pt-v7/12 text-left align-top text-sm whitespace-pre-wrap [\u0026:last-of-type]:pr-0 \",\"children\":[[\"$\",\"p\",\"f90813a86f0e\",{\"children\":[\"5236.35 GB/s\"]}]]}],[\"$\",\"td\",\"2\",{\"className\":\"pt-v5/12 pr-g1 md:pt-v7/12 text-left align-top text-sm whitespace-pre-wrap [\u0026:last-of-type]:pr-0 \",\"children\":[[\"$\",\"p\",\"066c375844ed\",{\"children\":[\"5245.15 GB/s\"]}]]}],[\"$\",\"td\",\"3\",{\"className\":\"pt-v5/12 pr-g1 md:pt-v7/12 text-left align-top text-sm whitespace-pre-wrap [\u0026:last-of-type]:pr-0 \",\"children\":[[\"$\",\"p\",\"d755cf0ebaae\",{\"children\":[\"Not applicable\"]}]]}]]}],[\"$\",\"tr\",\"1\",{\"className\":\"border-b border-theme-border-02 last:border-b-0\",\"children\":[[\"$\",\"td\",\"0\",{\"className\":\"pt-v5/12 pr-g1 md:pt-v7/12 text-left align-top text-sm whitespace-pre-wrap [\u0026:last-of-type]:pr-0 \",\"children\":[[\"$\",\"p\",\"d240b7d59536\",{\"children\":[\"With reshape\"]}]]}],[\"$\",\"td\",\"1\",{\"className\":\"pt-v5/12 pr-g1 md:pt-v7/12 text-left align-top text-sm whitespace-pre-wrap [\u0026:last-of-type]:pr-0 \",\"children\":[[\"$\",\"p\",\"8790fcbcd9c0\",{\"children\":[\"4430.27 GB/s\"]}]]}],[\"$\",\"td\",\"2\",{\"className\":\"pt-v5/12 pr-g1 md:pt-v7/12 text-left align-top text-sm whitespace-pre-wrap [\u0026:last-of-type]:pr-0 \",\"children\":[[\"$\",\"p\",\"bb0dae419480\",{\"children\":[\"4524.45 GB/s\"]}]]}],[\"$\",\"td\",\"3\",{\"className\":\"pt-v5/12 pr-g1 md:pt-v7/12 text-left align-top text-sm whitespace-pre-wrap [\u0026:last-of-type]:pr-0 \",\"children\":[[\"$\",\"p\",\"5fdab253c17f\",{\"children\":[[\"$\",\"strong\",\"5e7cf49c56c3\",{\"children\":[\"6212.21 GB/s\"]}]]}]]}]]}]]}]]}],[\"$\",\"p\",null,{\"className\":\"mt-v1 text-sm text-theme-text-sec\",\"children\":\"Table 4. MXFP8 quantization kernel comparison (E4M3, 32-block scaling) by memory bandwidth utilization. ‚ÄúWith reshape‚Äù includes the time to reshape the scale factor for tcgen05.mma.\"}]]}]\n"])

self.__next_f.push([1,"b1:[\"$\",\"p\",\"13497ba19f27\",{\"children\":[\"Other optimizations to the MXFP8 quantization included building a fused quantization kernel that outputs both non-transposed and transposed results (the latter needed for backward passes), as well as fusing MXFP8 quantization logic directly into other kernels to minimize HBM access as much as possible. For example, we attached MXFP8 dequantization/quantization logic to the prologue and epilogue of our fused MXFP8 SwiGLU kernel.\"]}]\nb2:[\"$\",\"h2\",\"d780f90ac3fa\",{\"id\":\"speedups\",\"children\":[\"$\",\"a\",null,{\"href\":\"#speedups\",\"className\":\"hover:opacity-90\",\"children\":[\"Speedups!\"]}]}]\nb3:[\"$\",\"p\",\"3d0bc278cbb6\",{\"children\":[\"With all the optimizations described above, we achieved \",[\"$\",\"strong\",\"7525f62323b5\",{\"children\":[\"3.5x faster MoE layer execution for both the forward and backward passes while maintaining the same training quality\"]}],\". When tested on one of our internal models, this translated into a 1.5x end-to-end training speedup on Blackwell and a 2x speedup compared to our original Hopper setup, measured in tokens per second per GPU (TPS/GPU). We believe that our stack runs MXFP8 MoE training faster than any combination of open-source alternatives.\"]}]\nca:T5c0,"])

self.__next_f.push([1,"b4:[\"$\",\"figure\",\"da5d4217390b\",{\"className\":\"stack\",\"children\":[[\"$\",\"div\",null,{\"className\":\"media-border-container overflow-hidden\",\"children\":[[\"$\",\"picture\",null,{\"className\":\"block h-full w-full\",\"children\":[[\"$\",\"source\",null,{\"media\":\"(min-width: 661px)\",\"srcSet\":\"$ca\"}],\"$undefined\",[\"$\",\"img\",null,{\"className\":\"object-cover h-full w-full\",\"alt\":\"Figure 9. End-to-end training TPS per GPU (internal model).\",\"src\":\"https://cdn.sanity.io/images/2hv88549/production/2608411ad16d9d6b80ea7e8418a4d5f27a017ddd-992x682.png?auto=format\",\"loading\":\"lazy\",\"decoding\":\"async\",\"width\":\"$undefined\",\"height\":\"$undefined\",\"fetchPriority\":\"auto\"}]]}],\"$undefined\"]}],[\"$\",\"figcaption\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"Figure 9. End-to-end training TPS per GPU (internal model).\"}}]]}]\n"])

self.__next_f.push([1,"b6:[\"$\",\"h2\",\"24a2e43e0aa7\",{\"id\":\"closing-thoughts\",\"children\":[\"$\",\"a\",null,{\"href\":\"#closing-thoughts\",\"className\":\"hover:opacity-90\",\"children\":[\"Closing thoughts\"]}]}]\nb7:[\"$\",\"p\",\"bf2ddcfa6f4b\",{\"children\":[\"We still have plenty of work ahead and many kernels left to optimize. Our current efforts focus on building more efficient multi-GPU communication, improving custom attention kernels, and preparing to transition to \",[\"$\",\"strong\",\"1cdc8961a1df\",{\"children\":[\"FP4\"]}],\" for future MoE training runs.\"]}]\nb8:[\"$\",\"p\",\"6081646b58eb\",{\"children\":[\"If you are interested in writing and optimizing high-performance kernels, training large coding models, or our work broadly, we‚Äôd love to hear from you. Reach out to us at \",[\"$\",\"$L28\",\"c35a525c25ba\",{\"link\":{\"_key\":\"1747b6824834\",\"_type\":\"link\",\"file\":null,\"href\":\"mailto:hiring@anysphere.co\",\"label\":null,\"linkType\":null,\"openInNewTab\":null,\"page\":null,\"post\":null,\"postLanguage\":null,\"simplePage\":null},\"children\":[\"hiring@anysphere.co\"]}],\".\"]}]\nb9:[\"$\",\"p\",\"9020be66156f\",{\"children\":[\"Special thanks to Benjamin Spector, Sasha Rush, Less Wright, Federico Cassano, Rohan Shah, Jacob Jackson, Sujay Jayakar, and Shengtong Zhang for reading this post and offering valuable feedback.\"]}]\nba:\"Filed under:\"\nbb:\"Author\"\n"])


## üèóÔ∏è KI·∫æN TR√öC V√Ä C√îNG NGH·ªÜ

### C√°c th√†nh ph·∫ßn ch√≠nh:
- **AI Engine:** H·ªá th·ªëng AI m·∫°nh m·∫Ω cho vi·ªác h·ªó tr·ª£ l·∫≠p tr√¨nh
- **Code Analysis:** Ph√¢n t√≠ch m√£ ngu·ªìn th√¥ng minh
- **Auto-completion:** Ho√†n thi·ªán m√£ t·ª± ƒë·ªông
- **Code Generation:** T·∫°o m√£ t·ª´ m√¥ t·∫£ t·ª± nhi√™n

## üìä HI·ªÜU SU·∫§T V√Ä T·ªêI ∆ØU H√ìA

### C√°c c·∫£i ti·∫øn g·∫ßn ƒë√¢y:
- C·∫£i thi·ªán hi·ªáu su·∫•t x·ª≠ l√Ω
- T·ªëi ∆∞u h√≥a thu·∫≠t to√°n AI
- N√¢ng cao ƒë·ªô ch√≠nh x√°c d·ª± ƒëo√°n
- Gi·∫£m th·ªùi gian ph·∫£n h·ªìi

## üîß API V√Ä T√çCH H·ª¢P

### C√°c API ch√≠nh:
- REST API cho t√≠ch h·ª£p
- Webhook cho th√¥ng b√°o
- SDK cho c√°c ng√¥n ng·ªØ l·∫≠p tr√¨nh ph·ªï bi·∫øn

---
*T√†i li·ªáu k·ªπ thu·∫≠t n√†y ƒë∆∞·ª£c c·∫≠p nh·∫≠t th∆∞·ªùng xuy√™n ƒë·ªÉ ph·∫£n √°nh c√°c thay ƒë·ªïi m·ªõi nh·∫•t.*
