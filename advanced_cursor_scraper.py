#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script cÃ o web nÃ¢ng cao Ä‘á»ƒ thu tháº­p thÃ´ng tin Ä‘áº§y Ä‘á»§ vá» Cursor AI
Bao gá»“m: website chÃ­nh, blog, forum, cá»™ng Ä‘á»“ng, Reddit, Discord
TÃ¡c giáº£: Cursor Assistant cho anh NghÄ©a
NgÃ y: 25/10/2025
"""

import requests
from bs4 import BeautifulSoup
import json
import time
import os
from datetime import datetime
import re
from urllib.parse import urljoin, urlparse
import logging
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager

# Thiáº¿t láº­p logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('advanced_cursor_scraping.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

class AdvancedCursorScraper:
    def __init__(self):
        self.base_url = "https://cursor.com"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })
        
        # CÃ¡c nguá»“n thÃ´ng tin cáº§n cÃ o
        self.sources = {
            'main_website': {
                'urls': [
                    'https://cursor.com',
                    'https://cursor.com/features',
                    'https://cursor.com/pricing',
                    'https://cursor.com/docs',
                    'https://cursor.com/blog'
                ],
                'type': 'website'
            },
            'reddit': {
                'urls': [
                    'https://www.reddit.com/r/cursor/',
                    'https://www.reddit.com/r/cursorai/',
                    'https://www.reddit.com/r/MachineLearning/search/?q=cursor&restrict_sr=1&sort=new'
                ],
                'type': 'reddit'
            },
            'github': {
                'urls': [
                    'https://github.com/getcursor/cursor',
                    'https://github.com/topics/cursor-ai'
                ],
                'type': 'github'
            },
            'discord': {
                'urls': [
                    'https://discord.gg/cursor'
                ],
                'type': 'discord'
            },
            'youtube': {
                'urls': [
                    'https://www.youtube.com/results?search_query=cursor+ai+tutorial',
                    'https://www.youtube.com/results?search_query=cursor+ai+review'
                ],
                'type': 'youtube'
            }
        }
        
        self.scraped_data = {
            'main_website': {},
            'reddit_posts': [],
            'github_repos': [],
            'discord_info': {},
            'youtube_videos': [],
            'community_insights': [],
            'tutorials_guides': [],
            'user_reviews': [],
            'metadata': {
                'scraped_at': datetime.now().isoformat(),
                'total_sources': 0,
                'successful_sources': 0
            }
        }
        
        # Khá»Ÿi táº¡o Selenium driver
        self.driver = None
        self.init_selenium()
    
    def init_selenium(self):
        """Khá»Ÿi táº¡o Selenium WebDriver"""
        try:
            chrome_options = Options()
            chrome_options.add_argument('--headless')
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
            
            self.driver = webdriver.Chrome(
                service=webdriver.chrome.service.Service(ChromeDriverManager().install()),
                options=chrome_options
            )
            logging.info("âœ… Khá»Ÿi táº¡o Selenium thÃ nh cÃ´ng")
        except Exception as e:
            logging.error(f"âŒ Lá»—i khá»Ÿi táº¡o Selenium: {e}")
            self.driver = None
    
    def scrape_reddit(self, url):
        """CÃ o Reddit posts"""
        logging.info(f"ğŸ” CÃ o Reddit: {url}")
        
        try:
            if self.driver:
                self.driver.get(url)
                time.sleep(3)
                
                # TÃ¬m cÃ¡c post
                posts = self.driver.find_elements(By.CSS_SELECTOR, '[data-testid="post-container"]')
                
                reddit_data = []
                for post in posts[:10]:  # Láº¥y 10 post Ä‘áº§u
                    try:
                        title_elem = post.find_element(By.CSS_SELECTOR, 'h3')
                        title = title_elem.text.strip()
                        
                        # TÃ¬m ná»™i dung post
                        content_elem = post.find_element(By.CSS_SELECTOR, '[data-testid="post-content"]')
                        content = content_elem.text.strip()
                        
                        # TÃ¬m sá»‘ upvote
                        upvote_elem = post.find_element(By.CSS_SELECTOR, '[data-testid="vote-arrows"]')
                        upvotes = upvote_elem.text.strip()
                        
                        reddit_data.append({
                            'title': title,
                            'content': content[:500],  # Giá»›i háº¡n 500 kÃ½ tá»±
                            'upvotes': upvotes,
                            'url': url,
                            'scraped_at': datetime.now().isoformat()
                        })
                    except Exception as e:
                        logging.warning(f"Lá»—i khi cÃ o post Reddit: {e}")
                        continue
                
                return reddit_data
            else:
                # Fallback vá»›i requests
                response = self.session.get(url)
                soup = BeautifulSoup(response.content, 'html.parser')
                return []
                
        except Exception as e:
            logging.error(f"Lá»—i cÃ o Reddit {url}: {e}")
            return []
    
    def scrape_github(self, url):
        """CÃ o GitHub repositories"""
        logging.info(f"ğŸ” CÃ o GitHub: {url}")
        
        try:
            response = self.session.get(url)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            github_data = []
            
            # TÃ¬m cÃ¡c repository
            repos = soup.find_all('article', class_='Box-row')
            
            for repo in repos[:10]:  # Láº¥y 10 repo Ä‘áº§u
                try:
                    title_elem = repo.find('h3', class_='wb-break-all')
                    if title_elem:
                        title = title_elem.get_text().strip()
                        
                        # TÃ¬m mÃ´ táº£
                        desc_elem = repo.find('p', class_='col-9')
                        description = desc_elem.get_text().strip() if desc_elem else ""
                        
                        # TÃ¬m stars
                        stars_elem = repo.find('a', href=lambda x: x and '/stargazers' in x)
                        stars = stars_elem.get_text().strip() if stars_elem else "0"
                        
                        github_data.append({
                            'title': title,
                            'description': description,
                            'stars': stars,
                            'url': url,
                            'scraped_at': datetime.now().isoformat()
                        })
                except Exception as e:
                    logging.warning(f"Lá»—i khi cÃ o repo GitHub: {e}")
                    continue
            
            return github_data
            
        except Exception as e:
            logging.error(f"Lá»—i cÃ o GitHub {url}: {e}")
            return []
    
    def scrape_youtube(self, url):
        """CÃ o YouTube videos"""
        logging.info(f"ğŸ” CÃ o YouTube: {url}")
        
        try:
            if self.driver:
                self.driver.get(url)
                time.sleep(5)
                
                # TÃ¬m cÃ¡c video
                videos = self.driver.find_elements(By.CSS_SELECTOR, 'ytd-video-renderer')
                
                youtube_data = []
                for video in videos[:10]:  # Láº¥y 10 video Ä‘áº§u
                    try:
                        title_elem = video.find_element(By.CSS_SELECTOR, '#video-title')
                        title = title_elem.get_attribute('title')
                        
                        # TÃ¬m channel
                        channel_elem = video.find_element(By.CSS_SELECTOR, '#channel-name a')
                        channel = channel_elem.text.strip()
                        
                        # TÃ¬m views
                        views_elem = video.find_element(By.CSS_SELECTOR, '#metadata-line span:first-child')
                        views = views_elem.text.strip()
                        
                        # TÃ¬m link
                        link_elem = video.find_element(By.CSS_SELECTOR, '#video-title')
                        video_url = link_elem.get_attribute('href')
                        
                        youtube_data.append({
                            'title': title,
                            'channel': channel,
                            'views': views,
                            'url': video_url,
                            'scraped_at': datetime.now().isoformat()
                        })
                    except Exception as e:
                        logging.warning(f"Lá»—i khi cÃ o video YouTube: {e}")
                        continue
                
                return youtube_data
            else:
                return []
                
        except Exception as e:
            logging.error(f"Lá»—i cÃ o YouTube {url}: {e}")
            return []
    
    def scrape_main_website_advanced(self):
        """CÃ o website chÃ­nh vá»›i thÃ´ng tin chi tiáº¿t hÆ¡n"""
        logging.info("ğŸš€ CÃ o website chÃ­nh vá»›i thÃ´ng tin nÃ¢ng cao...")
        
        main_data = {
            'homepage': {},
            'features': {},
            'pricing': {},
            'docs': {},
            'blog': {}
        }
        
        for page_name, url in [
            ('homepage', 'https://cursor.com'),
            ('features', 'https://cursor.com/features'),
            ('pricing', 'https://cursor.com/pricing'),
            ('docs', 'https://cursor.com/docs'),
            ('blog', 'https://cursor.com/blog')
        ]:
            logging.info(f"ğŸ“„ CÃ o trang: {page_name}")
            
            try:
                response = self.session.get(url, timeout=30)
                response.raise_for_status()
                soup = BeautifulSoup(response.content, 'html.parser')
                
                page_data = {
                    'url': url,
                    'title': soup.find('title').text if soup.find('title') else '',
                    'description': '',
                    'headings': [],
                    'content_sections': [],
                    'ai_mentions': [],
                    'code_examples': [],
                    'testimonials': [],
                    'cta_buttons': [],
                    'scraped_at': datetime.now().isoformat()
                }
                
                # Meta description
                meta_desc = soup.find('meta', attrs={'name': 'description'})
                if meta_desc:
                    page_data['description'] = meta_desc.get('content', '')
                
                # Headings
                for tag in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
                    for heading in soup.find_all(tag):
                        text = heading.get_text().strip()
                        if text:
                            page_data['headings'].append({
                                'level': tag,
                                'text': text
                            })
                
                # Content sections
                for section in soup.find_all(['div', 'section'], class_=re.compile(r'content|section|feature|benefit', re.I)):
                    section_text = section.get_text().strip()
                    if section_text and len(section_text) > 50:
                        page_data['content_sections'].append(section_text)
                
                # AI mentions
                ai_keywords = ['AI', 'artificial intelligence', 'machine learning', 'neural', 'GPT', 'assistant', 'intelligent', 'smart', 'automated']
                for element in soup.find_all(text=re.compile('|'.join(ai_keywords), re.I)):
                    if element.parent:
                        ai_text = element.parent.get_text().strip()
                        if ai_text and len(ai_text) > 30:
                            page_data['ai_mentions'].append(ai_text)
                
                # Code examples
                for code_block in soup.find_all(['code', 'pre']):
                    code_text = code_block.get_text().strip()
                    if code_text and len(code_text) > 20:
                        page_data['code_examples'].append(code_text)
                
                # Testimonials
                for testimonial in soup.find_all(['div', 'blockquote'], class_=re.compile(r'testimonial|review|quote|feedback', re.I)):
                    testimonial_text = testimonial.get_text().strip()
                    if testimonial_text and len(testimonial_text) > 30:
                        page_data['testimonials'].append(testimonial_text)
                
                # CTA buttons
                for button in soup.find_all(['button', 'a'], class_=re.compile(r'cta|button|download|get|start|try', re.I)):
                    button_text = button.get_text().strip()
                    if button_text:
                        page_data['cta_buttons'].append(button_text)
                
                main_data[page_name] = page_data
                logging.info(f"âœ… ÄÃ£ cÃ o xong {page_name}: {len(page_data['headings'])} headings, {len(page_data['content_sections'])} sections")
                
            except Exception as e:
                logging.error(f"âŒ Lá»—i cÃ o {page_name}: {e}")
                main_data[page_name] = {'error': str(e)}
        
        self.scraped_data['main_website'] = main_data
    
    def scrape_community_sources(self):
        """CÃ o cÃ¡c nguá»“n cá»™ng Ä‘á»“ng"""
        logging.info("ğŸŒ Báº¯t Ä‘áº§u cÃ o cÃ¡c nguá»“n cá»™ng Ä‘á»“ng...")
        
        # Reddit
        for url in self.sources['reddit']['urls']:
            reddit_posts = self.scrape_reddit(url)
            self.scraped_data['reddit_posts'].extend(reddit_posts)
            time.sleep(2)
        
        # GitHub
        for url in self.sources['github']['urls']:
            github_repos = self.scrape_github(url)
            self.scraped_data['github_repos'].extend(github_repos)
            time.sleep(2)
        
        # YouTube
        for url in self.sources['youtube']['urls']:
            youtube_videos = self.scrape_youtube(url)
            self.scraped_data['youtube_videos'].extend(youtube_videos)
            time.sleep(3)
    
    def extract_community_insights(self):
        """TrÃ­ch xuáº¥t insights tá»« cá»™ng Ä‘á»“ng"""
        logging.info("ğŸ’¡ TrÃ­ch xuáº¥t insights tá»« cá»™ng Ä‘á»“ng...")
        
        insights = {
            'common_issues': [],
            'feature_requests': [],
            'user_tips': [],
            'tutorials': [],
            'reviews': [],
            'discussions': []
        }
        
        # Tá»« Reddit posts
        for post in self.scraped_data['reddit_posts']:
            title = post.get('title', '').lower()
            content = post.get('content', '').lower()
            
            if any(keyword in title for keyword in ['tutorial', 'guide', 'how to', 'tips']):
                insights['tutorials'].append(post)
            elif any(keyword in title for keyword in ['review', 'opinion', 'thoughts']):
                insights['reviews'].append(post)
            elif any(keyword in title for keyword in ['issue', 'problem', 'bug', 'error']):
                insights['common_issues'].append(post)
            elif any(keyword in title for keyword in ['request', 'suggestion', 'feature']):
                insights['feature_requests'].append(post)
            else:
                insights['discussions'].append(post)
        
        # Tá»« YouTube videos
        for video in self.scraped_data['youtube_videos']:
            title = video.get('title', '').lower()
            if any(keyword in title for keyword in ['tutorial', 'guide', 'how to', 'tips', 'review']):
                insights['tutorials'].append(video)
        
        self.scraped_data['community_insights'] = insights
    
    def save_advanced_data(self):
        """LÆ°u dá»¯ liá»‡u nÃ¢ng cao"""
        # Táº¡o thÆ° má»¥c
        os.makedirs('cursor_ai_library_advanced', exist_ok=True)
        
        # LÆ°u dá»¯ liá»‡u JSON
        with open('cursor_ai_library_advanced/advanced_data.json', 'w', encoding='utf-8') as f:
            json.dump(self.scraped_data, f, ensure_ascii=False, indent=2)
        
        # Táº¡o bÃ¡o cÃ¡o cá»™ng Ä‘á»“ng
        self.create_community_report()
        
        logging.info("ğŸ’¾ ÄÃ£ lÆ°u dá»¯ liá»‡u nÃ¢ng cao vÃ o cursor_ai_library_advanced/")
    
    def create_community_report(self):
        """Táº¡o bÃ¡o cÃ¡o vá» cá»™ng Ä‘á»“ng"""
        report = f"""# CURSOR AI - BÃO CÃO Cá»˜NG Äá»’NG

*TÃ i liá»‡u Ä‘Æ°á»£c táº¡o tá»± Ä‘á»™ng - {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}*

## ğŸŒ CÃC NGUá»’N Cá»˜NG Äá»’NG ÄÃƒ CÃ€O

### Reddit Posts
- **Tá»•ng sá»‘ posts:** {len(self.scraped_data['reddit_posts'])}
- **Nguá»“n:** r/cursor, r/cursorai, r/MachineLearning

### GitHub Repositories
- **Tá»•ng sá»‘ repos:** {len(self.scraped_data['github_repos'])}
- **Nguá»“n:** github.com/getcursor/cursor, github.com/topics/cursor-ai

### YouTube Videos
- **Tá»•ng sá»‘ videos:** {len(self.scraped_data['youtube_videos'])}
- **Nguá»“n:** YouTube search results

## ğŸ’¡ INSIGHTS Tá»ª Cá»˜NG Äá»’NG

### Tutorials vÃ  HÆ°á»›ng dáº«n
"""
        
        for tutorial in self.scraped_data['community_insights']['tutorials'][:10]:
            report += f"- {tutorial.get('title', 'N/A')}\n"
        
        report += f"""
### Reviews vÃ  ÄÃ¡nh giÃ¡
"""
        
        for review in self.scraped_data['community_insights']['reviews'][:10]:
            report += f"- {review.get('title', 'N/A')}\n"
        
        report += f"""
### Váº¥n Ä‘á» thÆ°á»ng gáº·p
"""
        
        for issue in self.scraped_data['community_insights']['common_issues'][:10]:
            report += f"- {issue.get('title', 'N/A')}\n"
        
        report += f"""
### Feature Requests
"""
        
        for request in self.scraped_data['community_insights']['feature_requests'][:10]:
            report += f"- {request.get('title', 'N/A')}\n"
        
        report += f"""
## ğŸ”— CÃC DIá»„N ÄÃ€N VÃ€ Cá»˜NG Äá»’NG QUAN TRá»ŒNG

### 1. Reddit Communities
- **r/cursor** - Cá»™ng Ä‘á»“ng chÃ­nh vá» Cursor
- **r/cursorai** - Tháº£o luáº­n vá» Cursor AI
- **r/MachineLearning** - Tháº£o luáº­n vá» AI/ML

### 2. GitHub
- **github.com/getcursor/cursor** - Repository chÃ­nh
- **github.com/topics/cursor-ai** - CÃ¡c project liÃªn quan

### 3. YouTube Channels
- TÃ¬m kiáº¿m "cursor ai tutorial" Ä‘á»ƒ tÃ¬m hÆ°á»›ng dáº«n
- TÃ¬m kiáº¿m "cursor ai review" Ä‘á»ƒ tÃ¬m Ä‘Ã¡nh giÃ¡

### 4. Discord
- **discord.gg/cursor** - Server Discord chÃ­nh thá»©c

## ğŸ“š TÃ€I LIá»†U Há»ŒC Táº¬P

### Tá»« cá»™ng Ä‘á»“ng:
"""
        
        for tutorial in self.scraped_data['community_insights']['tutorials'][:15]:
            report += f"- {tutorial.get('title', 'N/A')}\n"
            if tutorial.get('url'):
                report += f"  Link: {tutorial['url']}\n"
        
        report += f"""
## ğŸ¯ KHUYáº¾N NGHá»Š CHO ANH NGHÄ¨A

### Äá»ƒ há»c Cursor hiá»‡u quáº£:
1. **Tham gia Reddit communities** - Cáº­p nháº­t thÃ´ng tin má»›i nháº¥t
2. **Xem YouTube tutorials** - Há»c cÃ¡ch sá»­ dá»¥ng thá»±c táº¿
3. **Tham gia Discord** - Há»i Ä‘Ã¡p trá»±c tiáº¿p vá»›i cá»™ng Ä‘á»“ng
4. **Theo dÃµi GitHub** - Cáº­p nháº­t phiÃªn báº£n má»›i
5. **Äá»c user reviews** - Hiá»ƒu Æ°u nhÆ°á»£c Ä‘iá»ƒm

### CÃ¡c chá»§ Ä‘á» nÃªn tÃ¬m hiá»ƒu:
- CÃ i Ä‘áº·t vÃ  setup Cursor
- CÃ¡c tÃ­nh nÄƒng AI chÃ­nh
- Tips vÃ  tricks sá»­ dá»¥ng
- Troubleshooting thÆ°á»ng gáº·p
- So sÃ¡nh vá»›i cÃ¡c cÃ´ng cá»¥ khÃ¡c

---
*BÃ¡o cÃ¡o Ä‘Æ°á»£c táº¡o tá»± Ä‘á»™ng tá»« dá»¯ liá»‡u cá»™ng Ä‘á»“ng*
"""
        
        with open('cursor_ai_library_advanced/COMMUNITY_REPORT.md', 'w', encoding='utf-8') as f:
            f.write(report)
    
    def run_advanced_scraping(self):
        """Cháº¡y cÃ o web nÃ¢ng cao"""
        logging.info("ğŸš€ Báº¯t Ä‘áº§u cÃ o web nÃ¢ng cao...")
        
        try:
            # CÃ o website chÃ­nh
            self.scrape_main_website_advanced()
            
            # CÃ o cÃ¡c nguá»“n cá»™ng Ä‘á»“ng
            self.scrape_community_sources()
            
            # TrÃ­ch xuáº¥t insights
            self.extract_community_insights()
            
            # LÆ°u dá»¯ liá»‡u
            self.save_advanced_data()
            
            logging.info("âœ… HoÃ n thÃ nh cÃ o web nÃ¢ng cao!")
            
        except Exception as e:
            logging.error(f"âŒ Lá»—i trong quÃ¡ trÃ¬nh cÃ o web: {e}")
        finally:
            if self.driver:
                self.driver.quit()

if __name__ == "__main__":
    scraper = AdvancedCursorScraper()
    scraper.run_advanced_scraping()